<<echo=FALSE, cache=FALSE>>=
set_parent('thesis.Rnw')
@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Багатовимірна каскадна нео-фаззі система, що еволюціонує}
\label{ch:MIMOEvolvingCascadedSystem}

Задача апроксимації та екстраполяції багатовимірних часових рядів доволі часто виникає у багатьох технічних, медико-біологічних та інших дослідженнях, де якість прийнятих рішень істотно залежить від точності синтезованих прогнозів. У багатьох реальних задачах часові ряди характеризуються високим рівнем нелінійності та нестаціонарності своїх параметрів, наявністю аномальних викидів. Зрозуміло, що традиційні методи аналізу часових рядів, засновані на регресійному, кореляційному та інших подібних підходах, що мають на меті апріорну наявність репрезентативної вибірки спостережень, є неефективними. Альтернативою традиційним статистичним методам може слугувати математичний апарат обчислювального інтелекту, зокрема штучні нейронні мережі та нейро-фаззі-системи \cite{ref44, ref45, ref43, ref46}, завдяки своїм універсальним апроксимувальним властивостям. Водночас з апроксимувальних властивостей зовсім не витікають екстраполюючі, оскільки врахування давньої передісторії для побудови прогнозувальної моделі може погіршити якість прогнозу. У зв'язку з цим під час оброблення нестаціонарних процесів треба відмовитися від процедур навчання, що базуються на зворотному поширенні помилок (багатошарові персептрони, рекурентні нейронні мережі, адаптивні нейромережеві системи нечіткого виведення – ANFIS \cite{ref85}) або методі найменших квадратів (радіально-базисні та функціонально пов’язані нейронні мережі) та скористатися процедурами на основі локальних критеріїв та «короткої» пам’яті типу алгоритма Качмажа-Уідроу-Хоффа. При цьому використані алгоритми навчання мусять забезпечувати не лише високу швидкодію, але й фільтруючі якості для придушення стохастичної «шумової» компоненти в оброблюваному сигналі. У зв’язку з цим синтез спеціалізованих гібридних систем обчислювального інтелекту для розв’язання задач прогнозування істотно нестаціонарних часових рядів за умов невизначеності, що забезпечують разом з високою швидкістю навчання і фільтрацію завад, є досить цікавою та перспективною задачею.

Таким чином, цей розділ присвячено синтезу багатовимірної гібридної системи обчислювального інтелекту, що здатна реалізувати нелінійне відображення $R^n \rightarrow R^g$ у режимі реального часу.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Багатовимірна каскадна система, що еволюціонує, побудована на нео-фаззі нейронах}
\label{sec:MIMOEvolvingCascadedSystemBuiltOnNFNs}

Для вирішеня задачі прогнозування та ідентифікації багатовимірних даних в умовах апріорної і поточної структурної та параметричної невизначеності як ніколи доречні переваги каскадно-кореляційної архітектури, адже системи з такою архітектурою успадковують всі переваги елементів, які використовуються в їх вузлах, а в процесі навчання автоматично підбирається необхідна кількість каскадів для того, щоб отримати модель адекватної складності для вирішення поставленого завдання \cite{ref51, ref55, ref48, ref52, ref53, ref54, ref56, ref57, ref58}. Однак, слід зазначити, що каскадно-кореляційна мережа у формі, що її запропонували С. Фальман і К. Лєб'єр \cite{ref48}, є системою з одним виходом, тобто не здатна реалізувати нелінійне відображення $R^n \rightarrow R^g$. Це досить серйозне обмеження, оскільки більшість практичних завдань містять кілька вихідних сигналів. Тож пропонуймо такі модифікацїї до архітектури каскaдно-кореляційної мережі CasCorLA:   
\begin{enumerate}
\item замість елементарних персептронів Розенблатта використовувати нео-фаззі нейрони (доцільність такого рішення було детально показано у розділі~\ref{ch:CascadedNeoFuzzySystemWithPoolOptimization}),
\item кількість нейронів у кожному каскаді відтепер має дорівнювати розмірності вектору вихідного сигналу системи.
\end{enumerate}

\begin{figure}
\begin{center}
\includegraphics[width=15cm]{MIMOCcascadeNetworkBuiltOnNFN.eps}
\caption{Архітектура гібридної MIMO системи, побудованої на нео-фаззі нейронах}
\label{fig:MIMOCcascadeNetworkBuiltOnNFN}
\end{center}
\end{figure}

Схему пропонованої архітектури наведено на рис.~\ref{fig:MIMOCcascadeNetworkBuiltOnNFN}.

Тоді вихідний сигнал системи формується з векторів, що його складають вихідні сигнали кращих нейронів останнього каскаду:

\begin{equation}
\hat{y}\left(k\right) = \left(\hat{y}_1^{*[m]}\left(k\right), \hat{y}_2^{*[m]}\left(k\right),\dots,\hat{y}_g^{*[m]}\left(k\right)\right)^T,  
\end{equation}
\medskip

де $g$ - кількість елементів вихідного вектору даних, що іх треба спрогнозувати чи ідентифікувати.\\
Для кожного з нео-фаззі нейронів системи в якості функцій належності можна використовувати трикутні конструкції:

\begin{equation}
\mu_{jli}^{[1]}\left(x_i\right)=
\begin{cases}
\frac{x_i-c_{d,l-1,i}^{[1]j}}{c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}\text { якщо } x_i\in\left[c_{d,l-1,i}^{[1]j},c_{dli}^{[1]j}\right],\\
\frac{c_{d,l+1,i}^{[1]j}-x_i}{c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}\text{ якщо }x_i \in \left[c_{dli}^{[1]j},c_{d,l+1,i}^{[1]j}\right],\\
0\text { у протилежному випадку},
\end{cases}
\end{equation}
\medskip

кубічні сплайни:

\begin{equation}
\mu_{jli}^{[1]}\left(x_i\right)=
\begin{cases}
\frac{1}{4}\left(2+3\frac{2x_i-c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}{c_{dli1}^{[1]j}-c_{d,l-1,i}^{[1]j}}-\left(\frac{2x_i-c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}{c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}\right)^3\right),\\
\text{якщо }x\in\left[c_{d,l-1,i}^{[1]j},c_{dli}^{[1]j}\right],\\
\frac{1}{4}\left(2-3\frac{2x_i-c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}{c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}+\left(\frac{2x_i-c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}{c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}\right)^3\right),\\
\text{якщо }x\in\left[c_{dli}^{[1]j},c_{d,l+1,i}^{[1]j}\right],\\
0\text{ у протилежному випадку},
\end{cases}
\end{equation}

або $B$-сплайни:

\begin{equation}
\mu_{jli}^{g[1]}=
\begin{cases}
\begin{rcases}
1\text{ якщо }x_{i}\in \left[c_{dli}^{[1]j},c_{d,l+1,i}^{[1]j}\right],\\
0\text{ у протилежному випадку}
\end{rcases}
\text{ якщо }g=1,\\
\frac{x_i-c_{dli}^{[1]j}}{c_{d,l+g-1,i}^{[1]j}-c_{dli}^{[1]j}}\mu_{dli}^{g-1,[1]j}\left(x_i\right)+\frac{c_{d,l+g,i}^{[1]j}-x_i}{c_{d,l+g,i}^{[1]j}-c_{d,l+g,i}^{[1]j}}\mu_{d,l+1,i}^{g-1,[1]j}\left(x_i\right),\\
\text{ якщо }g>1,
\end{cases}
\end{equation}
\medskip

де $\mu_{dli}^{g[1]j}\left(x_i\right)$ -- $l$-й сплайн $g$-ого порядку. Варто зауважити, що всі ці конструкції задовільняють умовам одиничного розбиття Руспіні.

Запишемо вихідний сигнал $j$-ого нео-фаззі нейрону $d$-ого виходу першого каскаду у вигляді

\begin{equation}
\begin{cases}
\begin{aligned}

&\hat{y}_d^{[1]j}\left(k\right)=\sum\limits_{i=1}^{n}f_{di}^{[1]j}\left(x_i\left(k\right)\right)=
\sum\limits_{i=1}^{n}\sum\limits_{l=1}^{h}{w_{dli}^{[1]j}\mu_{dli}^{[1]j}\left(x_i\left(k\right)\right)},\\
&\text{якщо }x_i\left(k\right) \text{ це } X_{li}^{j}\text{ тоді вихід }w_{dli}^{[1]j}.

\end{aligned}
\end{cases}
\end{equation}
\medskip

вихідні сигнали нео-фаззі нейронів другого каскаду:

\begin{equation}
\begin{aligned}
\hat{y}_d^{[2]j}=&\sum\limits_{i=1}^{n}\sum\limits_{l=1}^{h}{w_{dli}^{[2]j}\mu_{dli}^{[2]j}\left(x_i\right)}+\\
&+\sum\limits_{d=1}^{g}\sum\limits_{l=1}^{h}{w_{dl,n+1}^{[2]j}\mu_{dl,n+1}^{[2]j}\left(\hat{y}_d^{*[1]}\right)}\text{ }\forall\text{ }{d=1,2,\dots,g}
\end{aligned}
\end{equation}
\medskip

вихідні сигнали $m$-ого каскаду:

\begin{equation}
\begin{aligned}
\hat{y}_d^{[2]j}=&\sum\limits_{i=1}^{n}\sum\limits_{l=1}^{h}{w_{dli}^{[2]j}\mu_{dli}^{[2]j}\left(x_i\right)}+\\
&+\sum\limits_{d=1}^{g}\sum\limits_{p=n+1}^{n+m-1}\sum\limits_{l=1}^{h}{w_{dlp}^{[m]j}\mu_{dlp}^{[m]j}\left(\hat{y}_d^{*[p-n]}\right)}\text{ }\forall\text{ }{d=1,2,\dots,g}
\end{aligned}
\end{equation}
\medskip

Введемо до розгляду надалі вектор функцій належності $j$-ого нейрону $d$-ого виходу $m$-ого каскаду:

\begin{equation}
\begin{aligned}
\mu_{d}^{[m]j}\left(k\right)=\biggl(&\mu_{d11}^{[m]j}\left(x_1\left(k\right)\right),\dots,\mu_{dh1}^{[m]j}\left(x_1\left(k\right)\right),\mu_{d12}^{[m]j}\left(x_2\left(k\right)\right),\\
&\dots,\mu_{dh2}^{[m]j}\left(x_2\left(k\right)\right),\dots,\mu_{dli}^{[m]j}\left(x_i\left(k\right)\right),\dots,\mu_{dhn}^{[m]j}\left(x_n\left(k\right)\right),\\
&\dots,\mu_{d1,n+1}^{[m]j}\left(\hat{y}^{*[1]}\left(k\right)\right),\dots,\mu_{dh,n+m-1}^{[m]j}\left(\hat{y}^{*[m-1]}\left(k\right)\right)\biggr)^T
\end{aligned}
\end{equation}
\medskip

та відповідний йому вектор синаптичних вагових коефіцієнтів

\begin{equation}
\begin{aligned}
w_{d}^{[m]j}=\biggl(&w_{d11}^{[m]j},\dots,w_{dh1}^{[m]j},w_{d12}^{[m]j},\dots,w_{dh2}^{[m]j},\dots,w_{dli}^{[m]j},\\
&\dots,w_{dhn}^{[m]j},w_{d1,n+1}^{[m]j},\dots,w_{dh,n+m-1}^{[m]j}\biggr)^T,
\end{aligned}
\end{equation}
\medskip

щоб записати вихідний сигнал системи у компактній формі:

\begin{equation}
\hat{y}_d^{[m]j}\left(k\right)=\left(w_d^{[m]j}\right)^T\mu_d^{[m]j}\left(k\right).
\end{equation}
\medskip

Для навчання нео-фаззі нейронів може бути використаний будь-який з методів адаптивної ідентифікації, що ми пропонували використовувати для навчання вузлів одновимірної нео-фаззі системи у першому розділі. Так корегувати вагові кофіцієнти можна за допомогою експоненційно зваженого рекурентного методу найменших квадратів:

\begin{equation}\label{eq:ExpWeightedRecurrentLeastSquaresLearning}
\begin{cases}
w_d^{[m]j}\left(k+1\right)=w_d^{[m]j}\left(k\right)+\\
+\frac{
P_d^{[m]j}\left(k\right)\left(y^d\left(k+1\right)-\left(w_d^{[m]j\left(k\right)}\right)^T\mu_d^{[m]j}\left(k+1\right)\right)}{\alpha+\left(\mu_d^{[m]j}\left(k+1\right)\right)^{T}P_d^{[m]j}\left(k\right)\mu_d^{[m]j}\left(k+1\right)
}\mu_d^{[m]j}\left(k+1\right),\\
P_d^{[m]j}\left(k+1\right)=\frac{1}{\alpha}\left(P_d^{[m]j}\left(k\right)-\frac{P_d^{[m]j}\left(k\right)\mu_d^{[m]j}\left(k+1\right)\left(\mu_d^{[m]j}\left(k+1\right)\right)^{T}P_d^{[m]j}\left(k\right)
}{\alpha+\left(\mu_d^{[m]j}\left(k+1\right)\right)^{T}P_d^{[m]j}\left(k\right)\mu_d^{[m]j}\left(k+1\right)
}\right),
\end{cases}
\end{equation}
\medskip

де $y^d\left(k+1\right),d=1,2,\dots,g$ -- зовнішній навчальний сигнал,  

$0<\alpha \leq 1$ -- фактор забування;

або градієнтного методу навчання, що, як зазначалося, відрізняється як згладжувальними, так і слідкуючими властивостями:

\begin{equation}\label{eq:GradientLearning}
\begin{aligned}
\begin{cases}
w_d^{[m]j}\left(k+1\right)&=w_d^{[m]j}\left(k\right)+\frac{y^d\left(k+1\right)-\left(w_d^{[m]j}\left(k\right)\right)^{T}\mu_d^{[m]j}\left(k+1\right)
}{r_d^{[m]j}\left(k+1\right)}\mu_d^{[m]j}\left(k+1\right),\\
r_d^{[m]j}\left(k+1\right)&=\alpha r_d^{[m]j}+\left\|\mu_d^{[m]j}\left(k+1\right)\right\|^{2},0\leq \alpha \leq 1.
\end{cases}
\end{aligned}
\end{equation}
\medskip
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Оптимізація пулу нео-фаззі нейронів багатовимірної каскадної системи, що еволюціонує}

Оскільки за мету було поставлено синтез такої багатовимірної каскадної системи, що б могла працювати саме в режимі реального часу, було б дуже доречно, якби система могла самостійно визначати найліпшу кількість функцій належності та їх форму, адже ці параметри також можуть змінюватися у часі. Тому у цьому підрозділі пропонується у кожному каскаді збільшити кількість нео-фаззі нейронів до такої, що є кратною (а не дорівнює, як пропонувалося у попередньому підрозділі) розмірності вектору вихідного сигналу та ввести узагальнюючі нейрони, що для пулу кожного каскаду визначатимуть локально оптимальні вихідні сигнали (тут під <<локально оптимальним вихідними сигналами>> слід розуміти сигнали, оптимальні у конкретний поточний момент часу). Таким чином, коли $g$ -- розмірність вихідного векторного сигналу, а $z$ -- кількість відмінних типів нейронів (що відрізняються за кількістю чи характером функцій належності) системи, у пулі першого каскаду знаходиться $zg$ нео-фаззі нейронів та $g$ нейронів-узагальнювачів $GN_d^{[1]}$, пул другого каскаду містить $z\left(g+1\right)$ нейронів та $g+1$ нейронів $GN_d^{[2]}$, останній каскад - $z\left(g+m-1\right)$ нейронів та $g+m-1$ нейронів $GN_d^{[m]}$.


\begin{figure}
\begin{center}
\includegraphics[width=16cm]{MIMOCcascadeNetworkBuiltOnNFNOptimized.eps}
\caption{Архітектура гібридної оптимізованої MIMO системи, побудованої на нео-фаззі нейронах}
\label{fig:MIMOCcascadeNetworkBuiltOnNFNOptimized}
\end{center}
\end{figure}

Схему такої оптимізованої MIMO (Multiple Input Multiple Output) архітектури зображено на рис.~\ref{fig:MIMOCcascadeNetworkBuiltOnNFN}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Метод визначення локально оптимальних вихідних сигналів пулу нео-фаззі нейронів багатовимірної каскадної системи, що еволюціонує}

Вихідні сигнали нейронів пулу кожного каскаду пропонується об'єднати узагальнюючим нейроном $GN^{[m]}$, що його було введено у розділі \ref{sec:NeuronPoolOptimisation}.

Таким чином, у кожному каскаді системи маємо $g$ $GN_d^{[m]}$ елементів, що узагальнюють вихідні сигнали нейронів пулу для кожного елементу вихідного вектору:

\begin{equation}
\hat{y}^{*[m]}\left(k\right)=\left(\hat{y}_{1}^{[m]}\left(k\right),\hat{y}_{2}^{[m]}\left(k\right),\dots,\hat{y}_{q}^{[m]}\left(k\right)\right)^T;
\end{equation}
\medskip

До першого узагальнюючого елементу першого каскаду $GN_1^{[1]}$ подаються сигнали 

\begin{equation}
\left(\hat{y}_1^{[1]}\left(k\right), \hat{y}_{g+1}^{[1]}\left(k\right),\dots,\hat{y}_{2g+1}^{[1]}\left(t\right),\dots\hat{y}_{\left(z-1\right)\left(g+1\right)}^{[1]}\left(k\right)\right)^T
\end{equation}
\medskip

до другого узагальнювача $GN_2^{[1]}$:

\begin{equation}
\left(\hat{y}_2^{[1]}\left(k\right), \hat{y}_{g+2}^{[1]}\left(k\right),\dots,\hat{y}_{2g+2}^{[1]}\left(t\right),\dots\hat{y}_{\left(z-1\right)\left(g+2\right)}^{[1]}\left(k\right)\right)^T
\end{equation}
\medskip

і, нарешті, вектор вхідних сигналів останнього узагальнюючого елементу першого каскаду $GN_{g}^{[1]}$: 

\begin{equation}
\left(\hat{y}_g^{[1]}\left(k\right), \hat{y}_{2g}^{[1]}\left(k\right),\dots,\hat{y}_{\left(z-1\right)g}^{[1]}\left(k\right)\right)^T.
\end{equation}
\medskip

Нагадаємо, що точність вихідного сигналу узагальнюючих елементів має бути не гіршою за точність будь-якого сигналу, що узагальнюється (подається на вхід до $GN_d^{[m]}$).
Рекурентна форма методу навчання <<на ковзному вікні>> елементів $GN_d^{[m]}$ кожного каскаду має вигляд

\begin{equation}\label{eq:GeneralizedOutputRecurrent}
\begin{cases}
\begin{aligned}
\tilde{P}_d^{[m]}\left(k+1\right)=&P_d^{[m]}\left(k\right)-\frac{P_d^{[m]}\left(k\right)\hat{y}_d^{[m]}\left(k+1\right)\hat{y}_d^{[m]T}\left(k+1\right)P_d^{[m]}\left(k\right)}{1+\hat{y}_d^{[m]T}\left(k+1\right)P_d^{[m]}\left(k\right)\hat{y}_d^{[m]}\left(k+1\right)},\\
P_d^{[m]}\left(k+1\right)=&\tilde{P}_d^{[m]}\left(k+1\right)+\\
&+\frac{\tilde{P}_d^{[m]}\left(k+1\right)\hat{y}_d\left(k-s+1\right)\hat{y}_d^{[m]T}\left(k-s+1\right)\tilde{P}_d^{[m]}\left(k+1\right)}{1-\hat{y}_d^{[m]T}\left(k-s+1\right)\tilde{P}_d^{[m]}\left(k+1\right)\hat{y}_d^{[m]}\left(k-s+1\right)},\\
\hat{y}_d^{*[m]}\left(k+1\right)=&\frac{\hat{y}_d^{[m]T}\left(k+1\right)P_d^{[m]}\left(k+1\right)E}{E^TP_d^{[m]}\left(k+1\right)E},
\end{aligned}
\end{cases}
\end{equation}
\medskip

а у випадку, коли $s=1$:

\begin{equation}
\begin{aligned}
\hat{y}_d^{*[m]}\left(k+1\right)&=\frac{\hat{y}_d^{[m]T}\left(k+1\right)\hat{y}_d^{[m]}\left(k+1\right)}{E^T\hat{y}_d^{[m]}\left(k+1\right)}=\\
&=\frac{\left\|\hat{y}_d^{[m]}\left(k+1\right)\right\|^2}{E^T\hat{y}_d^{[m]}\left(k+1\right)}=\\
&=\frac{\sum\limits_{j=1}^q{\left(\hat{y}_d^{[m]}\left(k+1\right)\right)^2}}{\sum\limits_{j=1}^q{\hat{y}_d^{[m]}\left(k+1\right)}}.
\end{aligned}
\end{equation}
\medskip

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Багатовимірна каскадна система, що еволюціонує, побудована на багатовимірних нео-фаззі нейронах}
\label{sec:MIMOEvolvingCascadedSystemBuiltOnMNFNs}

\begin{figure}
\begin{center}
\includegraphics[width=13cm]{MIMOBuiltOnNFNProblem.eps}
\caption{Iлюстрація надмірності MIMO системи, побудованої на нео-фаззі нейронах}
\label{fig:MIMOBuiltOnNFNProblem}
\end{center}
\end{figure}

Архітектура багатовимірної каскадної системи, яка ґрунтується на звичайних нео-фаззі нейронах, що її описано у підрозділі \ref{sec:MIMOEvolvingCascadedSystemBuiltOnNFNs}, є надмірною, адже вектор вхідних сигналів $x\left(k\right)$ (для першого каскаду) подається на однотипні нелінійні синапси $NS_{di}^{[1]j}$ нео-фаззі нейронів, кожен з яких на виході генерує сигнал $\hat{y}_d^{[1]j}\left(k\right),d=1,2,\dots,g$. У результаті компоненти вихідного вектора 

\begin{equation}
\hat{y}^{[1]j}\left(k\right)=\left(\hat{y}_1^{[1]j}\left(k\right), \hat{y}_2^{[1]j}\left(k\right),\dots,\hat{y}_g^{[1]j}\left(k\right)\right)^{T}
\end{equation}
\medskip

обчислюються незалежно один від одного, хоча при цьому

\begin{equation}
\mu_{1il}\left(x_i\left(k\right)\right)=\mu_{2il}\left(x_i\left(k\right)\right)=\mu_{jil}\left(x_i\left(k\right)\right)=\mu_{nil}\left(x_i\left(k\right)\right).
\end{equation}
\medskip

Надмірність архітектури, що її наведено на рис.~\ref{fig:MIMOCcascadeNetworkBuiltOnNFN}, проілюстрована на рис.~\ref{fig:MIMOBuiltOnNFNProblem}, де зеленим кольором позначені неодноразово обчислювані тотожні значення функцій належності $\mu_{111}$ та $\mu_{j11}$, червоним кольором -- тотожні $\mu_{12n}$ та $\mu_{j2n}$. Уникнути цього можна, якщо ввести до розгляду багатовимірний нео-фаззі нейрон, що є модифікацією систем, запропонованих у \cite{ref63, ref55}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Багатовимірний нео-фаззі нейрон}
\label{sec:MultidimentionalNeoFuzzyNeuron}
Вузлами багатовимірного нео-фаззі нейрону MNFN (cхема наведена на рис.~\ref{fig:MNFN}) є складені нелінійні синапси $MNS_i^{[1]j}$, кожен з яких містить $h$ функцій належності $\mu_{li}^{[1]j}$ та $gh$ настроюваних синаптичних вагових коефіцієнтів, але тільки $hn$ функцій належності, що в $g$ разів менше, ніж у випадку, коли каскад сформований із звичайних нео-фаззі нейронів.

\begin{figure}
\begin{center}
\includegraphics[width=12cm]{MNFN.eps}
\caption{Багатовимірний нео-фаззі нейрон}
\label{fig:MNFN}
\end{center}
\end{figure}

Введемо надалі до розгляду $\left(hn \times 1\right)$ - вектор функцій належності

\begin{equation}
\begin{aligned}
\mu^{[1]j}\left(k\right)=\biggl(\mu_{11}^{[1]j}\left(x_1\left(k\right)\right),\mu_{21}^{[1]j}\left(x_1\left(k\right)\right),\dots,\mu_{h1}^{[1]j}\left(x_1\left(k\right)\right),\\
\dots,\mu_{hn}^{[1]j}\left(x_n\left(k\right)\right)\biggr)^{T}
\end{aligned}
\end{equation}
\medskip

та $\left(g \times hn\right)$ - матрицю синаптичних вагових коефіцієнтів

\begin{equation}
W^{[1]j}=\left(
\begin{matrix}    w_{111}^{[1]j}&w_{112}^{[1]j}&\dots&w_{1li}^{[1]j}&\dots&w_{1hn}^{[1]j}\\
 w_{211}^{[1]j}&w_{212}^{[1]j}&\dots&w_{2li}^{[1]j}&\dots&w_{2hn}^{[1]j}\\ 
    \vdots&\vdots&&\vdots&&\vdots\\    w_{g11}^{[1]j}&w_{g12}^{[1]j}&\dots&w_{gli}^{[1]j}&\dots&w_{ghn}^{[1]j}\\
\end{matrix}
\right)
\end{equation}
\medskip

і запишемо сигнал на виході $MN_j^{[1]}$ у $k$-й момент часу у вигляді

\begin{equation}
\hat{y}^{[1]j}\left(k\right)=W^{[1]j}\mu^{[1]j}\left(k\right).
\end{equation}
\medskip

Навчання багатовимірного нео-фаззі нейрону можна реалізувати за допомогою матричної модифікації експоненційно-зваженого рекурентного методу найменших квадратів \eqref{eq:ExpWeightedRecurrentLeastSquaresLearning} у формі

\begin{equation}
\begin{aligned}
\begin{cases}
W^{[1]j}\left(k+1\right)&=W^{[1]j}\left(k\right)+\\
&+\frac{\left(y\left(k+1\right)-W^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)\right)
\left(\mu^{[1]j}\left(k+1\right)\right)^{T}P^{[1]j}\left(k\right)}{\alpha+\left(\mu^{[1]j\left(k+1\right)}\right)^{T}P^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)
},\\
P^{[1]j}\left(k+1\right)&=\frac{1}{\alpha}\left(P^{[1]j}\left(k\right)-\frac{P^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)\left(\mu^{[1]j}\left(k+1\right)\right)^{T}P^{[1]j}\left(k\right)
}{\alpha+\left(\mu^{[1]j}\left(k+1\right)\right)^{T}P^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)
}\right),\\
&0 < \alpha \leq 1
\end{cases}
\end{aligned}
\end{equation}
\medskip

або багатовимірного варіанту методу \eqref{eq:GradientLearning}

\begin{equation}
\begin{aligned}
\begin{cases}
W^{[1]j}\left(k+1\right)&= W^{[1]j}\left(k\right) + \frac{y\left(k+1\right)-W^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)}{r^{[1]j}\left(k+1\right)}\times\\
&\times\left(\mu^{[1]j}\left(k+1\right)\right)^{T},\\
r^{[1]j}\left(k+1\right)&=\alpha r^{[1]j}\left(k\right)+\left\|\mu^{[1]j}\left(k+1\right)\right\|^{2},\\
&0 \leq \alpha \leq 1,
\end{cases}
\end{aligned}
\end{equation}
\medskip

де $y\left(k+1\right)=\left(y^{1}\left(k+1\right),y^{2}\left(k+1\right),\dots, y^{g}\left(k+1\right)\right)^{T}$.

Аналогічним чином проводиться навчання інших каскадів, при цьому вектор функцій належності $m$-го каскаду $\mu^{[m]j}\left(k+1\right)$ збільшує свою розмірність на $\left(m-1\right)g$ компоненти, що їх утворили виходи попередніх каскадів.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Метод визначення локально оптимального вихідного сигналу пулу багатовимірних нео-фаззі нейронів каскадної системи, що еволюціонує}

У цьому підрозділі запропоновано узагальнюючий нейрон $GMN^{[m]}$ та рекурентний метод його навчання, який б об'єднував усі вихідні сигнали нейронів $MNFN^{[m]}$ пулу каскаду у вихідний сигнал

\begin{equation}
\hat{y}^{*[m]}\left(k\right)=\left(\hat{y}_1^{*[m]}\left(k\right), \hat{y}_2^{*[m]}\left(k\right),\dots, \hat{y}_g^{*[m]}\left(k\right)\right)^{T}
\end{equation}
\medskip

з точністю не меншою від точності будь-якого з сигналів $\hat{y}_j^{[m]}\left(k\right)$.

Розв'язати це завдання можна, знову скориставшись апаратом невизначених множників Лагранжа та адаптивного багатовимірного узагальненого прогнозування \cite{ref63}.

Введемо до розгляду вихідний сигнал нейрону $GMN^{[m]}$ у вигляді

\begin{equation}
\hat{y}^{*[m]}\left(k\right)=\sum\limits_{j=1}^{q}{c_j^{[m]}\hat{y}_j^{[m]}\left(k\right)}=\hat{y}^{[m]}\left(k\right)c^{[m]},
\end{equation}
\medskip

де $\hat{y}^{[m]}\left(k\right)=\left(\hat{y}_1^{[m]}\left(k\right), \hat{y}_2^{[m]}\left(k\right),\dots,\hat{y}_q^{[m]}\left(k\right)\right)^{T}$-- $\left(g \times q\right)$-матриця

$c^{[m]}$ -- $\left(q \times 1\right)$-вектор коефіцієнтів узагальнення, що відповідають умовам незміщенності

\begin{equation}
\label{eq:MIMOGeneralizingNeuronUnbiasenessConstraint}
\sum\limits_{j=1}^{q}{c_j^{[m]}}=E^{T}c^{[m]}=1,
\end{equation}
\medskip

$E=\left(1,1,\dots,1\right)^{T}$-- вектор, утворений одиницями.

Введемо критерій навчання

\begin{equation}
\begin{aligned}
E^{[m]}\left(k\right)&=\sum\limits_{\tau=1}^k\left\|y\left(\tau\right)-\hat{y}^{[m]}\left(\tau\right)c^{[m]}\right\|^2=\\
&=Tr\left(\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I\otimes c^{[m]}\right)^{T}\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I \otimes c^{[m]}\right)\right)
\end{aligned}
\end{equation}
\medskip

де $Y\left(k\right)=\left(y^T\left(1\right), y^T\left(2\right),\dots,y^T\left(k\right)\right)^{T}$-- $\left(k \times s\right)$ матриця спостережень,

\begin{equation}
\begin{aligned}
\hat{Y}^{[m]}\left(k\right)=\left(
\begin{matrix}
\hat{y}_1^{[m]T}\left(1\right)&\hat{y}_2^{[m]T}\left(1\right)&\dots&\hat{y}_q^{[m]T}\left(1\right)\\
\hat{y}_1^{[m]T}\left(2\right)&\hat{y}_2^{[m]T}\left(2\right)&\dots&\hat{y}_q^{[m]T}\left(2\right)\\
\vdots&\vdots&&\vdots\\
\hat{y}_1^{[m]T}\left(k\right)&\hat{y}_2^{[m]T}\left(k\right)&\dots&\hat{y}_q^{[m]T}\left(k\right)\\
\end{matrix}
\right),
\end{aligned}
\end{equation}
\medskip

$I$ -- одинична $\left(g \times g\right)$ матриця,

$\otimes$ -- символ тензорного добутку.

З урахуванням обмежень \eqref{eq:MIMOGeneralizingNeuronUnbiasenessConstraint} запишемо функцію Лагранжа

\begin{equation}
\begin{aligned}
L^{[m]}\left(k\right)&=E^{[m]}\left(k\right)+\lambda\left(E^{T}c^{[m]}-1\right)=\\
&=\sum\limits_{\tau=1}^{k}\left\|y\left(\tau\right)-\hat{y}^{[m]}\left(\tau\right)c^{[m]}\right\|^2+\lambda\left(E^Tc^{[m]}-1\right)=\\
&=Tr\left(\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I\otimes c^{[m]}\right)^T\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I\otimes c^{[m]}\right)\right)=\\
&+\lambda\left(E^Tc^{[m]}-1\right)=\\
&=Tr\left(V^{[m]T}\left(k\right)V^{[m]}\left(k\right)\right)+\lambda\left(E^Tc^{[m]}-1\right),
\end{aligned}
\end{equation}
\medskip

де $V^{[m]}\left(k\right)=Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I \otimes c^{[m]}$-- $\left(k \times g\right)$ матриця оновлень.

Розв'язання системи рівнянь Каруша-Куна-Таккера

\begin{equation}
\begin{cases}
\nabla_{c^{[m]}}L^{[m]}\left(k\right)=\overrightarrow{0},\\
\frac{\partial L^{[m]}\left(k\right)}{\partial \lambda}=0
\end{cases}
\end{equation}
\medskip

призводить до очевидного результату

\begin{equation}
\begin{cases}
c^{[m]}=\left(R^{[m]}\left(k\right)\right)^{-1}E\left(E^T\left(R^{[m]}\left(k\right)\right)^{-1}\right)^{-1}\\
\lambda=-2E^T\left(R^{[m]}\left(k\right)\right)^{-1}E,
\end{cases}
\end{equation}
\medskip

де 

\begin{equation}
R^{[m]}\left(k\right)=V^{[m]T}\left(k\right)V^{[m]}\left(k\right).
\end{equation}
\medskip

Таким чином, можна організувати оптимальне об'єднання виходів усіх нейронів пулу кожного каскаду. Зрозуміло, що в якості таких нейронів можуть використовуватися не тільки багатовимірні нео-фаззі нейрони, але й будь-які інші конструкції, що реалізують нелінійне відображення $R^{n+\left(m-1\right)g} \rightarrow R^g$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Висновки до розділу~\ref{ch:MIMOEvolvingCascadedSystem}}

\begin{enumerate}
\item Розглянута задача апроксимації та екстраполяції багатовимірних часових рядів за умови апріорної і поточної структурної та параметричної невизначеності; проаналізовані існуючі гібрідні системи обчислювального інтелекту, що використовуються для вирішення задач прогнозування та індентифікацїї багатовимірних даних у пакетному режимі; сформовані вимоги та обмеження до шуканої гібридної системи, здатної реалізувати нелінійне відображення $R^{n}\rightarrow R^{g}$ у режимі реального часу.
\item Зсинтезовано каскадну архітектуру системи, що ґрунтується на нео-фаззі нейронах, здатну реалізувати нелінійне відображення $R^{n}\rightarrow R^{g}$ у режимі послідовного обробляння даних.
\item Запропоновано архітектуру багатовимірного нео-фаззі нейрона та метод його навчання, що забезпечують підвищену швидкість налаштування синаптичних ваг та додаткові згладжуючі властивості.
\item Запропоновано архітектуру та рекурентний метод навчання багатовимірного узагальнюючого елементу, що в режимі реального часу реалізує оптимальне об'єднання багатовимірних вихідних сигналів нейронів пулу каскаду.
\item Запропоновано MIMO архітектуру та методи навчання гібридної каскадної нейронної мережі з оптимізацією пулу багатовимірних нейронів у кожному каскаді, що реалізують оптимальний за точністю прогноз нелінійних стохастичних і хаотичних сигналів у онлайн режимі.
\end{enumerate}