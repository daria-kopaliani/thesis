\chapter{Багатовимірна каскадна нео-фаззі система, що еволюціонує}
\label{ch:MIMOEvolvingCascadedSystem}

Задача апроксимації та екстраполяції багатовимірних часових рядів доволі часто виникає у багатьох технічних, медико-біологічних та інших дослідженнях, де якість прийнятих рішень істотно залежить від точності синтезованих прогнозів. У багатьох реальних задачах часові ряди характеризуються високим рівнем нелінійності та нестаціонарності своїх параметрів, наявністю аномальних викидів. Зрозуміло, що традиційні методи аналізу часових рядів, засновані на регресійному, кореляційному та інших подібних підходах, що мають на меті апріорну наявність доволі великої вибірки спостережень, є неефективними. Альтернативою традиційним статистичним методам може слугувати математичний апарат обчислювального інтелекту, зокрема штучні нейронні мережі [1, 2] та нейро-фаззі-системи [3], завдяки своїм універсальним апроксимувальним властивостям. Водночас з апроксимувальних властивостей зовсім не витікають екстраполюючі, оскільки врахування давньої передісторії для побудови прогнозувальної моделі може погіршити якість прогнозу. У зв'язку з цим під час оброблення нестаціонарних процесів треба відмовитися від процедур навчання, що базуються на зворотному поширенні помилок (багатошарові персептрони, рекурентні нейронні мережі, адаптивні нейромережеві системи нечіткого виведення – ANFIS) або методі найменших квадратів (радіально-базисні та функціонально пов’язані нейронні мережі) та скористатися процедурами на основі локальних критеріїв та «короткої» пам’яті типу алгоритма Качмажа-Уідроу-Хоффа. При цьому використані алгоритми навчання мусять забезпечувати не лише високу швидкодію, але й фільтруючі якості для придушення стохастичної «шумової» компоненти в оброблюваному сигналі. У зв’язку з цим синтез спеціалізованих гібридних систем обчислювального інтелекту для розв’язання задач прогнозування істотно нестаціонарних часових рядів за умов невизначеності, що забезпечують разом з високою швидкістю навчання і фільтрацію завад, є досить цікавою та перспективною задачею.

Таким чином, цей розділ присвячено синтезу багатовимірної гібридної системи обчислювального інтелекту, що здатна реалізувати нелінійне відображення $R^n \rightarrow R^g$ у режимі реального часу.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Багатовимірна каскадна система, що еволюціонує, побудована на нео-фаззі нейронах}\label{ch:MIMOEvolvingCascadedSystem}
\label{sec:MIMOEvolvingCascadedSystemBuiltOnNFNs}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Для вирішеня задачі прогнозування та ідентифікації багатовимірних даних в умовах апріорної і поточної структурної та параметричної невизначеності як ніколи доречні переваги каскадно-кореляційної архітектури, адже системи з такою архітектурою успадковують всі переваги елементів, які використовуються в їх вузлах, а в процесі навчання автоматично підбирається необхідна кількість каскадів для того, щоб отримати модель адекватної складності для вирішення поставленого завдання \hl{[12]}. Однак, слід зазначити, що каскадно-кореляційна мережа у формі, що її запропонували С. Фальман і К. Ліб'єр, є системою з одним виходом, тобто не здатна реалізувати нелінійне відображення $R^n \rightarrow R^g$. Це досить серйозне обмеження, оскільки більшість практичних завдань містять кілька вихідних сигналів. Тож пропонуймо такі модифікацїї до архітектури каскаодно-кореляційної мережі CasCorLA:   
\begin{enumerate}
\item замість елементарних персептронів Розенблата використовувати нео-фаззі нейрони (доцільність такого рішення було детально показано у розділі~\ref{ch:CascadedNeoFuzzySystemWithPoolOptimization}),
\item кількість нейронів у кожному каскаді відтепер має дорівнювати розмірності вектору вихідного сигналу системи.
\end{enumerate}

\begin{figure}
\begin{center}
\includegraphics[width=15cm]{MIMOCcascadeNetworkBuiltOnNFN.eps}
\caption{Архітектура гібридної MIMO системи, побудованої на нео-фаззі нейронах}
\label{fig:MIMOCcascadeNetworkBuiltOnNFN}
\end{center}
\end{figure}

Схему пропонованої архіткетури наведено на рис.~\ref{fig:MIMOCcascadeNetworkBuiltOnNFN}.

Тоді вихідний сигнал системи формується з векторів, що його складають вихідні сигнлани кращих нейронів останнього каскаду:

\begin{equation}
\hat{y}\left(k\right) = \left(\hat{y}_1^{*[m]}\left(k\right), \hat{y}_2^{*[m]}\left(k\right),\dots,\hat{y}_g^{*[m]}\left(k\right)\right)^T,  
\end{equation}
\medskip

де $g$ - кількість елементів вихідного вектору даних, що іх треба спрогнозувати чи ідентифікувати.\\
Для кожного з нео-фаззі нейронів системи в якості функцій належності можна використовувати трикутні конструкції:

\begin{equation}
\mu_{jli}^{[1]}\left(x_i\right)=
\begin{cases}
\frac{x_i-c_{d,l-1,i}^{[1]j}}{c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}\text { якщо } x_i\in\left[c_{d,l-1,i}^{[1]j},c_{dli}^{[1]j}\right],\\
\frac{c_{d,l+1,i}^{[1]j}-x_i}{c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}\text{ якщо }x_i \in \left[c_{dli}^{[1]j},c_{d,l+1,i}^{[1]j}\right],\\
0\text { у протилежному випадку},
\end{cases}
\end{equation}
\medskip

кубічні сплайни:

\begin{equation}
\mu_{jli}^{[1]}\left(x_i\right)=
\begin{cases}
\frac{1}{4}\left(2+3\frac{2x_i-c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}{c_{dli1}^{[1]j}-c_{d,l-1,i}^{[1]j}}-\left(\frac{2x_i-c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}{c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}\right)^3\right),\\
\text{якщо }x\in\left[c_{d,l-1,i}^{[1]j},c_{dli}^{[1]j}\right],\\
\frac{1}{4}\left(2-3\frac{2x_i-c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}{c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}+\left(\frac{2x_i-c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}{c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}\right)^3\right),\\
\text{якщо }x\in\left[c_{dli}^{[1]j},c_{d,l+1,i}^{[1]j}\right],\\
0\text{ у протилежному випадку},
\end{cases}
\end{equation}

або $B$-сплайни:

\begin{equation}
\mu_{jli}^{g[1]}=
\begin{cases}
\begin{rcases}
1\text{ якщо }x_{i}\in \left[c_{dli}^{[1]j},c_{d,l+1,i}^{[1]j}\right],\\
0\text{ у протилежному випадку}
\end{rcases}
\text{ якщо }g=1,\\
\frac{x_i-c_{dli}^{[1]j}}{c_{d,l+g-1,i}^{[1]j}-c_{dli}^{[1]j}}\mu_{dli}^{g-1,[1]j}\left(x_i\right)+\frac{c_{d,l+g,i}^{[1]j}-x_i}{c_{d,l+g,i}^{[1]j}-c_{d,l+g,i}^{[1]j}}\mu_{d,l+1,i}^{g-1,[1]j}\left(x_i\right),\\
\text{ якщо }g>1,
\end{cases}
\end{equation}
\medskip

де $\mu_{dli}^{g[1]j}\left(x_i\right)$ -- $l$-й сплайн $g$-ого порядку. Варто зауважити, що всі ці конструкції задовольняють умови одиничного розбиття Руспіні.

Запишемо вихідний сингнал $j$-ого нео-фаззі нейрону $d$-ого виходу першого каскаду у вигляді

\begin{equation}
\begin{cases}
\begin{aligned}

&\hat{y}_d^{[1]j}\left(k\right)=\sum\limits_{i=1}^{n}f_{di}^{[1]j}\left(x_i\left(k\right)\right)=
\sum\limits_{i=1}^{n}\sum\limits_{l=1}^{h}{w_{dli}^{[1]j}\mu_{dli}^{[1]j}\left(x_i\left(k\right)\right)},\\
&\text{ЯКЩО }x_i\left(k\right) \in X_{li}^{j}\text{ , ТОДІ ВИХІД }w_{dli}^{[1]j}.

\end{aligned}
\end{cases}
\end{equation}
\medskip

вихідні сигнали нео-фаззі нейронів другого каскаду:

\begin{equation}
\begin{aligned}
\hat{y}_d^{[2]j}=&\sum\limits_{i=1}^{n}\sum\limits_{l=1}^{h}{w_{dli}^{[2]j}\mu_{dli}^{[2]j}\left(x_i\right)}+\\
&\sum\limits_{d=1}^{g}\sum\limits_{l=1}^{h}{w_{dl,n+1}^{[2]j}\mu_{dl,n+1}^{[2]j}\left(\hat{y}_d^{*[1]}\right)}\text{ }\forall\text{ }{d=1,2,\dots,g}
\end{aligned}
\end{equation}
\medskip

вихідні сигнали $m$-ого каскаду:

\begin{equation}
\begin{aligned}
\hat{y}_d^{[2]j}=&\sum\limits_{i=1}^{n}\sum\limits_{l=1}^{h}{w_{dli}^{[2]j}\mu_{dli}^{[2]j}\left(x_i\right)}+\\
&\sum\limits_{d=1}^{g}\sum\limits_{p=n+1}^{n+m-1}\sum\limits_{l=1}^{h}{w_{dlp}^{[m]j}\mu_{dlp}^{[m]j}\left(\hat{y}_d^{*[p-n]}\right)}\text{ }\forall\text{ }{d=1,2,\dots,g}
\end{aligned}
\end{equation}
\medskip

Введемо до розгляду надалі вектор функцій належності $j$-ого нейрону $d$-ого виходу $m$-ого каскаду:

\begin{equation}
\begin{aligned}
\mu_{d}^{[m]j}\left(k\right)=\biggl(&\mu_{d11}^{[m]j}\left(x_1\left(k\right)\right),\dots,\mu_{dh1}^{[m]j}\left(x_1\left(k\right)\right),\mu_{d12}^{[m]j}\left(x_2\left(k\right)\right),\\
&\dots,\mu_{dh2}^{[m]j}\left(x_2\left(k\right)\right),\dots,\mu_{dli}^{[m]j}\left(x_i\left(k\right)\right),\dots,\mu_{dhn}^{[m]j}\left(x_n\left(k\right)\right),\\
&\dots,\mu_{d1,n+1}^{[m]j}\left(\hat{y}^{*[1]}\left(k\right)\right),\dots,\mu_{dh,n+m-1}^{[m]j}\left(\hat{y}^{*[m-1]}\left(k\right)\right)\biggr)^T
\end{aligned}
\end{equation}
\medskip

та відповідний йому вектор синаптичних вагових коефіцієнтів

\begin{equation}
\begin{aligned}
w_{d}^{[m]j}=\biggl(&w_{d11}^{[m]j},\dots,w_{dh1}^{[m]j},w_{d12}^{[m]j},\dots,w_{dh2}^{[m]j},\dots,w_{dli}^{[m]j},\\
&\dots,w_{dhn}^{[m]j},w_{d1,n+1}^{[m]j},\dots,w_{dh,n+m-1}^{[m]j},\biggr)^T,
\end{aligned}
\end{equation}
\medskip

щоб записати вихідний сигнал системи у компактній формі:

\begin{equation}
\hat{y}_d^{[m]j}\left(k\right)=\left(w_d^{[m]j}\right)^T\mu_d^{[m]j}\left(k\right).
\end{equation}
\medskip

Для навчання нео-фаззі нейронів може бути використаний будь-який з методів адаптивної ідентифікації, що ми пропонували використовувати для навчання вузлів одновимірної нео-фаззі системи у першому розділі. Так корегувати вагові кофіцієнти можна за допомгою експоненційно зваженого рекурентного методу найменших квадратів:

\begin{equation}\label{eq:ExpWeightedRecurrentLeastSquaresLearning}
\begin{cases}
w_d^{[m]j}\left(k+1\right)=w_d^{[m]j}\left(k\right)+\\
\frac{
P_d^{[m]j}\left(k\right)\left(y^d\left(k+1\right)-\left(w_d^{[m]j\left(k\right)}\right)^T\mu_d^{[m]j}\left(k+1\right)\right)}{\alpha+\left(\mu_d^{[m]j}\left(k+1\right)\right)^{T}P_d^{[m]j}\left(k\right)\mu_d^{[m]j}\left(k+1\right)
}\mu_d^{[m]j}\left(k+1\right),\\
P_d^{[m]j}\left(k+1\right)=\frac{1}{\alpha}\left(P_d^{[m]j}\left(k\right)-\frac{P_d^{[m]j}\left(k\right)\mu_d^{[m]j}\left(k+1\right)\left(\mu_d^{[m]j}\left(k+1\right)\right)^{T}P_d^{[m]j}\left(k\right)
}{\alpha+\left(\mu_d^{[m]j}\left(k+1\right)\right)^{T}P_d^{[m]j}\left(k\right)\mu_d^{[m]j}\left(k+1\right)
}\right),
\end{cases}
\end{equation}
\medskip

де $y^d\left(k+1\right),d=1,2,\dots,g$ -- зовнішній навчальний сигнал,  

$0<\alpha \leq 1$ -- фактор забування;

або градієнтний метод навчання, що, як зазначалося, відрізняється як згладжувальними, так і слідкуючими властивостями:

\begin{equation}\label{eq:GradientLearning}
\begin{aligned}
\begin{cases}
w_d^{[m]j}\left(k+1\right)&=w_d^{[m]j}\left(k\right)+\frac{y^d\left(k+1\right)-\left(w_d^{[m]j}\left(k\right)\right)^{T}\mu_d^{[m]j}\left(k+1\right)
}{r_d^{[m]j}\left(k+1\right)}\mu_d^{[m]j}\left(k+1\right),\\
r_d^{[m]j}\left(k+1\right)&=\alpha r_d^{[m]j}+\left\|\mu_d^{[m]j}\left(k+1\right)\right\|^{2},0\leq \alpha \leq 1.
\end{cases}
\end{aligned}
\end{equation}
\medskip
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Оптимізація пулу нео-фаззі нейронів багатовимірної каскадної системи, що еволюціонує}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Оскільки за мету було поставлено синтез такої багатовимірної каскадної системи, що б могла працювати саме в режимі реального часу, було б дуже доречно, якби система могла самостійно визначати найліпшу кількість функцій належності та їх форму, адже ці параметри також можуть змінюватися у часі. Тому у цьому підрозділі пропонується у кожному каскаді збільшити кількість нео-фаззі нейронів до такої, що є кратною (а не дорівнює, як пропоноувалося у попередньому підрозділі) розмірності вектору вихідного сигналу ввести узагальнюючі нейрони, що для пулу кожного каскаду визначатимуть локально оптимальні вихідні сигнали (тут під <<локально опатимальнми вихідними сигналами>> слід розуміти сингали, отипмальні у конкретний момент часу). Таким чином, коли $g$ -- розмірність вихідного векторного сигналу, а $z$ -- кількість відмінних типів нейронів (що відрізняються за кількістю чи характером функцій належності) системи, у пулі першого каскаду знаходиться $zg$ нео-фаззі нейронів та $g$ нейронів-узагальнювачів $GN_d^{[1]}$, пул другого каскаду містить $z\left(g+1\right)$ нейронів та $g+1$ нейронів $GN_d^{[2]}$, останній каскад - $z\left(g+m-1\right)$ нейронів та $g+m-1$ нейронів $GN_d^{[m]}$.


\begin{figure}
\begin{center}
\includegraphics[width=16cm]{MIMOCcascadeNetworkBuiltOnNFNOptimized.eps}
\caption{Архітектура гібридної оптимізованої MIMO системи, побудованої на нео-фаззі нейронах}
\label{fig:MIMOCcascadeNetworkBuiltOnNFNOptimized}
\end{center}
\end{figure}

Схему такої оптимізованої MIMO (Multiple Input Multiple Output) архіткетури зображено на рис.~\ref{fig:MIMOCcascadeNetworkBuiltOnNFN}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Метод визначення локально оптимальних вихідних сигналів пулу нео-фаззі нейронів багатовимірної каскадної системи, що еволюціонує}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Вихідні сигнали нейронів пулу кожного каскаду пропонується об'єднати узагальнюючим нейроном $GN^{[m]}$, що його було введено у \hl{???} розділі.

Таким чином, у кожному каскаді системи маємо $g$ $GN_d^{[m]}$ елементів, що узагальнюють вихідні сигнали нейронів пулу для кожного елементу вихідного вектору:

\begin{equation}
\hat{y}^{*[m]}\left(k\right)=\left(\hat{y}_{1}^{[m]}\left(k\right),\hat{y}_{2}^{[m]}\left(k\right),\dots,\hat{y}_{q}^{[m]}\left(k\right)\right)^T;
\end{equation}
\medskip

До першого узагальнюючого елементу першого каскаду $GN_1^{[1]}$ подаються сигнали 

\begin{equation}
\left(\hat{y}_1^{[1]}\left(k\right), \hat{y}_{g+1}^{[1]}\left(k\right),\dots,\hat{y}_{2g+1}^{[1]}\left(t\right),\dots\hat{y}_{\left(z-1\right)\left(g+1\right)}^{[1]}\left(k\right)\right)^T
\end{equation}
\medskip

до другого узагальнювача $GN_2^{[1]}$:

\begin{equation}
\left(\hat{y}_2^{[1]}\left(k\right), \hat{y}_{g+2}^{[1]}\left(k\right),\dots,\hat{y}_{2g+2}^{[1]}\left(t\right),\dots\hat{y}_{\left(z-1\right)\left(g+2\right)}^{[1]}\left(k\right)\right)^T
\end{equation}
\medskip

і, нарешті, вектор ухідних сигналів останнього узагальнюючого елементу першого каскаду $GN_{g}^{[1]}$: 

\begin{equation}
\left(\hat{y}_g^{[1]}\left(k\right), \hat{y}_{2g}^{[1]}\left(k\right),\dots,\hat{y}_{\left(z-1\right)g}^{[1]}\left(k\right)\right)^T
\end{equation}
\medskip

Нагадаємо, точність вихідного сигналу узагальнюючих елементів має бути не меншою від точності будь-якого сингналу, що узагальнюється (подається на вхід до $GN_d^{[m]}$).
Рекурентна форма методу навчання <<на ковзному вікні>> елементів $GN_d^{[m]}$ кожного каскаду має вигляд

\begin{equation}\label{eq:GeneralizedOutputRecurrent}
\begin{cases}
\begin{aligned}
\tilde{P}_d^{[m]}\left(k+1\right)=&P_d^{[m]}\left(k\right)-\frac{P_d^{[m]}\left(k\right)\hat{y}_d^{[m]}\left(k+1\right)\hat{y}_d^{[m]T}\left(k+1\right)P_d^{[m]}\left(k\right)}{1+\hat{y}_d^{[m]T}\left(k+1\right)P_d^{[m]}\left(k\right)\hat{y}_d^{[m]}\left(k+1\right)},\\
P_d^{[m]}\left(k+1\right)=&\tilde{P}_d^{[m]}\left(k+1\right)+\\
&\frac{\tilde{P}_d^{[m]}\left(k+1\right)\hat{y}_d\left(k-s+1\right)\hat{y}_d^{[m]T}\left(k-s+1\right)\tilde{P}_d^{[m]}\left(k+1\right)}{1-\hat{y}_d^{[m]T}\left(k-s+1\right)\tilde{P}_d^{[m]}\left(k+1\right)\hat{y}_d^{[m]}\left(k-s+1\right)},\\
\hat{y}_d^{*[m]}\left(k+1\right)=&\frac{\hat{y}_d^{[m]T}\left(k+1\right)P_d^{[m]}\left(k+1\right)E}{E^TP_d^{[m]}\left(k+1\right)E},
\end{aligned}
\end{cases}
\end{equation}
\medskip

а у випадку, коли $s=1$:

\begin{equation}
\begin{aligned}
\hat{y}_d^{*[m]}\left(k+1\right)&=\frac{\hat{y}_d^{[m]T}\left(k+1\right)\hat{y}_d^{[m]}\left(k+1\right)}{E^T\hat{y}_d^{[m]}\left(k+1\right)}\\
&=\frac{\left\|\hat{y}_d^{[m]}\left(k+1\right)\right\|^2}{E^T\hat{y}_d^{[m]}\left(k+1\right)}\\
&=\frac{\sum\limits_{j=1}^q{\left(\hat{y}_d^{[m]}\left(k+1\right)\right)^2}}{\sum\limits_{j=1}^q{\hat{y}_d^{[m]}\left(k+1\right)}}.
\end{aligned}
\end{equation}
\medskip
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Багатовимірна каскадна система, що еволюціонує, побудована на багатовимірних нео-фаззі нейронах}
\label{sec:MIMOEvolvingCascadedSystemBuiltOnMNFNs}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\begin{center}
\includegraphics[width=13cm]{MIMOBuiltOnNFNProblem.eps}
\caption{Iлюстрація надмірності MIMO системи, побудованої на NFN нейронах}
\label{fig:MIMOBuiltOnNFNProblem}
\end{center}
\end{figure}

Архітектура багатовимірної каскадної системи, яка ґрунтується на звичайних нео-фаззі нейронах, що її описано у підрозділі \ref{sec:MIMOEvolvingCascadedSystemBuiltOnNFNs}, є надмірною, адже вектор вхідних сигналів $x\left(k\right)$ (для першого каскаду) подається на однотипні нелінійні синапси $NS_{di}^{[1]j}$ нео-фаззі нейронів, кожен з яких на виході генерує сигнал $\hat{y}_d^{[1]j}\left(k\right),d=1,2,\dots,g$. У результаті компоненти вихідного вектора 

\begin{equation}
\hat{y}^{[1]j}\left(k\right)=\left(\hat{y}_1^{[1]j}\left(k\right), \hat{y}_2^{[1]j}\left(k\right),\dots,\hat{y}_g^{[1]j}\left(k\right)\right)^{T}
\end{equation}
\medskip

обчислюються незалежно один від одного, хоча, як проілюстровано на рис.~\ref{fig:MIMOBuiltOnNFNProblem}, $\mu_{1il}\left(x_i\left(k\right)\right)=\mu_{2il}\left(x_i\left(k\right)\right)=\mu_{jil}\left(x_i\left(k\right)\right)=\mu_{nil}\left(x_i\left(k\right)\right)$. Уникнути цього можна, якщо ввести до розгляду багатовимірний нео-фаззі нейрон, що є модифікацією систем, запропонованих у \hl{[??,??]}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Багатовимірний нео-фаззі нейрон}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Вузлами багатовимірного нео-фаззі нейрону MNFN (cхема наведена на на рис.~\ref{fig:MNFN}) є складені нелінійні синапси $MNS_i^{[1]j}$, кожен з яких має $h$ функцій належності $\mu_{li}^{[1]j}$ та $gh$ настроюваних синаптичних вагових коефіцієнтів, але тільки $hn$ функцій належності, що в $g$ разів менше, ніж у випадку, коли каскад сформований із звичайних нео-фаззі нейронів.

\begin{figure}
\begin{center}
\includegraphics[width=12cm]{MNFN.eps}
\caption{Багатовимірний нео-фаззі нейрон}
\label{fig:MNFN}
\end{center}
\end{figure}

Введемо надалі до розгляду $\left(hn \times 1\right)$ - вектор функцій належності

\begin{equation}
\begin{aligned}
\mu^{[1]j}\left(k\right)=\biggl(\mu_{11}^{[1]j}\left(x_1\left(k\right)\right),\mu_{21}^{[1]j}\left(x_1\left(k\right)\right),\dots,\mu_{h1}^{[1]j}\left(x_1\left(k\right)\right),\\
\dots,\mu_{hn}^{[1]j}\left(x_n\left(k\right)\right)\biggr)^{T}
\end{aligned}
\end{equation}
\medskip

та $\left(g \times hn\right)$ - матрицю синаптичних вагових коефіцієнтів

\begin{equation}
W^{[1]j}=\left(
\begin{matrix}    w_{111}^{[1]j}&w_{112}^{[1]j}&\dots&w_{1li}^{[1]j}&\dots&w_{1hn}^{[1]j}\\
 w_{211}^{[1]j}&w_{212}^{[1]j}&\dots&w_{2li}^{[1]j}&\dots&w_{2hn}^{[1]j}\\ 
    \vdots&\vdots&&\vdots&&\vdots\\    w_{g11}^{[1]j}&w_{g12}^{[1]j}&\dots&w_{gli}^{[1]j}&\dots&w_{ghn}^{[1]j}\\
\end{matrix}
\right),
\end{equation}
\medskip

і запишемо сигнал на виході $MN_j^{[1]}$ у $k$-й момент часу у вигляді

\begin{equation}
\hat{y}^{[1]j}\left(k\right)=W^{[1]j}\mu^{[1]j}\left(k\right).
\end{equation}
\medskip

Навчаняя багатовимірного нео-фаззі нейрону можна реалізувати за допомогою матричної модифікації експоненційно-зваженого рекурентного методу найменших квадратів \eqref{eq:ExpWeightedRecurrentLeastSquaresLearning} у формі

\begin{equation}
\begin{aligned}
\begin{cases}
W^{[1]j}\left(k+1\right)&=W^{[1]j}\left(k\right)+\\
&\frac{\left(y\left(k+1\right)-W^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)\right)
\left(\mu^{[1]j}\left(k+1\right)\right)^{T}P^{[1]j}\left(k\right)}{\alpha+\left(\mu^{[1]j\left(k+1\right)}\right)^{T}P^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)
},\\
P^{[1]j}\left(k+1\right)&=\frac{1}{\alpha}\left(P^{[1]j}\left(k\right)-\frac{P^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)\left(\mu^{[1]j}\left(k+1\right)\right)^{T}P^{[1]j}\left(k\right)
}{\alpha+\left(\mu^{[1]j}\left(k+1\right)\right)^{T}P^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)
}\right),\\
&0 < \alpha \leq 1
\end{cases}
\end{aligned}
\end{equation}
\medskip

або багатовимірного варіанту методу \eqref{eq:GradientLearning}

\begin{equation}
\begin{aligned}
\begin{cases}
W^{[1]j}\left(k+1\right)&= W^{[1]j}\left(k\right) + \frac{y\left(k+1\right)-W^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)}{r^{[1]j}\left(k+1\right)}\\
&\times\left(\mu^{[1]j}\left(k+1\right)\right)^{T},\\
r^{[1]j}\left(k+1\right)&=\alpha r^{[1]j}\left(k\right)+\left\|\mu^{[1]j}\left(k+1\right)\right\|^{2},\\
&0 \leq \alpha \leq 1,
\end{cases}
\end{aligned}
\end{equation}
\medskip

де $y\left(k+1\right)=\left(y^{1}\left(k+1\right),y^{2}\left(k+1\right),\dots, y^{g}\left(k+1\right)\right)^{T}$.

Аналогічним чином проводиться навчання інших каскадів, при цьому вектор функцій належності $m$-го каскаду $\mu^{[m]j}\left(k+1\right)$ збільшує свою розмірність на $\left(m-1\right)g$ компоненти, що їх утворили виходи попередніх каскадів.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Метод визначення локально оптимального вихідного сигналу пулу багатовимірних нео-фаззі нейронів каскадної системи, що еволюціонує}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

У цьому підрозділі запропоновано узагальнюючий нейрон $GMN^{[m]}$ та рекурентний метод його навчання, щоб він об'єднував усі вихідні сигнали нейронів $MNFN^{[m]}$ пулу каскаду у сигнал

\begin{equation}
\hat{y}^{*[m]}\left(k\right)=\left(\hat{y}_1^{*[m]}\left(k\right), \hat{y}_2^{*[m]}\left(k\right),\dots, \hat{y}_g^{*[m]}\left(k\right)\right)^{T}
\end{equation}
\medskip

з точністю не меншою від точності будь-якого з сигналів $\hat{y}_j^{[m]}\left(k\right)$.

Розв'язати це завдання можна, знову скориставшись апаратом невизначених множників Лагранжа та адаптивного багатовимірного узагальненого прогнозування% \cite{refU}.

Введемо до розгляду вихідний сигнал нейрону $GMN^{[m]}$ у вигляді

\begin{equation}
\hat{y}^{*[m]}\left(k\right)=\sum\limits_{j=1}^{q}{c_j^{[m]}\hat{y}_j^{[m]}\left(k\right)}=\hat{y}^{[m]}\left(k\right)c^{[m]},
\end{equation}
\medskip

де $\hat{y}^{[m]}\left(k\right)=\left(\hat{y}_1^{[m]}\left(k\right), \hat{y}_2^{[m]}\left(k\right),\dots,\hat{y}_q^{[m]}\left(k\right)\right)^{T}$-- $\left(g \times q\right)$-матриця

$c^{[m]}$-- $\left(q \times 1\right)$-вектор коефіцієнтів узагальнення, що відповідають умовам незміщенності

\begin{equation}
\sum\limits_{j=1}^{q}{c_j^{[m]}}=E^{T}c^{[m]}=1,
\end{equation}
\medskip

$E=\left(1,1,\dots,1\right)^{T}$-- вектор, утворений одиницями.

Введемо критерій навчання

\begin{equation}
\begin{aligned}
E^{[m]}\left(k\right)&=\sum\limits_{\tau=1}^k\left\|y\left(\tau\right)-\hat{y}^{[m]}\left(\tau\right)c^{[m]}\right\|^2\\
&=Tr\left(\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I\otimes c^{[m]}\right)^{T}\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I \otimes c^{[m]}\right)\right)
\end{aligned}
\end{equation}
\medskip

де $Y\left(k\right)=\left(y^T\left(1\right), y^T\left(2\right),\dots,y^T\left(k\right)\right)^{T}$-- $\left(k \times s\right)$ матриця спостережень,

\begin{equation}
\begin{aligned}
\hat{Y}^{[m]}\left(k\right)=\left(
\begin{matrix}
\hat{y}_1^{[m]T}\left(1\right)&\hat{y}_2^{[m]T}\left(1\right)&\dots&\hat{y}_q^{[m]T}\left(1\right)\\
\hat{y}_1^{[m]T}\left(2\right)&\hat{y}_2^{[m]T}\left(2\right)&\dots&\hat{y}_q^{[m]T}\left(2\right)\\
\vdots&\vdots&&\vdots\\
\hat{y}_1^{[m]T}\left(k\right)&\hat{y}_2^{[m]T}\left(k\right)&\dots&\hat{y}_q^{[m]T}\left(k\right)\\
\end{matrix}
\right),
\end{aligned}
\end{equation}
\medskip

де $I$ -- одинична $\left(g \times g\right)$ матриця,

$\otimes$ -- символ тензорного добутку.

З урахуванням ?? обмежень запишемо функцію Лагранжа

\begin{equation}
\begin{aligned}
L^{[m]}\left(k\right)&=E^{[m]}\left(k\right)+\lambda\left(E^{T}c^{[m]}-1\right)\\
&=\sum\limits_{\tau=1}^{k}\left\|y\left(\tau\right)-\hat{y}^{[m]}\left(\tau\right)c^{[m]}\right\|^2+\lambda\left(E^Tc^{[m]}-1\right)\\
&=Tr\left(\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I\otimes c^{[m]}\right)^T\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I\otimes c^{[m]}\right)\right)\\
&+\lambda\left(E^Tc^{[m]}-1\right)\\
&=Tr\left(V^{[m]T}\left(k\right)V^{[m]}\left(k\right)\right)+\lambda\left(E^Tc^{[m]}-1\right),
\end{aligned}
\end{equation}
\medskip

де $V^{[m]}\left(k\right)=Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I \otimes c^{[m]}$-- $\left(k \times g\right)$ матриця оновлень.

Розв'язання системи рівнянь Каруша-Куна-Таккера

\begin{equation}
\begin{cases}
\nabla_{c^{[m]}}L^{[m]}\left(k\right)=\overrightarrow{0},\\
\frac{\partial L^{[m]}\left(k\right)}{\partial \lambda}=0
\end{cases}
\end{equation}
\medskip

призводить до очевидного результату

\begin{equation}
\begin{cases}
c^{[m]}=\left(R^{[m]}\left(k\right)\right)^{-1}E\left(E^T\left(R^{[m]}\left(k\right)\right)^{-1}\right)^{-1}\\
\lambda=-2E^T\left(R^{[m]}\left(k\right)\right)^{-1}E,
\end{cases}
\end{equation}
\medskip

де $R^{[m]}\left(k\right)=V^{[m]T}\left(k\right)V^{[m]}\left(k\right)$.

Таким чином, можна організувати оптимальне об'єднання виходів усіх нейронів пулу кожного каскаду. Зрозуміло, що в якості таких нейронів можуть використовуватися не тільки багатовимірні нео-фаззі нейрони, але й будь-які інші конструкції, що реалізують нелінійне відображення \mbox{$R^{n+\left(m-1\right)g}\rightarrow R^g$}.