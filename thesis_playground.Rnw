\documentclass{vakthesis}
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian,ukrainian]{babel}
\usepackage{geometry}
\usepackage{mathtools}% http://ctan.org/pkg/mathtools
\usepackage{amsmath}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{color,soul}
\usepackage{graphicx}
\graphicspath{ {images/} }

%\geometry{hmargin={30mm,15mm},lines=29,vcentering}
\everymath=\expandafter{\the\everymath\displaystyle}

\geometry{a4paper, total={170mm,257mm}, left=20mm, top=20mm}
 
%\DeclareMathSizes{10}{10}{10}{10}
\begin{document}  
  \title{Еволюційні нейро-фаззі мережі з каскадною структурою для інтелектуального аналізу данних}
  \author{Копаліані Дар'я Сергіївна}
	\supervisor{Бодянський Євгеній Володимирович}{доктор технічних наук, професор}
	\speciality{05.13.23}
	\udc{004.032.26}
	\institution{Харківський національний університет радіоелектроніки}{Харків}
	\date{2015}
	
	\maketitle
	
	% Зміст
	\tableofcontents
	
  \newcommand{\V}[1]{\mathit{#1}}
  \let\originalleft\left
  \let\originalright\right
  \renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
  \renewcommand{\right}{\aftergroup\egroup\originalright}
  \renewcommand{\floatpagefraction}{.8}%
  \renewcommand{\topfraction}{.75}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Багатовимірна каскадна нео-фаззі система, що еволюціонує}
\label{ch:MIMOEvolvingCascadedSystem}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Задача прогнозування багатовимірних часових рядів доволі часто виникає у багатьох технічних, медико-біологічних та інших дослідженнях, де якість прийнятих рішень істотно залежить від точності синтезованих прогнозів.  У багатьох реальних задачах часові ряди характеризуються високим рівнем нелінійності та нестаціонарності своїх параметрів, наявністю аномальних викидів. Зрозуміло, що традиційні методи аналізу часових рядів, засновані на регресійному, кореляційному та інших подібних підходах, що мають на меті апріорну наявність доволі великої вибірки спостережень, є неефективними. Альтернативою традиційним статистичним методам може слугувати математичний апарат обчислювального інтелекту, а також штучні нейронні мережі [1, 2] та нейро-фаззі-системи [3], завдяки своїм універсальним апроксимувальним властивостям. Водночас з апроксимувальних властивостей зовсім не витікають екстраполюючі, оскільки врахування давньої передісторії для побудови прогнозувальної моделі може погіршити якість прогнозу. У зв'язку з цим під час оброблення нестаціонарних процесів треба відмовитися від процедур навчання, що базуються на зворотному поширенні помилок (багатошарові персептрони, рекурентні нейронні мережі, адаптивні нейромережеві системи нечіткого виведення – ANFIS) або методі найменших квадратів (радіально-базисні та функціонально пов’язані нейронні мережі) та скористатися процедурами на основі локальних критеріїв та «короткої» пам’яті типу алгоритма Качмажа-Уідроу-Хоффа. При цьому використані алгоритми навчання мусять забезпечувати не лише високу швидкодію, але й фільтруючі якості для придушення стохастичної «шумової» компоненти в оброблюваному сигналі. У зв’язку з цим синтез спеціалізованих гібридних систем обчислювального інтелекту для розв’язання задач прогнозування істотно нестаціонарних часових рядів за умов невизначеності, що забезпечують разом з високою швидкістю навчання і фільтрацію завад, є досить цікавою та перспективною задачею.

Таким чином, цей розділ присвячено сиснтесзу багатовимірної гібридної системи обчислювального інтелекту, що здатна реалізувати нелінійне відображення $R^n \rightarrow R^g$ у режимі реального часу.

\section{Багатовимірна каскадна система, що еволюціонує, побудована на нео-фаззі нейронах}\label{ch:MIMOEvolvingCascadedSystem}
\label{ch:MIMOEvolvingCascadedSystemBuiltOnNFNs}

Для вирішеня \ref{fig:NFN} задачі прогнозування та ідентифікації багатовимірних даних в умовах апріорної і поточної структурної та параметричної невизначеності як ніколи доречні переваги каскадно-кореляційної архітектури, адже системи з такою архітектурою успадковують всі переваги елементів, які використовуються в їх вузлах, а в процесі навчання автоматично підбирається необхідна кількість каскадів для того, щоб отримати модель адекватної складності для вирішення поставленого завдання \hl{[12]}.

\begin{figure}
\begin{center}
\includegraphics[height=18cm]{NFN.pdf}
\caption{This is a figure}
\label{fig:NFN}
\end{center}
\end{figure}

Однак, слід зазначити, що каскадно-кореляційна мережа у формі, що її запропонували С. Фальман і К. Лебьер, є системою з одним виходом, тобто не здатна реалізувати нелінійне відображення $R^n \rightarrow R^g$. Це досить серйозне обмеження, оскільки більшість практичних завдань містять кілька вихідних параметрів. Тож пропонуймо наступні модифікацїї до архітектури каскаодно-кореляційної мережі CasCorLA:   
\begin{enumerate}
\item замість елементарних персептронів Розенблата використовувати нео-фаззі нейрони (доцільність такого рішення було детально показано у першому розділі),
\item кількість нейронів у кожному каскаді відтепер має дорівнювати розмірності вектору вихідного сигналу системи.
\end{enumerate}

Тоді вихідний сигнал системи дорівнюватиме вектору, що його формують вихідні синглани кращих нейронів останнього каскаду:

\begin{equation}
\hat{y}\left(k\right) = \left(\hat{y}_1^{*[m]}\left(k\right), \hat{y}_2^{*[m]}\left(k\right),\dots,\hat{y}_g^{*[m]}\left(k\right)\right)^T,  
\end{equation}
\medskip

де $g$ - кількість елементів вихідного вектору даних, що іх треба спрогнозувати чи ідентифікувати.\\
Для кожного з нео-фаззі нейронів системи в якості функцій належності можна використовувати трикутні конструкції:

\begin{equation}
\mu_{jli}^{[1]}\left(x_i\right)=
\begin{cases}
\frac{x_i-c_{d,l-1,i}^{[1]j}}{c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}\text { якщо } x_i\in\left[c_{d,l-1,i}^{[1]j},c_{dli}^{[1]j}\right],\\
\frac{c_{d,l+1,i}^{[1]j}-x_i}{c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}\text{ якщо }x_i \in \left[c_{dli}^{[1]j},c_{d,l+1,i}^{[1]j}\right],\\
0\text { у протилежному випадку},
\end{cases}
\end{equation}
\medskip

кубічні сплайни:

\begin{equation}
\mu_{jli}^{[1]}\left(x_i\right)=
\begin{cases}
\frac{1}{4}\left(2+3\frac{2x_i-c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}{c_{dli1}^{[1]j}-c_{d,l-1,i}^{[1]j}}-\left(\frac{2x_i-c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}{c_{dli}^{[1]j}-c_{d,l-1,i}^{[1]j}}\right)^3\right),\\
\text{якщо }x\in\left[c_{d,l-1,i}^{[1]j},c_{dli}^{[1]j}\right],\\
\frac{1}{4}\left(2-3\frac{2x_i-c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}{c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}+\left(\frac{2x_i-c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}{c_{d,l+1,i}^{[1]j}-c_{dli}^{[1]j}}\right)^3\right),\\
\text{якщо }x\in\left[c_{dli}^{[1]j},c_{d,l+1,i}^{[1]j}\right],\\
0\text{ у протилежному випадку},
\end{cases}
\end{equation}

або $B$-сплайни:

\begin{equation}
\mu_{jli}^{g[1]}=
\begin{cases}
\begin{rcases}
1\text{ якщо }x_{i}\in \left[c_{dli}^{[1]j},c_{d,l+1,i}^{[1]j}\right],\\
0\text{ у протилежному випадку}
\end{rcases}
\text{ якщо }g=1,\\
\frac{x_i-c_{dli}^{[1]j}}{c_{d,l+g-1,i}^{[1]j}-c_{dli}^{[1]j}}\mu_{dli}^{g-1,[1]j}\left(x_i\right)+\frac{c_{d,l+g,i}^{[1]j}-x_i}{c_{d,l+g,i}^{[1]j}-c_{d,l+g,i}^{[1]j}}\mu_{d,l+1,i}^{g-1,[1]j}\left(x_i\right),\\
\text{ якщо }g>1,
\end{cases}
\end{equation}
\medskip

де $\mu_{dli}^{g[1]j}\left(x_i\right)$ -- $l$-й сплайн $g$-ого порядку.

Запишемо вихідний сингнал $j$-ого нео-фаззі нейрону $d$-ого виходу першого каскаду у вигляді

\begin{equation}
\begin{cases}
\begin{aligned}

&\hat{y}_d^{[1]j}\left(k\right)=\sum\limits_{i=1}^{n}f_{di}^{[1]j}\left(x_i\left(k\right)\right)=
\sum\limits_{i=1}^{n}\sum\limits_{l=1}^{h}{w_{dli}^{[1]j}\mu_{dli}^{[1]j}\left(x_i\left(k\right)\right)},\\
&\text{ЯКЩО }x_i\left(k\right) \in X_{li}^{j}\text{ , ТОДІ ВИХІД }w_{dli}^{[1]j}.

\end{aligned}
\end{cases}
\end{equation}
\medskip

вихідні сигнали нео-фаззі нейронів другого каскаду:

\begin{equation}
\begin{aligned}
\hat{y}_d^{[2]j}=&\sum\limits_{i=1}^{n}\sum\limits_{l=1}^{h}{w_{dli}^{[2]j}\mu_{dli}^{[2]j}\left(x_i\right)}+\\
&\sum\limits_{d=1}^{g}\sum\limits_{l=1}^{h}{w_{dl,n+1}^{[2]j}\mu_{dl,n+1}^{[2]j}\left(\hat{y}_d^{*[1]}\right)}\text{ }\forall\text{ }{d=1,2,\dots,g}
\end{aligned}
\end{equation}
\medskip

вихідні сигнали $m$-ого каскаду:

\begin{equation}
\begin{aligned}
\hat{y}_d^{[2]j}=&\sum\limits_{i=1}^{n}\sum\limits_{l=1}^{h}{w_{dli}^{[2]j}\mu_{dli}^{[2]j}\left(x_i\right)}+\\
&\sum\limits_{d=1}^{g}\sum\limits_{p=n+1}^{n+m-1}\sum\limits_{l=1}^{h}{w_{dlp}^{[m]j}\mu_{dlp}^{[m]j}\left(\hat{y}_d^{*[p-n]}\right)}\text{ }\forall\text{ }{d=1,2,\dots,g}
\end{aligned}
\end{equation}
\medskip

Введемо до розгляду надалі вектор функцій належності $j$-ого нейрону $d$-ого виходу $m$-ого каскаду:

\begin{equation}
\begin{aligned}
\mu_{d}^{[m]j}\left(k\right)=\biggl(&\mu_{d11}^{[m]j}\left(x_1\left(k\right)\right),\dots,\mu_{dh1}^{[m]j}\left(x_1\left(k\right)\right),\mu_{d12}^{[m]j}\left(x_2\left(k\right)\right),\\
&\dots,\mu_{dh2}^{[m]j}\left(x_2\left(k\right)\right),\dots,\mu_{dli}^{[m]j}\left(x_i\left(k\right)\right),\dots,\mu_{dhn}^{[m]j}\left(x_n\left(k\right)\right),\\
&\dots,\mu_{d1,n+1}^{[m]j}\left(\hat{y}^{*[1]}\left(k\right)\right),\dots,\mu_{dh,n+m-1}^{[m]j}\left(\hat{y}^{*[m-1]}\left(k\right)\right)\biggr)^T
\end{aligned}
\end{equation}
\medskip

та відповідний йому вектор синаптичних вагових коефіцієнтів

\begin{equation}
\begin{aligned}
w_{d}^{[m]j}=\biggl(&w_{d11}^{[m]j},\dots,w_{dh1}^{[m]j},w_{d12}^{[m]j},\dots,w_{dh2}^{[m]j},\dots,w_{dli}^{[m]j},\\
&\dots,w_{dhn}^{[m]j},w_{d1,n+1}^{[m]j},\dots,w_{dh,n+m-1}^{[m]j},\biggr)^T,
\end{aligned}
\end{equation}
\medskip

щоб записати вихідний сигнал системи у компактній формі:

\begin{equation}
\hat{y}_d^{[m]j}\left(k\right)=\left(w_d^{[m]j}\right)^T\mu_d^{[m]j}\left(k\right).
\end{equation}
\medskip

Для навчання нео-фаззі нейронів може бути використаний будь-який з методів адаптивної ідентифікації, що ми пропонували використовувати для навчання вузлів одновимірної нео-фаззі системи у першому розділі. Так корегувати вагові кофіцієнти можна за допомгою експоненційно зваженого рекурентного методу найменших квадратів:

\begin{equation}\label{eq:ExpWeightedRecurrentLeastSquaresLearning}
\begin{cases}
w_d^{[m]j}\left(k+1\right)=w_d^{[m]j}\left(k\right)+\\
\frac{
P_d^{[m]j}\left(k\right)\left(y^d\left(k+1\right)-\left(w_d^{[m]j\left(k\right)}\right)^T\mu_d^{[m]j}\left(k+1\right)\right)}{\alpha+\left(\mu_d^{[m]j}\left(k+1\right)\right)^{T}P_d^{[m]j}\left(k\right)\mu_d^{[m]j}\left(k+1\right)
}\mu_d^{[m]j}\left(k+1\right),\\
P_d^{[m]j}\left(k+1\right)=\frac{1}{\alpha}\left(P_d^{[m]j}\left(k\right)-\frac{P_d^{[m]j}\left(k\right)\mu_d^{[m]j}\left(k+1\right)\left(\mu_d^{[m]j}\left(k+1\right)\right)^{T}P_d^{[m]j}\left(k\right)
}{\alpha+\left(\mu_d^{[m]j}\left(k+1\right)\right)^{T}P_d^{[m]j}\left(k\right)\mu_d^{[m]j}\left(k+1\right)
}\right),
\end{cases}
\end{equation}
\medskip

де $y^d\left(k+1\right),d=1,2,\dots,g$ -- зовнішній навчальний сигнал,  

$0<\alpha \leq 1$ -- фактор забування;

або градієнтний метод навчання, що, як зазначалося, відрізняється як згладжувальними, так і слідкуючими властивостями:

\begin{equation}\label{eq:GradientLearning}
\begin{aligned}
\begin{cases}
w_d^{[m]j}\left(k+1\right)&=w_d^{[m]j}\left(k\right)+\frac{y^d\left(k+1\right)-\left(w_d^{[m]j}\left(k\right)\right)^{T}\mu_d^{[m]j}\left(k+1\right)
}{r_d^{[m]j}\left(k+1\right)}\mu_d^{[m]j}\left(k+1\right),\\
r_d^{[m]j}\left(k+1\right)&=\alpha r_d^{[m]j}+\left\|\mu_d^{[m]j}\left(k+1\right)\right\|^{2},0\leq \alpha \leq 1.
\end{cases}
\end{aligned}
\end{equation}
\medskip

\subsection{Оптимізація пулу нео-фаззі нейронів багатовимірної каскадної системи, що еволюціонує}

Оскільки за мету було поставлено синтез такої багатовимірної каскадної системи, що б могла працювати саме в режимі реального часу, було б дуже доречно, якби система могла самостійно визначати найліпшу кількість функцій належності та їх форму, адже ці параметри також можуть змінюватися у часі. Тому у цьому підрозділі пропонується у кожному каскаді збільшити кількість нео-фаззі нейронів до такої, що є кратною (а не дорівнює, як пропоноувалося у попередньому підрозділі) розмірності вектору вихідного сигналу ввести узагальнюючі нейрони, що у кожному каскаді узагальнюватимуть вихідні сигнали нейронів пулу.

Тут буде схема.
Тут буде схема.
Тут буде схема.
Тут буде схема.
Тут буде схема.
Тут буде схема.

\subsubsection{Метод визначення поточно оптимальних вихідних сигналів пулу нео-фаззі нейронів багатовимірної каскадної системи, що еволюціонує}

Вихідні сигнали нейронів пулу кожного каскаду пропонується об'єднати узагальнюючим нейроном $GN_d^{[m]}$, що його було введено у \hl{???} розділі.
Таким чином, у кожному каскаді системи маємо $g$ $GN_d^{[m]}$ елементів, що узагальнюють вихідні сигнали нейронів пулу для кожного елементу вихідного вектору:

\begin{equation}
\hat{y}_d^{*[m]}\left(k\right)=\left(\hat{y}_{d1}^{[m]}\left(k\right),\hat{y}_{d2}^{[m]}\left(k\right),\dots,\hat{y}_{dq}^{[m]}\left(k\right)\right)^T;
\end{equation}
\medskip

Нагадаймо, точність вихідного сигналу узагальнюючих елементів має бути не меншою від точності будь-якого сингналу, що узагальнюється (подається на вхід до $GN_d^{[m]}$).
Рекурента форма метод навчання <<на плаваючому вікні>> елементів $GN_d^{[m]}$ кожного каскаду має вигляд

\begin{equation}\label{eq:GeneralizedOutputRecurrent}
\begin{cases}
\begin{aligned}
\tilde{P}_d^{[m]}\left(k+1\right)=&P_d^{[m]}\left(k\right)-\frac{P_d^{[m]}\left(k\right)\hat{y}_d^{[m]}\left(k+1\right)\hat{y}_d^{[m]T}\left(k+1\right)P_d^{[m]}\left(k\right)}{1+\hat{y}_d^{[m]T}\left(k+1\right)P_d^{[m]}\left(k\right)\hat{y}_d^{[m]}\left(k+1\right)},\\
P_d^{[m]}\left(k+1\right)=&\tilde{P}_d^{[m]}\left(k+1\right)+\\
&\frac{\tilde{P}_d^{[m]}\left(k+1\right)\hat{y}_d\left(k-s+1\right)\hat{y}_d^{[m]T}\left(k-s+1\right)\tilde{P}_d^{[m]}\left(k+1\right)}{1-\hat{y}_d^{[m]T}\left(k-s+1\right)\tilde{P}_d^{[m]}\left(k+1\right)\hat{y}_d^{[m]}\left(k-s+1\right)},\\
\hat{y}_d^{*[m]}\left(k+1\right)=&\frac{\hat{y}_d^{[m]T}\left(k+1\right)P_d^{[m]}\left(k+1\right)E}{E^TP_d^{[m]}\left(k+1\right)E},
\end{aligned}
\end{cases}
\end{equation}
\medskip

а у випадку, коли $s=1$:

\begin{equation}
\begin{aligned}
\hat{y}_d^{*[m]}\left(k+1\right)&=\frac{\hat{y}_d^{[m]T}\left(k+1\right)\hat{y}_d^{[m]}\left(k+1\right)}{E^T\hat{y}_d^{[m]}\left(k+1\right)}\\
&=\frac{\left\|\hat{y}_d^{[m]}\left(k+1\right)\right\|^2}{E^T\hat{y}_d^{[m]}\left(k+1\right)}\\
&=\frac{\sum\limits_{j=1}^q{\left(\hat{y}_d^{[m]}\left(k+1\right)\right)^2}}{\sum\limits_{j=1}^q{\hat{y}_d^{[m]}\left(k+1\right)}}.
\end{aligned}
\end{equation}
\medskip

\section{Багатовимірна каскадна система, що еволюціонує, побудована на багатовимірних нео-фаззі нейронах}
\label{sec:MIMOEvolvingCascadedSystemBuiltOnMNFNs}

Архітектура багатовимірної каскадної системи, яка ґрунтується на звичайних нео-фаззі нейронах, що її описано у \ref{sec:MIMOEvolvingCascadedSystemBuiltOnNFNs} є надмірною, адже вектор вхідних сигналів $x\left(k\right)$ (для першого каскаду) подається на однотипні нелінійні синапси $NS_{di}^{[1]j}$ нео-фаззі нейронів, кожен з яких на виході генерує сигнал $\hat{y}_d^{[1]j}\left(k\right),d=1,2,\dots,g$. У результаті компоненти вихідного вектора 

\begin{equation}
\hat{y}^{[1]j}\left(k\right)=\left(\hat{y}_1^{[1]j}\left(k\right), \hat{y}_2^{[1]j}\left(k\right),\dots,\hat{y}_g^{[1]j}\left(k\right)\right)^{T}
\end{equation}
\medskip

обчислюються незалежно один від одного, хоча $\mu_{1il}\left(x_i\left(k\right)\right)=\mu_{2il}\left(x_i\left(k\right)\right)=\mu_{jil}\left(x_i\left(k\right)\right)=\mu_{nil}\left(x_i\left(k\right)\right)$. Уникнути цього можна, якщо ввести до розгляду багатовимірний нео-фаззі нейрон, що є модифікацією систем, запропонованих у \hl{[??,??]}.

\subsection{Багатовимірний нео-фаззі нейрон}

Вузлами багатовимірного нео-фаззі нейрону є складені нелінійні синапси $MNS_i^{[1]j}$, кожен з яких має $h$ функцій належності $\mu_{li}^{[1]j}$ та $gh$ настроюваних синаптичних вагових коефіцієнтів, але тільки $hn$ функцій належності, що в $g$ разів менше, ніж у випадку, коли каскад сформований із звичайних нео-фаззі нейронів.

Введемо надалі до розгляду $\left(hn \times 1\right)$ - вектор функцій належності

\begin{equation}
\mu^{[1]j}\left(k\right)=\left(\mu_{11}^{[1]j}\left(x_1\left(k\right)\right),\mu_{21}^{[1]j}\left(x_1\left(k\right)\right),\dots,\mu_{h1}^{[1]j}\left(x_1\left(k\right)\right),\dots,\mu_{hn}^{[1]j}\left(x_n\left(k\right)\right)\right)^{T}
\end{equation}
\medskip

та $\left(g \times hn\right)$ - матрицю синаптичних вагових коефіцієнтів

\begin{equation}
W^{[1]j}=\left(
\begin{matrix}    w_{111}^{[1]j}&w_{112}^{[1]j}&\dots&w_{1li}^{[1]j}&\dots&w_{1hn}^{[1]j}\\
 w_{211}^{[1]j}&w_{212}^{[1]j}&\dots&w_{2li}^{[1]j}&\dots&w_{2hn}^{[1]j}\\ 
    \vdots&\vdots&&\vdots&&\vdots\\    w_{g11}^{[1]j}&w_{g12}^{[1]j}&\dots&w_{gli}^{[1]j}&\dots&w_{ghn}^{[1]j}\\
\end{matrix}
\right),
\end{equation}
\medskip

і запишемо сигнал на виході $MN_j^{[1]}$ у $k$-й момент часу у вигляді

\begin{equation}
\hat{y}^{[1]j}\left(k\right)=W^{[1]j}\mu^{[1]j}\left(k\right).
\end{equation}
\medskip

Навчаняя багатовимірного нео-фаззі нейрону можна реалізувати за допомогою матричної модифікації експоненційно-зваженого рекурентного методу найменших квадратів \eqref{eq:ExpWeightedRecurrentLeastSquaresLearning} у формі

\begin{equation}
\begin{aligned}
\begin{cases}
W^{[1]j}\left(k+1\right)&=W^{[1]j}\left(k\right)+\\
&\frac{\left(y\left(k+1\right)-W^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)\right)
\left(\mu^{[1]j}\left(k+1\right)\right)^{T}P^{[1]j}\left(k\right)}{\alpha+\left(\mu^{[1]j\left(k+1\right)}\right)^{T}P^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)
},\\
P^{[1]j}\left(k+1\right)&=\frac{1}{\alpha}\left(P^{[1]j}\left(k\right)-\frac{P^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)\left(\mu^{[1]j}\left(k+1\right)\right)^{T}P^{[1]j}\left(k\right)
}{\alpha+\left(\mu^{[1]j}\left(k+1\right)\right)^{T}P^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)
}\right),\\
&0 < \alpha \leq 1
\end{cases}
\end{aligned}
\end{equation}
\medskip

або багатовимірного варіанту методу \eqref{eq:GradientLearning}

\begin{equation}
\begin{aligned}
\begin{cases}
W^{[1]j}\left(k+1\right)&= W^{[1]j}\left(k\right) + \frac{y\left(k+1\right)-W^{[1]j}\left(k\right)\mu^{[1]j}\left(k+1\right)}{r^{[1]j}\left(k+1\right)}\\
&\times\left(\mu^{[1]j}\left(k+1\right)\right)^{T},\\
r^{[1]j}\left(k+1\right)&=\alpha r^{[1]j}\left(k\right)+\left\|\mu^{[1]j}\left(k+1\right)\right\|^{2},\\
&0 \leq \alpha \leq 1,
\end{cases}
\end{aligned}
\end{equation}
\medskip

де $y\left(k+1\right)=\left(y^{1}\left(k+1\right),y^{2}\left(k+1\right),\dots, y^{g}\left(k+1\right)\right)^{T}$.


Аналогічним чином проводиться навчання інших каскадів, при цьому вектор функцій належності $m$-го каскаду $\mu^{[m]j}\left(k+1\right)$ збільшує свою розмірність на $\left(m-1\right)g$ компоненти, що їх утворили виходи попередніх каскадів.

\subsection{Метод визначення поточно оптимального вихідного сигналу пулу багатовимірних нео-фаззі нейронів каскадної системи, що еволюціонує}

У цьому підрозділі буде запропоновано узагальнюючий нейрон $GMN^{[m]}$ та рекурентний метод його навчання, щоб він об'єднував усі вихідні сигнали нейронів $MN{[m]}$ пулу каскаду у сигнал

\begin{equation}
\hat{y}^{*[m]}\left(k\right)=\left(\hat{y}_1^{*[m]}\left(k\right), \hat{y}_2^{*[m]}\left(k\right),\dots, \hat{y}_g^{*[m]}\left(k\right)\right)^{T},
\end{equation}
\medskip

з точністю не меншою від точності будь-якого з сигналів $\hat{y}_j^{[m]}\left(k\right)$.


Розв'язати це завдання можна, знову скориставший апаратом невизначених множників Лагранжа та адаптивного багатовимірного узагальненого прогнозування% \cite{refU}.

Введемо до розгляду вихідний сигнал нейрону $GMN^{[m]}$ у вигляді

\begin{equation}
\hat{y}^{*[m]}\left(k\right)=\sum\limits_{j=1}^{q}{c_j^{[m]}\hat{y}_j^{[m]}\left(k\right)}=\hat{y}^{[m]}\left(k\right)c^{[m]},
\end{equation}
\medskip

де $\hat{y}^{[m]}\left(k\right)=\left(\hat{y}_1^{[m]}\left(k\right), \hat{y}_2^{[m]}\left(k\right),\dots,\hat{y}_q^{[m]}\left(k\right)\right)^{T}$-- $\left(g \times q\right)$-матриця

$c^{[m]}$-- $\left(q \times 1\right)$-вектор коефіцієнтів узагальнення, що відповідають умовам незміщенності

\begin{equation}
\sum\limits_{j=1}^{q}{c_j^{[m]}}=E^{T}c^{[m]}=1,
\end{equation}
\medskip


$E=\left(1,1,\dots,1\right)^{T}$-- вектор, утворений одиницями.


Введемо критерій навчання

\begin{equation}
\begin{aligned}
E^{[m]}\left(k\right)&=\sum\limits_{\tau=1}^k\left\|y\left(\tau\right)-\hat{y}^{[m]}\left(\tau\right)c^{[m]}\right\|^2\\
&=Tr\left(\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I\otimes c^{[m]}\right)^{T}\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I \otimes c^{[m]}\right)\right)
\end{aligned}
\end{equation}
\medskip


де $Y\left(k\right)=\left(y^T\left(1\right), y^T\left(2\right),\dots,y^T\left(k\right)\right)^{T}$-- $\left(k \times s\right)$ матриця спостережень,

\begin{equation}
\begin{aligned}
\hat{Y}^{[m]}\left(k\right)=\left(
\begin{matrix}
\hat{y}_1^{[m]T}\left(1\right)&\hat{y}_2^{[m]T}\left(1\right)&\dots&\hat{y}_q^{[m]T}\left(1\right)\\
\hat{y}_1^{[m]T}\left(2\right)&\hat{y}_2^{[m]T}\left(2\right)&\dots&\hat{y}_q^{[m]T}\left(2\right)\\
\vdots&\vdots&&\vdots\\
\hat{y}_1^{[m]T}\left(k\right)&\hat{y}_2^{[m]T}\left(k\right)&\dots&\hat{y}_q^{[m]T}\left(k\right)\\
\end{matrix}
\right),
\end{aligned}
\end{equation}
\medskip

де $I$ -- одинична $\left(g \times g\right)$ матриця,

$\otimes$ -- символ тензорного добутку.

З урахуванням ?? обмежень запишемо функцію Лагранжа

\begin{equation}
\begin{aligned}
L^{[m]}\left(k\right)&=E^{[m]}\left(k\right)+\lambda\left(E^{T}c^{[m]}-1\right)\\
&=\sum\limits_{\tau=1}^{k}\left\|y\left(\tau\right)-\hat{y}^{[m]}\left(\tau\right)c^{[m]}\right\|^2+\lambda\left(E^Tc^{[m]}-1\right)\\
&=Tr\left(\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I\otimes c^{[m]}\right)^T\left(Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I\otimes c^{[m]}\right)\right)\\
&+\lambda\left(E^Tc^{[m]}-1\right)\\
&=Tr\left(V^{[m]T}\left(k\right)V^{[m]}\left(k\right)\right)+\lambda\left(E^Tc^{[m]}-1\right),
\end{aligned}
\end{equation}
\medskip

де $V^{[m]}\left(k\right)=Y\left(k\right)-\hat{Y}^{[m]}\left(k\right)I \otimes c^{[m]}$-- $\left(k \times g\right)$ матриця оновлень.

Розв'язання системи рівнянь Каруша-Куна-Таккера

\begin{equation}
\begin{cases}
\nabla_{c^{[m]}}L^{[m]}\left(k\right)=\overrightarrow{0},\\
\frac{\partial L^{[m]}\left(k\right)}{\partial \lambda}=0
\end{cases}
\end{equation}
\medskip

призводить до очевидного результату

\begin{equation}
\begin{cases}
c^{[m]}=\left(R^{[m]}\left(k\right)\right)^{-1}E\left(E^T\left(R^{[m]}\left(k\right)\right)^{-1}\right)^{-1}\\
\lambda=-2E^T\left(R^{[m]}\left(k\right)\right)^{-1}E,
\end{cases}
\end{equation}
\medskip

де $R^{[m]}\left(k\right)=V^{[m]T}\left(k\right)V^{[m]}\left(k\right)$.

Таким чином, можна організувати оптимальне об'єднання виходів усіх нейронів пулу кожного каскаду. Зрозуміло, що в якості таких нейронів можуть використовуватися не тільки багатовимірні нео-фаззі нейрони, але й будь-які інші конструкції, що реалізують нелінійне відображення \mbox{$R^{n+\left(m-1\right)g}\rightarrow R^g$}.
\bibliographystyle{ugost2008ns}
\bibliography{references}	
\end{document}