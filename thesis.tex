\documentclass{vakthesis}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian,ukrainian]{babel}
\usepackage{geometry}
\usepackage{mathtools}% http://ctan.org/pkg/mathtools
\usepackage{amsmath}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{color,soul}
\usepackage{graphicx}
\usepackage{MnSymbol}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage[labelsep=endash]{caption}
\usepackage[shortcuts]{extdash}

\graphicspath{{images/}}


%\geometry{hmargin={30mm,15mm},lines=29,vcentering}
\everymath=\expandafter{\the\everymath\displaystyle}

\geometry{a4paper, total={170mm,257mm}, left=25mm, top=20mm}

%This is to make proper table captions, you most definately do *not* want to edit this
\makeatletter
\let\ORG@makecaption\@makecaption
\let\ORGlongtable\longtable
\let\ORGLT@makecaption\LT@makecaption
\AtBeginDocument{%
  \let\@maketablecaption\ORG@makecaption
  \let\longtable\ORGlongtable
  \let\LT@makecaption\ORGLT@makecaption
}
\makeatother


%\DeclareMathSizes{10}{10}{10}{10}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}  
  \title{Еволюційні нейро-фаззі мережі з каскадною структурою для інтелектуального аналізу данних}
  \author{Копаліані Дар'я Сергіївна}
	\supervisor{Бодянський Євгеній Володимирович}{доктор технічних наук, професор}
	\speciality{05.13.23}
	\udc{004.032.26}
	\institution{Харківський національний університет радіоелектроніки}{Харків}
	\date{2015}
	
	\maketitle
	
	% Зміст
	\tableofcontents
	
  \newcommand{\V}[1]{\mathit{#1}}
  \let\originalleft\left
  \let\originalright\right
  \renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
  \renewcommand{\right}{\aftergroup\egroup\originalright}
  \renewcommand{\floatpagefraction}{.9}%
  \renewcommand{\topfraction}{.9}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Вступ}
\label{ch:Intro}

Кінець ХХ століття характеризується помітним сплеском досліджень в області штучних нейронних мереж завдяки тому, що, з одного боку, у другій половині 1980-их років був відкритий алгоритм зворотного поширення похибки, внаслідок чого вдалося подолати критичні зауваження Мінського і Пайперту~\cite{ref150}, а з іншого -- через те, що з року у рік справджувався закон Муру, дозволяючи персональним комп'ютерам проводити дедалі складніші обчислення. У 1990-ті роки теорія штучних нейронних мереж стрімко розвивається, а отримані результати успішно застосовуються для вирішення широкого кола завдань ідентифікації, прогнозування, управління, кластерування та класифікування. Однак, в той же час, стають чітко зрозумілими недоліки традиційних нейромережевих архітектур: велика обчислювальна складність, абсолютна неінтерпретіруемость результатів,  емпіричний характер вибору архітектури мережі для вирішення будь-якої задачі. У зв'язку з цим застосування нейронних мереж в певному ряді випадків неефективно.
З середини 1990-их років по справжній момент у світі активно ведуться дослідження з розробки методів, що дозволяють подолати зазначені недоліки. Останнім часом все більшої популярності набувають так звані гібридні нейро-фаззі мережі, що об'єднують в собі переваги нейромережевого підходу і систем нечіткого висновування.


\paragraph{Актуальність теми}

Традиційно під гібридними нейро-фаззі мережами розуміють штучні нейронні мережі з можливістю той чи інший спосіб отримувати знання про те, за якими правилами проводиться генерація вихідного сигналу. Таким чином вирішується проблема неінтерпретіруемості результатів, однак, слід зазначити, що гібридні нейро-фаззі системи не здатні працювати в реальному режимі часу, а крім того часто є адаптивними лише з тієї точки зору, що можуть налаштовувати свої синаптичні вагові коефіцієнти в процесі навчання, не маючи при цьому механізмів структурної адаптації. Метою даної роботи є розробка гібридних еволюційних нейро-фаззі архітектур, а також методів їх налаштування і навчання, що дозволяють подолати обмеження як традиційних, так і існуючих гібридних нейро-фаззі мереж.

\paragraph{Зв'язок роботи з науковими програмами, планами, темами}

Дисертаційна робота виконана в рамках держбюджетних тем <<Еволюційні гібридні системи обчислювального інтелекту зі змінною структурою для інтелектуального аналізу даних>> (№ ДР 0110U000458) та <<Нейро-фаззі системи для поточної кластеризації та класифікації послідовностей даних за умов їх викривленості відсутніми та аномальними спостереженнями>> (№ ДР 0113U000361), які виконувалися згідно указу Міністерства освіти і науки України за результатами конкурсного відбору проектів наукових досліджень. В рамках зазначених НДР здобувачкою в якості
виконавця розроблено модифікації гібридних архітектур та адаптивні методи їх навчання для вирішення задач прогнозування, емуляції та кластерування в on-line режимі.

\paragraph{Мета та задачі дослідження}

Метою дослідження є розробка гібридних еволюційних штучних нейро-фаззі мереж і методів їх навчання з підвищеною швидкодією і можливостями інтерпретації вихідного сигналу, а також параметричної та структурної адаптації в режимі послідовної обробки інформації.
Поставлені цілі досягаються шляхом вирішення таких основних завдань: 
\begin{itemize}
\item аналіз існуючих методів структурної адаптації нейронних мереж;
\item розробка гібридних штучних нейронів з підвищенною швидкодією, а також методів їх навчання;
\item імітаційне моделювання розроблених архітектур і методів їх навачання, розв'язання практичних завдань.
\end{itemize}

{\it Об’єктом дослідження} є процес динамічного інтелектуального аналізу даних.

{\it Предметом дослідження} є гібридні нейромережі та нейро-фаззі системи, що призначені для вирішення задач динамічної інтелектуальної обробки нестаціонарних нелінійних сигналів  , зокрема багатовимірних, за умов невизначеності, та методи їх навчання.

{\it Методи дослідження}. Теорія штучних нейронних мереж, що дозволила синтезувати нові еволюційні архітектури нейронних мереж, нечітка логіка, що дала можливість реалізувати нечіткий висновок на основі розроблених архітектур, теорія оптимізації, що забезпечила розробку методів настройки синаптичних ваг з підвищеною швидкодією і стійкістю до зашумленими даними для пропонованих в рамках дисертаційної роботи зростаючих нейронних мереж, а також апарат математичної статистики, спираючись на який, була проведена систематизація та використання отриманих в результаті роботи даних для наукових і практичних висновків.

\paragraph{Наукова новизна отриманих результатів}
До нових, одержаних особисто авторкою, належать такі результати:

\begin{itemize}
\item вперше запропонована архітектура багатовимірного нео-фаззі нейрона та метод його навчання, що забезпечають підвищену швидкість налаштування синаптичних ваг та додаткові згладжуючі властивості;
\item запропоновані архітектура та методи навчання гібридної каскадної нейронної мережі з оптимізацією пулу нейронів (як одновимірних так і багатовимірних) у кожному каскаді, що реалізують оптимальний за точністю прогноз нелінійних стохастичних і хаотичних сигналів у онлайн режимі;
\item впреше запропонований розширений нео-фаззі нейрон, який дозволяє реалізовувати нечітке висновуння за Такаґі\=/Суґено довільного порядку, що має покращенні апроксимуючі властивості;
\item запропонована архітектура і метод самонавчання еволюційної каскадної нейро-фаззі системи для послідовного кластерування потоків даних з автоматичним визначенням поточно оптимальної кількості кластерів.
\end{itemize}

\paragraph{Практичне значення отриманих результатів}

Запропоновані в роботі архітектури та адаптивні методи навчання еволюційних мереж і нейро-фаззі мереж, що забезпечують оптимальну точність вихідного сигналу в умовах апріорної та поточної невизначеності і можуть бути використані в різних областях, де дані представлені в числовій формі у вигляді таблиць «об'єкт-властивість» або часових послідовностей в режимі послідовної або пакетної обробки. Використання комплексу запропонованих адаптивних методів навчання та архітектур дозволяє підвищити ефективність застосування еволюційних штучних нейронних мереж та нейро-фаззі систем для вирішення задач прогнозування та ідентифікації даних різної фізичної природи та кластерування у послідовному режимі. Отримані теоретичні результати були досліджені експериментально на тестових і реальних даних, де показали свою перевагу над відомими у світовій практиці методами. Запропоновані гібридні нейромережеві архітектури, а також методи їх самонастроювання і навчання реалізовані у вигляді програмних засобів.

Синтезовані в роботі методи підтвердили свою ефективність в задачі прогнозування витрат нормогодин на ремонті роботи візків вагонів 61-425, 61-181, 47Д та 47К. Результати досліджень впроваджені у ТОВ <<Харківський вагонобудівний завод>>, м. Харків, що підтверджено відповідним актом (акт від 11.10.2015). Також пропонована самонавчана гібридна нейро-фаззі система, що еволюціонує, використовується для вирішення задачі розпізнання зображень та впроваджена у ТОВ <<Факторіал Комплексіті>>, м. Харків, що підтверджено актом впровадження від 01.09.2015.

\paragraph{Особистий вклад здобувача} 

Усі положення, що виносяться на захист, основні результати теоретичних та експериментальних досліджень отримані здобувачем особисто. Їх основний зміст викладено у роботах \cite{ref151, ref152, ref153, ref154, ref155, ref156, ref157, ref158, ref159, ref160, ref161, ref162, ref163}. Внесок авторки в публікаціях, написаних у співавторстві такий: 
У~\cite{ref151} запронована архітектура LS-FSVM-NFN cистеми, що ґрунтується на нео-фаззі нейронах, і дозволяє використовувати методи оптимізації другого порядку,
у~\cite{ref152} запропонована структура адаптивного нео-фаззі-предиктора та багатовимірного нео-фаззі-нейрона,
у~\cite{ref153} удосконалені методи навчання гібридної каскадної системи, що дозволяють обробляти потоки даних у послідовному режимі,
у~\cite{ref154} запропонована архітектура багатовимірної нейро-фаззі системи з оптимізованим пулом нейронів,
у~\cite{ref155} запроновані методи навчання гібридної каскадної нейронної мережі, що забезпечують обчислювальну простоту та характеризуються як слідкуючими, так і фільтруючими властивостями,
у~\cite{ref156} удосконалені методи навчання гібридної нейро-фаззі системи для обробляння нестаціонарних стохастичних та хаотичних сигналів нелінійних об’єктів з необхідною точністю,
у~\cite{ref157} запропонована модифікація нео-фаззі нейрону ENFN з поліпшеними апроксимуючими властивостями, що реалізовує нечітке висновуння за Такагі-Сугено довільного порядку,
у~\cite{ref158} запронована архітектура нейро-фаззі системи, що еволюціонує, для вирішення задачі нечіткого кластерування потоків даних,
у~\cite{ref159} вдосконалені методи навчання гібридної нейронної мережі для вирішення задачі адаптивного обробляння нелінійних часових рядів, що поєднують у собі високу шводкодію та фільтруючі властивості,
у~\cite{ref160} вдосконалена архітектура системи зі змінною кількістю вузлів у каскадах,
у~\cite{ref161} запронована архітектура каскадної нейро-фаззі мережі, що ґрунтується на розширених нео-фаззі нейронах,
у~\cite{ref162} запропонований метод самонавчання гібридної нейро-фаззі системи для кластерування даних високої розмірності у послідовному режимі,
у~\cite{ref163} запронована архітектура нейро-фаззі системи, що еволюціонує, для вирішення задачі нечіткого кластерування потоків даних.
 
\paragraph{Апробація результатів дисертації} 

Основні результати дисертаційної роботи були представлені та обговорені на VII міжнародній школі-семінарі «Теорія прийняття рішень»: (Ужгород 2014), міжнародній науковій конференції Интеллектуальные системы принятия решений и проблемы вычислительного интеллекта ISDMCI’2015 (Херсон 2015), міжнародній конференції Advances in Data Science. International Workshop and Networking Event (Holny Mejera, Poland, 2015), та міжнародній конференції <<on "Computer Science \& Information Technologies"~CSIT’2015>> (Львів 2015).

\paragraph{Публікації} 

Основні положення дисертаційної роботи опубліковані в 13 наукових роботах: 1 розділ моногарфії, 7 статей у періодичних виданнях з технічних наук, включених до переліків МОН України, у тому числі 6 статей у в журналах, що входить до міжнародних наукометричних баз, 5 публікації у працях конференцій.




\chapter{Огляд стану проблеми та постановка задачі дослідження}
%\label{ch:Intro}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Основні події в нейронних мережах досліджень}

Дисципліна нейронних моделей моделює людський мозок. Середній людський мозок складається з близько \hl{1011} нейронів різних типів, при цьому кожен нейрон поєднується до десятків тисяч синапсів. Таким чином, нейромережеві моделі також називаються конекціонистичними модедями. Інформація обробляється, в основному, в зовнігньому шарі головного мозку \hl{-} його корі. Когнітивні функції, серед яких мова, абстрактне мислення,також навчання і пам'ять, є найбільш складними для визначення з точки зору нейро механізмів операціями мозку.

У 1940-х роках, Маккалох та Піттс~\cite{ref123} виявили, що нейрон може бути змодельовано як простий пороговий пристрій для виконання логічної функції. У 1949 році Хебб \hl{[14]} запропонував правило Хебба, що описує вплив процесу навчання на синаптичний зв`язок між двома нейронами. У 1952 році на підставі фізичних властивостей клітинних мембран і трансмембраних білків Ходжкін і Хакслі \hl{[15]} описали такі явища, як нейроні стрільби та поширення потенціалу дії, через набір еволюційних рівнянь. Ця робота принесла Ходжкіну та Хакслі Нобелівську премію в 1963 р. В кінці 1950-х і початку 1960-х, Розенблатт \hl{[31]} запропонован модель перцептрона, а Відроу і Хофф \hl{[38]} запропоновали модель Adaline (адаптивний лінійний елемент), яка використовувала метод найменших квадратів (МНК) у якості алгоритму навчання.

У 1969 році Мінським і Папертом \hl{[28]} було математично доведено неможливість використання перцептрона для складної логічної функції. Це призвело до істотного зменьшення інтересу до нейронних мереж. В той же час, модель Adaline, а також її багатошарова версія під назвою Madaline була успішно використана при розв'язанні багатьох завдань. Тим не менш, область застосування таких моделей все ще залишалась дуже вузькою, основною причиною цьому було використання виключно лінійної функції активації.

У 1970-ті роки, Гроссберг \hl{[12, 13]}, фон дер Мальцбруг \hl{[37]} і Фукусіма \hl{[9]}, натхнені результатами досліждень зорової кори, опублікували новаторські роботи на предмет навчання і конкурентної самоорганізації. Фукусіма запропонував свою модель когнітрону і неокогнітрону, в рамках парадигми конкурентного навчання. Неокогнітрон являє собою ієрархічну багатошарову нейронну мережу, спеціально призначену для розпізнавання візуальних образів. Також в цей період були запропоновані кілька лінійних моделей асоціативної пам'яті. У 1982 році Кохонен запропонував самоорганізації карту (SOM) [23]. СДЛ адаптивно перетворює входять шаблони сигналів довільних розмірів на одно- або двовимірних дискретних відображень в топологічно впорядковано. Гросберг і Карпентер [4, 13] запропонована адаптивна теорія резонансу (АРТ) моделі в середині 1980-х років. Модель ART, також заснований на конкурентному навчання, є поворотною і самоорганізації.

Модель Хопфілда введена в 1982 році [17] ввів в сучасну епоху нейронних мереж наукових досліджень. Модель працює на системному рівні, а не на одному рівні нейронів. Це повторюється нейронна мережа працює з Хебба правилом. Ця мережа може бути використана в якості асоціативної пам'яті для зберігання інформації і для вирішення завдань оптимізації. Машина Больцмана [1] був введений в 1985 році як розширення мережі Хопфілда шляхом включення стохастичних нейронів. навчання Больцмана заснований на методі, званому модельований отжиг [20]. У 1987 році Коско запропонував адаптивну двобічної асоціативної пам'яті (BAM) [24]. Хеммінга мережу, запропонованого Липпманом в середині 1980-х років [25] заснований на конкурентному навчання, і є найбільш простий асоціативної пам'яті. У 1988 році, Чуа і Ян [5] розширили модель Хопфілда, запропонувавши клітинну модель нейронної мережі. Мережа робота є динамічна мережева модель і особливо підходить для двовимірної обробки сигналів і реалізації НВІС.

Найбільш помітним орієнтиром в нейронної мережі досліджень є алгоритм зворотного поширення (BP) навчання, пропонованих для моделі багатошаровий персептрон (MLP) в 1986 році Rumelhart, Хінтон, і Вільямс [33]. Надалі, алгоритм BP був виявлений вже був винайдений в 1974 році Werbos [39]. У 1988 році, Broomhead і Lowe запропонував мережеву модель радіальної базисної функції (RBF) [3]. Обидва MLP і RBF мережі є універсальними аппроксіматоров.

У 1982 році Оя запропонував мережу аналіз головних компонент (PCA) для класичного статистичного аналізу [29]. У 1994 році загальний запропонував незалежний аналіз компонент (ICA) [6]. ICA є узагальненням PCA, і це, як правило, використовується для виділення ознак і сліпого поділу джерел (BSS). З тих пір багато алгоритми нейронних мереж для класичних статистичних методів, таких як аналіз Фішера лінійний дискримінантний (LDA), канонічного кореляційного аналізу (ССА) і факторного аналізу, були запропоновані.

У 1985 році Перл представила модель байєсівської мережі [30]. Байєсівської мережі є найбільш відомим графічна модель в AI. Він володіє характеристикою буття як статистичного та подання знань формалізм. Вона закладає фундамент для виведення сучасного ІІ.

Ще однією важливою подією в області машинного навчання і нейронних мережевих спільнот є підтримка векторної машини (SVM), запропонований Вапніка і ін. на початку 1990-х років [36]. SVM грунтується на статистичної теорії навчання і особливо корисно для класифікації з малими розмірами зразка. SVM використовується для класифікації, регресії і кластеризації. Завдяки успішному застосуванню в SVM, метод ядра викликав широкий інтерес.

На додаток до нейронних мереж, нечіткої логіки і еволюційні обчислення двох інших великих м'яких обчислень парадигми. Soft Computing є обчислювальним структура, яка може терпіти неточність і невизначеність, а не в залежності від точних математичних обчислень. Нечіткої логіки [40] може включати людські знання в систему за допомогою нечітких правил. 

%Еволюційні обчислення [16, 34] бере свій початок від теорії Дарвіна природного відбору, і може оптимізувати в домені, який важко вирішити іншими засобами. Ці методи в даний час широко використовуються для підвищення интерпретируемость нейронних мереж або для вибору оптимальної архітектури та параметрів нейронних мереж.

%Таким чином, мозок являє собою динамічну систему обробки інформації, яка розвивається її структуру і функціональність в часі через обробки інформації на різних рівнях ієрархії: квантова, молекулярний (генетичний), один нейрон, ансамбль нервових, когнітивна та еволюційна [19]:

%На квантовому рівні частки, які складають кожну молекулу, безперервно рухатися, перебуваючи в кількох штатах в той же час, які характеризуються ймовірністю, фази, частоти і енергії. Ці стани можуть змінюватися відповідно до принципів квантової механіки.
%• На молекулярному рівні, РНК і білкові молекули розвиваються в клітці і взаємодіяти безперервним чином, на основі інформації, що зберігається в ДНК і від зовнішніх факторів, а також впливати на функціонування клітини (нейрона).
%• На рівні окремого нейрона, внутрішні інформаційні процеси і на зовнішньому пристрої стимули зміни синапси і викликають нейрон для формування сигналу для передачі на інші нейрони.
%• На рівні нейрональних ансамблів, все нейрони працюють разом як функція ансамблю через безперервного навчання.
%• На рівні всього мозку, когнітивні процеси відбуваються в Інкрементальний множинної завдання протягом усього життя / множинні методи режиму, наприклад, мови і мислення, а також глобальні інформаційні процеси навчання проявляються, наприклад, свідомості.
%• На рівні популяції індивідуумів, види розвиваються в процесі еволюції шляхом зміни генетичного коду ДНК.

%Побудова обчислювальних моделей, які об'єднують принципи різних рівнів інформації може бути ефективним для вирішення складних завдань. Ці моделі називаються інтеграційні Коннекшіоністскій системи навчання [19]. Інформаційні процеси на різних рівнях в взаємодіють між собою і впливати один на одного інформаційної ієрархії.

\section{Навчання та самонавчання штучних нейронних мереж}

Здатність навчатися – основна властивість біологічного мозку, а оскільки штучна нейронна мережа в деякому сенсі моделює мозок, поняття «навчання» посідає щонайперше місце в теорії штучних нейронних мереж. Математичні проблеми, що пов’язані з навчанням, вивчають у напрямі загальної теорії штучних нейронних мереж, який дістав назву «нейроматематика» [51, 52].

Із точки зору нейроматематики, навчання тлумачать як завдання адаптувати параметри, а можливо, й архітектуру мережі, щоби, оптимізуючи прийнятий критерій якості, розв’язати поставлену задачу. Таке визначення є узвичаєним та неявно припускає, що нейроматематика ґрунтується на методах оптимізації та ідентифікації

Звичайно припускають, що навчання має перманентний характер та з часом мережа покращує свої характеристики, постійно «наближаючись» до оптимального розв’язку поставленої задачі.

Тип та характер навчання обумовлені, насамперед, обсягом попередньої та поточної інформації про довкілля, в яке «занурили» мережу, а також критерієм якості (цільовою функцією), що характеризує рівень відповідності нейронної мережі до розв’язуваної нею задачі. Інформацію про довкілля здебільшого задають у вигляді навчальної вибірки образів або зразків, що їх оброблюючи мережа дістає відомості, необхідні для отримання шуканого розв’язку. Саме характер та обсяг цієї інформації визначають як тип навчання, так і конкретний метод.

З погляду математики, навчання нейронних мереж – це багатопараметрична задача нелінійного оптимування. Більшість методів навчання можна розділити на два класи: навчання з учителем (із заохоченням) та навчання без учителя (без заохочення, або самонавчання). Методи навчання з учителем застосовують у випадках, коли відома бажана реакція системи в кожну мить часу, себто відомий навчальний сиґнал, який упливає на налаштування параметрів системи, що навчається. Рівень «навченості» системи формально визначають за значенням цільової функції, тобто за тим станом, якого має в результаті набути коректно навчена система.

У цій схемі «учителю» відома інформація про довкілля, представлена послідовністю або пакетом ухідних векторів x , а також «правильна реакція» на ці сиґнали, представлена навчальним сиґналом y??. В процесі навчання реакція нейронної
мережі y розбігається з «правильною» реакцією вчителя, через що постає похибка e = y?? ??? y . Мета навчання – так налаштувати параметри ШНМ, щоби деяка скалярна функція похибки E(e) (критерій якості) досягла свого найменшого значення. Навченою вважають мережу, яка в деякому, переважно статистичному сенсі повторює реакцію вчителя. Позаяк інформація про довкілля здебільшого має нестаціонарний характер, навчання відбувається безперервно, для чого використовують ті чи інші рекурентні процедури.

Альтернатива навчанню «з учителем» – навчання за умов, коли правильна реакція на сиґнали довкілля невідома. Цю парадигму називають навчанням «без учителя», або самонавчанням. Штучні нейронні мережі, що самонавчаються, здебільшого, призначені аналізувати внутрішню латентну структуру вхідної інформації та розв’язують задачі автоматичного класифікування, кластерування, факторного аналізу, компресування даних тощо. У теорії ШНМ самонавчання звичайно розглядають як конкурування або самоорганізування нейронів мережі, що топологічно взаємозв’язані між собою.

Щоби покращити якість навчання та прискорити збіжність, ітераційне навчання можна повторювати циклічно на так званому «вікні» – наборі послідовних значень навчального сиґналу (у дискретному випадку) або проміжкові часу (у неперервному випадку). Граничним варіантом процедур з повторюванням є пакетне та послідовне (у реальному часі) модифікування. Під пакетним режимом розуміють випадок, коли всю вибірку даних задано попередньо, а навчання відбувається «епохами». У режимі реального часу повторювання відсутнє, хоча якщо зорганізувати навчання в «прискореному» машинному часі, навчання може повторюватися. Це не суперечить концепції реального часу, бо саме поняття «реальний час» залежить од певної ЕОМ. До прикладу, якщо дані надходять щохвилини, то швидкодійна ЕОМ за 1 хвилину може обробити увесь масив даних кілька разів.

\section{Гібридні системи обчислювального інтелекту}

В обчислювальному інтелекті вельми поширеним є підхід створювати систем обробляння даних на основі кількох наукових напрямів. Як засвідчують теоретичні та практичні результати, таким гібридним системам властивий синергетичний ефект, тобто вони виявляють такі властивості, яких не мають системи, що їх утворюють [85].
Одним з яскравих прикладів гібридних систем обчислювального інтелекту є нейро-фазі системи, які поєднують у собі нейронні мережі другого покоління та нечіткі системи [9, 85]. Нейронна мережа може навчатися на вхідних та вихідних даних для визначення поведінки системи, але отримані знання будуть сховані в її синапсових вагах і їх не можна буде витлумачити. Однак, якщо виразити ваги нейронної мережі за допомогою нечітких правил, з’являється можливість подолати неінтерпретовність результатів роботи нейронної мережі. У такий спосіб нейрофаззі системи дають змогу створювати системи обробляння інформації та отримують більш шіроке застосування.
Розвиваючи гібридний підхід, запропоновано й просунутіші поєднання наукових напрямків, наприклад, теорії штучних нейронних мереж та індуктивного моделювання даних [86]. Ефективність кластерування даних залежить у великій мірі від якості обраної математичної моделі розв’язуваної або досліджуваної задачі. Як уже мовилося вище, однією з проблем кластерування даних є змінна кількість кластерів оброблюваних даних. Відповідно постає складна задача обрати належну математичну модель. Індуктивне моделювання має тут ефективний розв’язок: налаштовувати не лише параметри системи обробляння даних, але також і її структуру. Проекція такого підходу на штучні нейронні мережі веде до ідеї змінювати кількість нейронів в шарах мережі, що обробляє дані. Дієвість роботи побудованих за цим принципом систем [87, 88] засвідчує плідність гібридного підходу.

%%\section {Гибридные нейро-фаззи системы и вопрос структурной адаптации}
\section {Гібрідні нейро-фаззі системи та питанная структурної адаптації}

В наш час для розв'язання задач, які пов'язані з інтелекутальною обробкою даних в умовах апріорної та поточної невизначеності, дослідники часто не обмежуються використанням якогось одного підходу (нейронні мережі, нечітка логіка, генетичні алгоритми тощо), а задля синергії зв'язують групу методів в одну гібридну систему~\cite{ref86, ref87} . Такі системи відповідають усім вимогам до інтелектуальних систем і отримали назву гібридні системи обчислювального інтелекту. Нейро-фаззі системи та м'які обчислювання є напрямами дисципліни обчислювального інтелекту, які й займаються проблемами таких систем. 
Серед основних характеристикамі систем, шо розробляються в рамках нейро-фаззі систем та м'яких обчислень, можна виділити наступні \hl{[27]}:
\begin{itemize}
\item обислювальні моделі, що засновані на біологічних прототипах,
\item паралельна обробка данних у послідовному режимі,
\item в основі системи лежать експертні знання,
\item стійкість системи до зашумління,
\item стійкість системи до виходу із строю підсистем.
\end{itemize}

%%Основными характеристиками систем, разрабатываемых в рамках направления нейро-фаззи системы и мягкие вычисления, являются :
%%– экспертные знания, заложенные в систему;
%%– вычислительные модели, основанные на биологических прототипах;
%%– «интенсивные вычисления» – параллельная обработка в последовательном
%%режиме;
%%– устойчивость к зашумлённым данным;
%%– устойчивость к выходу из строя элементов подсистем;

Варто зазначити, що одною із головних умов до такого типу систем, є їх орієнтованість на розв'язання практичних завдань, що означає здатність оброблювати великі масиви даних великої розмірності, які можуть мати пропущені та зашумленні значення. Однак навчання таких систем зводиться до налаштування сінаптичних коефіцієнтів та/або адаптації бази нечітких правил. Тобто архітектура такої системи не зазнає жодних змін, що може в деяких випадках призвести до погіршення точності результатів. В зв'язку з цим видаюється очевидно корисним зсінтезувати таку гібридну нейро-фаззі архітектуру та такі алгоритми її навчання, що здатні змінувати не тільки параметри систему, а й її архітектуру.

%%Кроме того, необходимо отметить, что одним из требований, выдвигаемых к таким системам, является их ориентирование на практические задачи, т.е. способность обрабатывать значительные объёмы данных большой размерности, содержащие пропущенные и зашумлённые значения. Однако обучение в таких системах сводится к настройке синаптических коэффициентов и/или адаптации базы нечётких правил, не затрагивая саму архитектуру, что может приводить к снижению точности получаемого решения. В связи с этим представляется весьма полезным синтезировать такую гибридную нейро-фаззи архитектуру и алгоритмы её обучения, которые могли бы настаивать не только параметры нейро-фаззи системы, но и саму её архитектуру в целом.

Як вже зазначалось, основною метою навчання є отримання нейронної мережі, яка здатна у найкращий спосіб відтворювати попередньо невідоме відображення $R^{n} \rightarrow R^{m}$. В якості такого відображення може виступати залежність вихідних параметрів процесу від вхідних, прогнозування від передісторії, класа об'єкту від набору його властивостей, управляючої дії від поточного стану об'єкта тощо. Коректне налаштування не тільки синаптичних коефіцієнтів, а й архітектури нейронної мережі, зокрема налаштування кількості шарів та кількості нейронів у кожному шарі, дозволяю суттєво покращити показники такої мережі. Серед підходів до налаштування архітектури нейронної мережі виділяють:
\begin{itemize}
\item деструктивний підхід: за основу береться заздалегіть надлишково складна модель, до неї застосовуються різні процедури, що видаляють із вихідної мережі елементи, які оказують негативниий або незначний позитивний ефект на кінцевий результат,
\item конструктивний підхід: за основу береться максимально проста модель (складається із одного або декількох нейронів), до неї застосовуються процедури, що додають вихідній мережі нові елементи до певного моменту, в залежності від використовуємого алгоритму. Як варіант, конструктивний алгоритм може стартувати з цілком нульової архітектури та самостійно генерувати шари мережі в процесі своєї роботи.
\end{itemize}

\subsection {Деструктивнийй підхід до налаштування архітектури нейронної мережі}

%%\subsection {Деструктивный подход к настройке архитектуры нейронной сети}
 
Основна ідея деструктивних алгоритмів полягає у видаленні параметрів, що мабть найменьший вплив на вихідний сигнал мережі. У ряді публікацій \hl{30-32} був сформульований та підтвержене припущення, що використання деструктивних алгоритмів нерідко призводить до покращення узагальнюючих властивостей мережі, допомагає нейтралізувати появу так званого ефекту перенавчання, а, крім того, після закінчення роботи такого алгоритму, архітектура мережі набуває меншого так більш простого вигляду, що очевидно позитвино відбивається на її обчислювальній складності.

У процесі функціонування деструктивних алгоритмів із мережі могут бути цілком видалені як деякі вхіжні параметри або вузли у прихованих шарах, так і лише деякі синаптичні зв'язки між нейронами, що мають лише один параметр hl{дефіс} ваговий коефіцієнт. Вочевидь, в основі деструктивного піходу до структурної адаптаціі нейрнонної мережі повина бути закладено обчислення деякої \hl{міри значущості}, яка буде характеризувати ступінь впливу кожного конкретного параметру на вихідний сигнал.

В одній із перших публікацій, що розглядає цю проблему, \hl{30} автори запропонували деструктивний алгоритм структурної оптимізації під назвою Optimal Brain Damage(OBD), яких складається із наступних етапів:
\begin{enumerate}
\item вибрати архітектури нейронної мережі,
\item використовуючи один із методів мінімізації цільової функції якості, провести навчання мережі,
\item для кожного елементу мережі обчислити міру значущості:
\begin{equation}\label{eq:obd}
s_{q}=h_{q}u_{q}^2/2,
\end{equation}
\medskip
де $u_{q}$ -- вихідний сигнал $q$-го елементу мережі,\\
$h_q$ розраховується по формулі:
\begin{equation}\label{eq:h_q}
h_q=\sum\limits_{\left(i,j\right)\in V_q}\frac{\partial^{2}{E}}{\partial{w_{ij}^{2}}},
\end{equation}
\medskip
де $V_q$ -- множина пар коефіцієнтів $i$ та $j$ для $q$-го елемента мережі,\\
E -- помилка на виході нейронної мережі,\\
$w_{ij}$ -- синаптичний ваговий коефіцієнт в $j$-м шарі,
\item видалити із мережи деяку кількість елемантів, для яких міра значущості $s_q$ найменьша. В цьому контексті під видаленням елементу мається на увазі зміна вихідного значення елемента на 0 та замороження його в такому стані,
\item повернутися на крок 2 та повторити процедуру.
\end{enumerate}

Вочевидь, при використанні такого методу значно збільшується обчислювальна складність алгоритма навчання, і через необхідність розрахувати міру значущості для кожного нейрону, і через додаткові ітерації перенавчання, які необхідно виконати після видалення кожного нейрона із мережі. Також суттєвим недоліком є те, що разом з видаленням нейрона ми видаляємо одразу декілька синаптичних зв'язків, хоча деякі із них можуть бути корисними. В \hl{31} було запропаново спосіб обійти цей недолік, а разом із тим збільшити швидкість процесу навчання. Цей подхід отримав назву Optimal Brain Surgeon(OBS) і складається він із наступних етапів:
\begin{enumerate}
\item вибрати мережу із достатньо надлишковою архітектурою та провести її навчання,
\item обчислити $H^{-1}$ - матрицю зворотню до гессіану: $H = \frac{\partial^{2}{E}}{\partial{w^{2}}}$,
\item обчислити міру значущості для кожного елемента:
\begin{equation}\label{eq:L_q}
L_q=w_{q}^{2}/\left(2[H^{-1}]_{qq}\right),
\end{equation}
\medskip
де $w_q$ -- $q$-ий ваговий коефіцієнт в мережі,
\item якщо мінімальний $S_q$ значно меньший за поточну помилку, то $w_q$ має бути видалений, після чого перейти до кроку номер 5, в іншому разі перейти до кроку номер 6,
\item оновити вектор вагових коефіцієнтів мережі, використовуючи наступний вираз:
\begin{equation}\label{eq:delta_w}
\delta{W}=-\frac{w_q}{[H^{-1}]_{qq}}H^{-1}\zeta_{q},
\end{equation}
\medskip
де $\zeta_{q}$ -- орт-вектор в площині вагових коефіцієнтів мережі, який відповідає $q$-й синаптичнй вазі,
\item усі незначущі вагові коефіцієнти видаляються, після чого бажано перенавчити мережу.
\end{enumerate}

Слід зауважити, що перший же крок цього методу, а саме вибір критерію надмірності архітектури, може викликати багато запитань. В цілому ж, запропонований в \hl{[31]} метод також характеризується значною обчислювальною складністю, хоч і меншою, в порівнянні з OBD, оскільки він не потребує перенавчання всю мережу після видалення кожного вагового коефіцієнта. Але описаним вище методам притаманний ще один істотний недолік - необхідність мати вже навчену нейронну мережу до початку роботи алгоритму. Це обмеження вдалося обійті завдяки методу, що був запропонований у \hl{[36]}. Міра значущості для вагових коефіцієнтів визначається через тестову статистику T, опираючись на умову того, що вага обнуляється у процесі навчання мережі:

\begin{equation}\label{eq:T_w}
T_{(w_q)}=\log\left(\frac{\left|\sum\limits_{k=1}^{N}\left(w_q-\eta{\left(\frac{\partial{E}}{\partial{w_q}}\right)}_k\right)\right|}{\eta\sqrt{\sum\limits_{k=1}^{N}{\left({\left(\frac{\partial{E}}{\partial{w_q}}\right)}_k-\left(\frac{\partial{E}}{\partial{w_q}}\right)\right)}^2}}\right),
\end{equation}
\medskip

де $N$ -- кількість прикладів у виборці.

У рамках деструктивного підходу відомі також і багато інших методів оптимізації архітектури нейронної мережі \hl{[33-35,37-39]}, проте в силу специфіки цього підходу всім їм в тій чи іншій мірі властива додаткова обчислювальна складність, а крім того орієнтація на нейронні мережі з архітектурою типу багатошарового персептрона. Використання таких алгоритмів для налаштування інших архітектур, зокрема для нейро-фаззі систем, неможливо, а розробка видається недоцільною, оскільки в будь-якому разі використання деструктивного алгоритму буде мати прямий негативнй вплив як на час навчання, так і на час функціонування мережі. Для систем, що розробляються в рамках напряму нейро-фаззі і м'яких обчисленнь, час є досить критичним параметром, оскільки ці системи орієнтуються на рішення практичних задач. У зв'язку з цим вельми привабливо виглядає використання конструктивного підходу для синтезу архітектури нейронної мережі, про що йдеться далі.

\subsection{Конструктивний підхід до налаштування архітектури нейронної мережі}
%\subsection{Конструктивный подход к настройке архитектуры нейронной сети}

Суть конструктивного підходу полягає в нарощуванні архітектури нейронної мережі і налаштування її вагових коефіцієнтів паралельно доки не будуть задоволені вимоги критерію зупинки. Тобто цец підхід дозволяє не тільки уникнути перенавчання нейронної мережі, а й оптимізацує структуру й архітектури мережі на етапі навчання.

%Суть конструктивного подхода заключается в наращивании архитектуры нейронной сети и настройке её весовых коэффициентов параллельно до того момента, пока не будут удовлетворены требования критерия останова. Этот подход (как и деструктивный) помогает избежать переобучения нейронной сети, а также производит структурную оптимизацию её архитектуры на этапе обучения.

Таким чином, використовуючи конструктивний підхід, можна повністю вирішити питання про вибір початкової архітектури мережі: в загальному випадку вона повинна бути максимально простою, складаєтися з одного або декількох нейронів (залежить від методу). Слід зауважити, що, як правило, в результаті роботи конкретного конструктивного алгоритму на виході виходить нейронна мережа з нетрадиційною архітектурою.

%Используя конструктивный подход, удаётся полностью решить вопрос о выборе начальной архитектуры сети: в общем случае она должна быть максимально простой, состоящей из одного или нескольких нейронов (зависит от метода). Следует заметить, что, как правило, в результате работы конкретного конструктивного алгоритма на выходе получается нейронная сеть с нетрадиционной архитектурой.

В \hl{[40]} Джон Платт описує нейронну мережу (RAN - Resource-Allocating Network), яка в процесі навчання додає в свою архітектуру нові обчислювальний елементи (штучні нейрони) кожен раз, коли на вхід мережі подається новий навчальний приклад, який в RAN має двошарову архітектуру. Перший шар складається з нейронів, які відповідають за локальну область з простору вхідних сигналів. У разі якщо вхідний сигнал віддаляється від області конкретного нейрона, то значення сигналу на його виході буде зменшуватися відповідно до співвідношення:

\begin{equation}\label{eq:x_j}
x_j=
\begin{cases}
{\left(1-\left(z_j/\chi{w^{2}_j}\right)\right)}^2,\text{ ЯКЩО } z_j<\chi{w_j}^2\\
0,\text{      ІНАШКЕ}
\end{cases}
\end{equation}
\medskip
\begin{equation}\label{eq:z_j}
z_j=\sum\limits_{j}{\left(c_{ij}-X_i\right)}^2,
\end{equation}
\medskip

де $X_i$ -- $i$-ий вхід нейронної мережі,

$c_{ij}$ -- $i$-ий ваговий коефіцієнт $j$-го нейрона першого шару,

$\chi$ -- параметр налаштування, який підбирається емпіричним шляхом.

%Одной из первых работ в этом направлении, получившей бурное развитие, является работа Джона Платта. В [40] он описывает нейронную сеть (RAN – Resource-Allocating Network), которая в процессе обучения добавляет в свою архитектуру новые вычислительный элементы (искусственные нейроны) каждый раз, когда на вход сети подаётся новый обучающий пример, который в RAN имеет двухслойную архитектуру. Первый слой состоит из нейронов, которые отвечают за локальную область из пространства входных сигналов. В случае если входной сигнал удаляется от области конкретного нейрона, то значение сигнала на его выходе будет уменьшаться согласно соотношению:

Виходи першого шару $x_j$ подаються на другий шар, який агрегує ці значення і генерує вихідний сигнал. Метою кожного синапсу другого шару є визначити, який вплив надає кожен нейрон першого шару на формування конкретного цільового вектора $\vec{y}$. Вихідним сигналом мережі $\vec{y}$ є зважена сума виходів першого шару плюс незалежний вектор $\vec{\gamma}$, що містить постійні елементи:
\begin{equation}\label{eq:vec_y}
\vec{y}=\sum\limits_{j}\vec{w}^{[\circ]}x_j+\vec{\gamma},
\end{equation}
\medskip
де $\vec{w}^{[\circ]}$---вектор синаптичних ваг вихідного шару, або в скалярній формі:
\begin{equation}
y_i=\sum\limits_{j}w^{[\circ]}_{ji}x_j+{\gamma}.
\end{equation}
\medskip

%Выходы первого слоя xj подаются на второй слой, который агрегирует эти значения и генерирует выходной сигнал. Целью каждого синапса второго слоя является определить, какое влияние оказывает каждый нейрон первого слоя на формирование конкретного целевого вектора y. Выходным сигналом сети y является взвешенная сумма выходов первого слоя плюс независимый вектор   , содержащий постоянные элементы:

Також $\vec{\gamma}$ є виходом нейронної мережі у випадку, якщо не активувався жоден з нейронів першого шару. У певному сенсі вираз ${\vec{w_j}^{[\circ]}x_j}$ може розглядатися як певний адитивний елемент, який може бути використаний для того, щоб отримати бажаний вихідний сигнал. 
Навчання RAN починається з нульового стану, тобто в першому шарі не міститься жодного нейрона, а в другому кількість нейронів дорівнєю розмірності задачі, проте, на цьому етапі всі вони, за вийнятком зсуву ${\gamma}$, не мають вхідних параметрів. Після подачі на вхід першого навчального прикладу у вхідній шар додається перший нейрон, центр функції активації \eqref{eq:x_j} якого встановлено наступним чином:
\begin{equation}\label{eq:vec_c_i}
\vec{c_i}=\vec{X},
\end{equation}
\medskip
де $k$-- номер прикладу у виборці.
%Также   является выходом нейронной сети в случае, если не активировался  [o] ни один из нейронов первого слоя. В определённом смысле выражение wj xj может рассматриваться как некоторый аддитивный элемент, который может быть добавлениливычтениз  длятого,чтобыполучитьжелаемыйвыходнойсигнал. Обучение RAN начинается с нулевого состояния, т.е. первый слой не содержит ни одного нейрона, а второй – ровно столько, какова размерность решаемой задачи, однако, на этом этапе все они не имеют входных параметров (за исключением смещения  ). После подачи на вход первого обучающего примера во входной слой добавляется первый нейрон с центром активационной функции (1.6) установленным следующим образом:

Вихід першого шару автоматично передається на всі нейрони другого шару, а його лінійні синапси налаштовуються таким чином, щоб різниця між виходом мережі і навчальним сигналом була мінімальна:
\begin{equation}
\vec{w_j}^{[\circ]}=\vec{Y}-\vec{y},
\end{equation}
\medskip
де $\vec{Y}$ -- бажаний вихід мережі.
%Также выход первого слоя автоматически распространяется на все нейроны второго слоя, а его линейные синапсы настраиваются таким образом, чтобы разница между выходом сети и обучающим сигналом была минимальна:

Доданий нейрон буде реагувати на нові вхідні сигнали, якщо вони будуть перебувати в певному інтервалі, який визначаються відстанню між найближчим вектором і новим вхідним образом
\begin{equation}
w_i=\omega\left\|\vec{X}-\vec{c}_{nearest}\right\|,
\end{equation}
\medskip
де $\omega$ -- параметр покриття, підібраний емпіричним шляхом. Чим більше значення цього параметра, тим на більшу кількість вхідних сигналів будуть реагувати вже існуючі нейрони першого шару.

%где   – параметр покрытия, выбираемый эмпирически. Чем больше значение этого параметра, тем на большее количество входных сигналов будут реагировать уже существующие нейроны первого слоя.

%В RAN используются два условия для добавления нового нейрона в первый слой сети. Во-первых, это происходит в случае, если входной сигнал находится далеко от уже существующих центров функций активации нейронов первого слоя:

У RAN використовуються дві умови для додавання нового нейрона в перший шар мережі. По-перше, це відбувається в тому випадку, якщо вхідний сигнал знаходиться далеко від вже існуючих центрів функцій активації нейронів першого шару:
\begin{equation}\label{eq:introActivFuncFirstLayer}
\left\|\vec{X}-\vec{c}_{nearest}\right\|>\delta{(t)},
\end{equation}
\medskip
а також у випадку, коли із допомогою поточного набору елементів не вдається забезпечити необхідну точніть вихідного сигналу:
\begin{equation}\label{eq:introActivFuncOutputSignal}
\left\|\vec{Y}-\vec{y}{(\vec{X})}\right\|>\varepsilon,
\end{equation}
\medskip
де $\varepsilon$ -- необхідна точність вихідного сигналу.
Якщо при подачі на вхід нового вектора на виході мережі ми отримуємо помилку більшу, ніж $\varepsilon$, то у вхідний шар мережі додається новий нейрон з центрами активаційних функцій, налаштованими на поточний вхідний образ.
Відстань $\delta{(k)}$ -- динамічний параметр, який змінює своє значення протягом процесу навчання. Для його калькуляції використовується наступний вираз:
\begin{equation}
\delta(k)=max\left({\delta_{max}e^{-i/{\tau}},\delta_{min}}\right),
\end{equation}
\medskip
де $\delta_{max}$, $\delta_{min}$, $\tau$ -- параметри, що вибираються емпірічно.

 Якщо згідно з умовами  \eqref{eq:introActivFuncFirstLayer} і \eqref{eq:introActivFuncOutputSignal} не потребується додавання нового нейрона у вхідний шар, то проводиться підналаштування вагових коефіцієнтів вихідного шару. Для цього можуть використовуватися градієнтні методи мінімізації або ж метод найменших квадратів.
 
%На первых этапах обучения в сеть преимущественно добавляются новые элементы, однако спустя некоторое время этот процесс замедляется и вместо добавления новых нейронов во входной слой производится настройка синаптических весовых коэффициентов выходного слоя. Такой порядок работы конструктивного алгоритма становится возможным, благодаря использованию двух условий добавления нового нейрона (1.13, 1.14), и обеспечивает оптимальную сложность модели нейронной сети наряду с хорошим уровнем обобщающих способностей. В случае использования исключительно (1.13) наиболее вероятно, что мы столкнёмся с эффектом переобучения, а в случае – (1.14) могут быть пропущены некоторые нейроны входного слоя, что повлияет на точность выходного сигнала сети.

На перших етапах навчання в мережу переважно додаються нові елементи, проте через деякий час цей процес сповільнюється і замість додавання нових нейронів у вхідному шар відбувається налаштування синаптичних вагових коефіцієнтів вихідного шару. Такий порядок роботи конструктивного алгоритму стає можливим завдяки використанню двох умов додавання нового нейрона \eqref{eq:introActivFuncFirstLayer} та \eqref{eq:introActivFuncOutputSignal}, забезпечує оптимальну складність моделі нейронної мережі поряд з хорошим рівнем узагальнюючих здібностей. У разі використання виключно \eqref{eq:introActivFuncFirstLayer} найбільш ймовірно, що це призведе до перенавчання, а в разі -- \eqref{eq:introActivFuncOutputSignal} можуть бути пропущені деякі нейрони вхідного шару, що вплине на точність вихідного сигналу мережі.

Серед недоліків запропонованого Джоном Платтом методу має сенс назвати досить велику кількість емпірично підбираємих параметрів, від яких безпосередньо залежить якість роботи RAN.

Надалі підхід до конструктивної організації архітектури нейронної мережі, відомий в англомовній літературі під назвою resource allocation, породив безліч різних модифікацій, які спрямовані на оптимізацію швидкості навчання і точності вихідного сигналу при вирішенні певного кола завдань \hl{[41-44]}.

%В рамках конструктивного подхода можно выделить такое направление, как каскадные нейронные сети [45-50], наиболее характерным и эффективным представителем которых является каскадно-корреляционная архитектура, предложенная С. Фальманом и К. Лебьером в работе [45]. Основная особенность сети этого типа заключается в возможности добавления новых узлов в процессе обучения. На рис. 1.1 приведена схема подобной сети, содержащая три каскада, n входов и один выход.

В рамках конструктивного підходу можна виділити такий напрямок, як каскадні нейронні мережі \hl{[45-50]}, найбільш характерним і ефективним представником яких є каскадно-кореляційний архітектура, запропонована Фальманом і \hl{Лебьером} в роботі \hl{[45]}. Основна особливість мережі цього типу полягає в можливості додавання нових вузлів в процесі навчання. На \hl{рис. 1.1} приведена схема подібної мережі, яка містить три каскаду, n входів і один вихід.

%В начале процесса обучения формируется стандартная однослойная структура с n входами и 1 единственным выходом (рис. 1.1), которая обучается при помощи того или иного нелинейного метода обучения.

На початку процесу навчання формується стандартна одношарова структура з $n$ входами і єдиним виходом \hl{(рис. 1.1)}, яка навчається за допомогою того чи іншого нелінійного методу навчання.

%После предъявления всей обучающей выборки x(1),x(2), ,x(N) оценивается точность аппроксимации и в случае, если ошибка слишком велика, формируется каскад из n2 нейронов-кандидатов, параллельно подключенных к входам сети 1, x1 , x2 , , xn и выходу первого каскада o[1] . Нейроны-кандидаты, как правило, отличаются друг от друга начальными значениями синаптических весов W[2](0), видом функций активации и алгоритмами обучения. Далее производится обучение нейронов второго каскада при «замороженных» синаптических весах W[1] (N) первого. На рис. 1.1 «замороженные» веса показаны в виде заштрихованных сумматоров. Среди n2 нейронов-кандидатов выбирается один нейрон-победитель, у которого параметр корреляции [46]

Після прохождення всієї навчальної вибірки $x(1),x(2),\dots,x(N)$ оцінюється точність апроксимації і в тому випадку, якщо помилка занадто велика, формується каскад з $n_2$ нейронів-кандидатів, паралельно підключених до входів мережі $1,x_1,x_2,\dots,x_n$ і виходу першого каскаду $o^{[1]}$. Нейрони-кандидати, як правило, відрізняються один від одного початковими значеннями синаптичних вагових коефіцієнтів $W^{[2]}(0)$, видом функцій активації та алгоритмами навчання. На наступному етапі проводиться навчання нейронів другого каскаду при «заморожених» синаптичних коефіцієнтах $W^{[1]}(N)$ першого каскаду. На \hl{рис. 1.1} «заморожені» коефіцієнти показані у вигляді заштрихованих суматорів. Серед $n_2$ нейронів-кандидатів вибирається один нейрон-переможець, параметр кореляції якого \hl{[46]}
\begin{equation}
R^2_{[q]}=\left|\sum^{N}\limits_{k=1}\left(o^{[2]}_q\left(k\right)-\bar{o}_q^{[2]}\right)\left(e^{[2]}_q\left(k\right)-\bar{e}_q^{[2]}\right)\right|,\text{ } q=1,2,\dots,n_2
\end{equation}
\medskip
%(здесь oq[2] и eq[2] – средние значения выходного сигнала и ошибки) является максимальным. Именно этот нейрон с «замороженными» весами W [ 2 ] ( N ) образует второй каскад, в то время как «проигравшие» нейроны изымаются из сети.

(тут $\bar{o}_q^{[2]}$ і $\bar{e}_q^{[2]}$ - середні значення вихідного сигналу і помилки) є максимальним. Саме цей нейрон з «замороженими» вагами $W^{[2]}(N)$ утворює другий каскад, в той час як нейрони, які програли, видаляються з мережі.

%Далее оценивается точность аппроксимации, обеспечиваемая вторым каскадом, и в случае необходимости формируется набор из n3 кандидатов третьего каскада, среди которых выбирается победитель с максимальным значением

Далі оцінюється точність апроксимації, що забезпечується другим каскадом, і в разі потреби формується набір з $n_3$ кандидатів третього каскаду, серед яких вибирається переможець з максимальним значенням
\begin{equation}
R^3_{[q]}=\left|\sum^{N}\limits_{k=1}\left(o^{[3]}_q\left(k\right)-\bar{o}_q^{[3]}\right)\left(e^{[3]}_q\left(k\right)-\bar{e}_q^{[3]}\right)\right|,\text{ } q=1,2,\dots,n_3
\end{equation}
\medskip
У разі досягнення необхідної точності процес нарощування каскадів завершується і вихідний сигнал останнього каскаду (на рис. 1.1 -- $\bar{o}_q^{[3]}$) приймається в якості вихідного сигналу мережі в цілому.
В якості основних відмінних рис каскадно-кореляційних мереж слід зазначити наступні:
\begin{itemize}
\item ці мережі не вимагають попереднього завдання ні архітектури, ні кількості нейронів в каскадах,
\item нейрони в мережу додаються в міру необхідності, утворюючи не приховати шари, а каскади, кожен з яких в якості вхідних сигналів використовує входи мережі і виходи попереднього каскаду,
\item навчання не пов'язане з концепцією зворотнього поширення помилок, що дозволяє істотно скоротити час налаштування
\item за рахунок «заморожування» синаптичних ваг сформованих раніше каскадів скорочуються обчислювальні витрати на навчання.
\end{itemize}

Головним недоліком даних мереж прийнято вважати неможливість їх навчання в режимі послідовної обробки інформації \hl{[51]}. Далі буде показано, як можна подолати це обмеження, синтезувавши на основі мережі, запропонованої в \hl{[45]}, архітектуру, яка буде відповідати критеріям, які висуваються до нейро-фаззі систем.

\subsection{Задача нечіткого кластерування даних}

Кластеризація є основним інструментом для аналізу даних. Він знаходить широке застосування в багатьох інженерних і наукових областях, включаючи розпізнавання образів, виділення ознак, вектор квантування, сегментації зображень, біоінформатики і інтелектуального аналізу даних. Кластеризація є класичним методом вибору прототипу ядра на основі нейронних мереж, таких як RBF мережі, і є найбільш корисним для нейро нечітких систем.
Кластеризація є неконтрольований метод класифікації, яка ідентифікує деяку неіз структуру, присутню в ЛОР безлічі об'єктів на основі показника подібності. Методи кластеризації можуть бути отримані зі статистичних моделей або конкурентного навчання, і, відповідно, вони можуть бути класифіковані в генеративної (або на основі моделі) і дискриміна- TIVE (або подібності на основі) підходів. Проблема кластеризация також може бути змодельована як КС. Кластеризація нейронні мережі є статистичні моделі, де функція щільності ймовірності (PDF) для даних оцінюється шляхом вивчення його параметрів.

Нечітке кластерування даних – один з напрямів кластерного аналізу, що використовує для обробляння даних деякі принципи та елементи нечіткої логіки [77, 78]. Концептуальний взаємозв’язок між кластерним аналізом та нечіткою логікою ґрунтується на тій обставині, що розв’язання задач структурування складних систем формує здебільшого кластери об’єктів, що є розмиті, нечіткі за своєю природою. Така нечіткість може полягати в тому, що перехід од належності до неналежності образів щодо певних кластерів радше поступовий, аніж стрибкуватий. Тому адекватнішою в таких випадках є не однозначна належність до певного кластеру, а низка рівнів належності до кількох кластерів. Вимога однозначно розкластерувати елементи досліджуваної проблемної області є вельми грубою та жорсткою, особливо у випадках, коли треба розв’язати погано або слабко структуровані задачі інтелектуального аналізу даних. Засоби нечіткого кластерування послаблюють цю вимогу введенням до розгляду нечітких кластерів та їхніх функцій належності, які приймають значення на інтервалі [0, 1].

З-поміж цілої низки методів та підходів нечіткого кластерування особливе місце займають методи, що ґрунтуються на цільових функціях [79]. Такі методи розв’язують задачу обробляння даних, оптимуючи деякий заздалегідь заданий критерій якості. Найвідомішим представником цього класу методів є метод нечітких с-середніх [77], що його широко застосовують у задачах різноманітної складності, коли навчальний сиґнал невідомий. Але хоча стандартний метод нечітких с-середніх є значно просунутіший супроти методів чіткого кластерування, але все ж таки й він має вади. Справа в тому, що однією
з умов використовування цього методу є вимога, щоби сума рівнів належності будь-якого образа за всіма кластерами дорівнювала одиниці. Ця штучна вимога
у випадках рівновіддаленості деякого образа від усіх кластерів спричиняє те, що такий образ отримує рівень належності до кожного з кластерів, який не залежить од відстані між образом та центром відповідного кластеру. Іще однією вадою методу, яка випливає з попередньої, є припущення, що під час обробляння даних образи, що належать новим кластерам, з’явитися не можуть. Вочевидь, в реалістичних задачах це не завжди так. До того ж образи, що надходять на вхід методу, можуть бути звичайним шумом, завадами. Стандартний метод нечітких с-середніх не впорається з такою ситуацією, що відповідно позначиться на ефективності кінцевого нечіткого розбиття даних.

Зазначену ваду долає метод можливісного нечіткого кластерування даних – метод можливісних с-середніх [77, 80, 81]. Він не має вимог щодо значення суми рівнів належності образів за всіма кластерами, що, відповідно, покращує його дієвість за умов наявності шуму в ухідному сиґналі. Проте цей метод має певні труднощі з його початковим ініціалізуванням.

Беручи до уваги те, що обидва розглянуті методи належать до методів нечіткого кластерування даних, тут годилося б зазначити, що коли виникає потреба виокремити стандартний метод нечітких с-середніх, його називають імовірнісним.

Два розглянуті методи нечіткого кластерування даних – імовірнісний та можливісний – є базові методи, які утворюють цілі сім’ї похідних від них і пристосованих до певних специфічних задач обробляння даних [77]. Зазвичай, кожен зі згаданих методів обробляє дані в пакетному режимі. У ситуаціях, коли дані надходять окремо й їх потрібно обробляти он-лайн, для методів нечіткого кластерування запропоновано послідовні модифікації [77, 82].

Іще одна вада стандартних методів нечіткого кластерування даних – нездатність виявляти кластери складної, несферичної форми. Цю ваду, опріч уже згаданого ієрархічного кластерування, долає метод нечіткого кластерування Ґустафсона-Кесселя, який, замість Евклідової, використовує Магаланобісову метрику [83].

Як видно, існує розмаїття методів нечіткого кластерування, зоснованих на оптимуванні цільової функції. Але усі вони ґрунтуються на припущенні, що дослідникові попередньо відома кількість кластерів, що їх треба виявити. Допевне, таке припущення слушне не для всіх задач, адже інколи кількість кластерів у вхідних даних може бути невідомою або вона може змінюватися в часі. Цей випадок є предметом розгляду систем обробляння даних із мінливою (еволюційною) архітектурою [84], які здебільшого є гібридними системами з поліпшеними архітектурами, що поєднують декілька напрямків обчислювального інтелекту.

%<<chapter2, child='SISOCascadedNetwork.Rnw'>>=
%@

%<<chapter3, child='MIMOCascadedNetwork.Rnw'>>=
%@

%<<chapter4, child='SelfLearningCascadedNetwork.Rnw'>>=
%@

%<<chapter5, child='Experiments.Rnw'>>=
%@

\bibliographystyle{ugost2008ns}
\bibliography{references}	

\end{document}
