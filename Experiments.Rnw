<<echo=FALSE, cache=FALSE>>=
set_parent('thesis.Rnw')
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Моделювання та практичне застосування розроблених методів та архітектур}
\label{ch:Experiments}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Моделювання самонавчанної нейро-фаззі системи, що еволюціонує}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Придумати назву1}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sssec:SelfLearningNetworkArtificialGeneratedExperiments}%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Одна з основних переваг, притаманних пропонованій самонавчанній нейро-фаззі системі, що еволюціонує, полягає в автоматичному визначенні оптимальної кількості кластерів та значення фаззифікатору на кожному етапі обробляння даних. Першу серію експериментів було проведено на штучно зсинтезованих наборах даних з різним ступенем розмитості та перекриття класів аби дослідити вплив значення параметру фаззіфікації на якість кластерування в режимі реального часу відповідно до обраного критерію дійсності.

\begin{figure}
\begin{center} = 6
\includegraphics{clustering01.pdf}
\caption{Штучно сгенеровані набори даних}
\label{fig:clustering01}
\end{center}
\end{figure}

Кожен з наборів даних, що їх наведено на рис.~\ref{fig:clustering01}, містить 80 спостережень з 2 ознаками (для очності) у кожному спостереженні. Тестові дані були сгенеровані таким чином, аби у першому наборі класи були чітко розподілені (crisp dataset), у другому наборі кластерні границі були дещо розмиті (fuzzy dataset), у третьому випадку класи сильно перетиналися (extra fuzzy dataset). Логічно припустити, що система, яка тестується, обере менше значення параметру фаззіфікації для першого датасету та більше для останнього, де границі класів спостережень є більш розмитими.

Спостереження надходили до нейро-фаззі мережі у послідовному режимі, вагові коефіцієнти нейронів були проініціалізовані, використовуючи пакетну модифікацію обраного алгоритму кластерування на датасеті з довільних двадцятьох спостережень відповідного набору даних (адже система, як і класичний fuzzy c-means, \hl{чутлива до ініціалзації)}. Локально оптимальні кількість кластерів та значення параметру фаззіфікації обумовлювалися максимальним середнім значенням рекурентних коефіцієнту розбиття PC \eqref{eq:reccurentPartitioningCoefficient} та Ксі-Бені індексу \eqref{eq:recurrentXieBeniIndex}: $\max{\frac{PC_j^{[m]} + 1 - XB_j^{[m]}}{2}}$ (у данному випадку використовувалося від'ємне значення Ксі-Бені індексу $1-XB\left(k\right)$, оскільки щоменше $XB_j^{[m]}$, то ліпшим є розбиття даних на кластери). 

Для першого набору даних (crsip dataset), як і передбачалося, оптимальним виявися другий каскад ($m=3$) з трьома кластерами і нейроном-переможцем із найменшим значенням параметру фаззіфікацїї $\beta = 2$ (рис.~\ref{fig:clustering02}). Така конфігурація є оптимальною відповідно до обох використовуваних індексів валідності -- найменше значення Ксі-Бені індексу $XB_j^{[m]}$ та найбільший коефіцієнт розбиття $PC_j^{[m]}$: 

\begin{equation*}
\begin{aligned}
PC_1^{[2]}=&0.9009951,\\
XB_1^{[2]}=&0.03349166.
\end{aligned}
\end{equation*}
\medskip

\begin{table}
\centering \small \begin{tabular}{rcccc}
\hline {Каскад 1 ($m=2$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.91758 & 0.7446 & 0.64787 & 0.59236 \\
Індекс Ксі-Бені &
0.052129 & 0.061034 & 0.092235 & 0.1294 \\
%——————————————————————————————————————————% 
\hline {Каскад 2 ($m=3$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.92643 & 0.6609 & 0.50214 & 0.43305 \\
Індекс Ксі-Бені &
0.027232 & 0.06872 & 0.17281 & 0.26914 \\
%——————————————————————————————————————————% 
\hline {Каскад 3 ($m=4$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.87218 & 0.5256 & 0.37605 & 0.31993 \\
Індекс Ксі-Бені &
0.15687 & 0.4153 & 0.84699 & 1.1765 \\
%——————————————————————————————————————————% 
\hline {Каскад 4 ($m=5$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.73909 & 0.45445 & 0.32428 & 0.27063 \\
Індекс Ксі-Бені &
0.12985 & 0.30637 & 0.68584 & 1.0551 \\
\hline
\end{tabular}
\caption{Індекси валідності (датасет 1)}
\end{table}

Лише одне спостереження у цьому датасеті (його позначено багряним квадратом) не належить жодному кластерові з ступінем більшим від $0.6$. Індекси валідності нейронів системи наведені у таблиці 5.1.%\ref{table:kysymys}

\begin{figure}
\begin{center}
\includegraphics{clustering002.pdf}
\caption{Набір даних з чітко вираженими класами (Crisp dataset)}
\label{fig:clustering02}
\end{center}
\end{figure}

Для набору даних з середньою вираженістю класів найліпшим виявився нейрон другого каскаду ($m=3$) і фаззіфікатором $\beta=3$ (таблиця 5.2). 

\begin{table}
\centering \small \begin{tabular}{rcccc}
\hline {Каскад 1 ($m=2$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.78414 & 0.58928 & 0.53853 & 0.52239 \\
Індекс Ксі-Бені &
0.16668 & 0.30834 & 0.3745 & 0.38723 \\
%——————————————————————————————————————————% 
\hline {Каскад 2 ($m=3$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
50084 & 0.71164 & 0.97275 & 0.4191 \\
Індекс Ксі-Бені &
0.009751 & 0.031235 & 0.087474 & 0.1323 \\
%——————————————————————————————————————————% 
\hline {Каскад 3 ($m=4$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.91888 & 0.47532 & 0.32777 & 0.28912 \\
Індекс Ксі-Бені &
0.052563 & 0.1757 & 0.27516 & 0.33766 \\
%——————————————————————————————————————————% 
\hline {Каскад 4 ($m=5$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.85618 & 0.34327 & 0.24778 & 0.22445 \\
Індекс Ксі-Бені &
0.048316 & 0.19887 & 0.34307 & 0.41228 \\
%——————————————————————————————————————————% 
\hline {Каскад 5 ($m=6$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.81295 & 0.30709 & 0.21636 & 0.19214 \\
Індекс Ксі-Бені &
0.060896 & 0.19702 & 0.31393 & 0.38668 \\
\hline
\end{tabular}
\caption{Індекси валідності (датасет 2)}
\end{table}

Як показано на рис.~\ref{fig:clustering03}, декілька спостережень у центрі (позначені багряними квадратами) можна віднести до 2 кластерів з відносно високим ступінем належності, проте більшість спостережнь можна чітко розкластеризувати, що ілюструється високим значенням коефіцієнту розбиття, та дуже низьким Ксі-Бені індексом:

\begin{equation*}
\begin{aligned}
PC_2^{[2]}=&0.9727868,\\
XB_2^{[2]}=&0.087474.
\end{aligned}
\end{equation*}
\medskip

\begin{figure}
\begin{center}
\includegraphics{clustering03.pdf}
\caption{Набір даних з нечіткими межами класів (fuzzy dataset)}
\label{fig:clustering03}
\end{center}
\end{figure}

Для набору з найменш чіткими межами класів (таблиця 5.3), система обрала нейроном-переможцем вузол третього каскаду ($m=4$) з високим параметром фаззіфікації $\beta = 4$:

\begin{equation*}
\begin{aligned}
PC_3^{[3]}=&0.335525,\\
XB_3^{[3]}=&0.2128333.
\end{aligned}
\end{equation*}
\medskip

На рис.~\ref{fig:clustering04} спостереження, для яких ступінь належності до будь-якого кластеру не перевищує $0.6$, позначені багряними квадратами. Як і очікувалося,  для цього набору даних кількість таких спостережень значно вища від попередніх датасетів з більш компактними та <<чіткими>> класами.

\begin{table}
\centering \small \begin{tabular}{rcccc}
\hline {Каскад 1 ($m=2$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.85094 & 0.71415 & 0.61734 & 0.57085 \\
Індекс Ксі-Бені &
0.10584 & 0.11462 & 0.13797 & 0.16101 \\
%——————————————————————————————————————————% 
\hline {Каскад 2 ($m=3$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.61668 & 0.42848 & 0.37779 & 0.35884 \\
Індекс Ксі-Бені &
0.1754 & 0.20364 & 0.22364 & 0.23995 \\
%——————————————————————————————————————————% 
\hline {Каскад 3 ($m=4$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.33458 & 0.44082 & 0.79405 & 0.29615 \\
Індекс Ксі-Бені &
0.20989 & 0.129 & 0.051039 & 0.26282 \\
%——————————————————————————————————————————% 
\hline {Каскад 4 ($m=5$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.50244 & 0.33067 & 0.26029 & 0.23318 \\
Індекс Ксі-Бені &
0.37268 & 0.61417 & 0.79695 & 0.93626 \\
%——————————————————————————————————————————% 
\hline {Каскад 5 ($m=6$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.53279 & 0.29731 & 0.22648 & 0.19858 \\
Індекс Ксі-Бені &
0.27407 & 0.47298 & 0.60569 & 0.70716 \\
\hline
\end{tabular}
\caption{Індекси валідності (датасет 3)}
\end{table}

Для очності у всіх наведених рисунках кольором позначені не тільки розкластеровані спостереження і центри кластерів, а й задній план (фон) малюнків, що дозволяє візуально визначити, до якого кластеру система віднесла б нові спостереження. Не дивно, що, тоді як для перших двох датасетів важко визначити домінуючий колір, оскільки кластери їх спостережень більш менш компактні та явно виражені, для останнього набору даних домінуючий колір -- сірий, сформований кольорами усіх кластерів, що ілюструє великий ступінь перекриття класів і, відповідно, високе значення оптимального параметру фаззіфікації $\beta$, що обрала система. 

\begin{figure}
\begin{center}
\includegraphics{clustering04.pdf}
\caption{Набір даних з класами, що перетинаються (extra fuzzy dataset)}
\label{fig:clustering04}
\end{center}
\end{figure}

Ця низка експериментів проілюструвала як важливо вірно визначати параметр фаззіфікації, оптимальне значення якого у випадку обробляння даних у послідовному режимі з високою вирогідністю змінюється у часі, а саме здатність визначати оптимальне значення цього параметру в онлайн режимі є відмінною особливістю попропонованої самонавчанної нейро-системи.
%\setFloatBlockFor{sssec:SelfLearningNetworkArtificialGeneratedExperiments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Придумати назву2}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sssec:SelfLearningNetworkIris}%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Наступна низка експериментів була проведена на наборі даних <<Іриси Фішера>> (Fisher's Iris data set). 

\begin{figure}
\begin{center}
\includegraphics{HierarchialClusteringOfIrisDataset.pdf}
\caption{Ієрархічне класерування датасету <<Іриси Фішера>>}
\label{fig:HierarchialClusteringOfIrisDataset}
\end{center}
\end{figure}

Це багатовимірний датасет для задачі класифікації, на прикладі якого англійський статистик та біолог Рональд Фішер в 1936 році продемонстрував роботу розробленого ним методу дискримінантного аналізу. Іноді його також називають <<Ірисами Андерсона>> (через те, що дані були зібрані американським ботаніком Едгаром Андерсоном). Цей набір даних став класичним і часто використовується в літературі для ілюстрації роботи різних статистичних алгоритмів.

\begin{figure}
\begin{center}
\includegraphics{ClusteredIrisDatasetM3Beta2.pdf}
\caption{Розкластерований датасет <<Іриси Фішера>> при $m=3$, $\beta = 2$ (Точність кластерування -- 96\%)}
\label{fig:ClusteredIrisDatasetM3Beta2}
\end{center}
\end{figure}

Проте цей датасет рідко використовується у кластерному аналізі, адже межі класів <<Verginica>> та <<Versicolor>> не можна чітко визначити, ґрунтуючись на даних, що їх використовував Фішер (що легко продемонструвати за допомогою ієрархічного кластерування, рис.~\ref{fig:HierarchialClusteringOfIrisDataset}). Саме цим і цікавий для нас цей набір даних: коли класичні методи чіткого кластерного аналізу не справляються з задачею, може стати у нагоді система, що реалізує нечітке кластерування зі змінним параметром фаззифікації та кількістю кластерів. Для більшості методів кластерного аналізу, зокрема для методу нечітких середніх (fuzzy c-means), необхідно заздалегідь задати кількість кластерів, і очевидним рішенням є прийняти $m=3$, адже маємо три класи: Iris Verginica, Iris Versicolor та Iris Setosa (рис.~\ref{fig:ClusteredIrisDatasetM3Beta2}). 

\begin{table}
\centering \small\begin{tabular}{rcccc}
& $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.8313073 & 0.8741245 & 0.8709475 & 0.8888124 \\
min & 0.7859722 & 0.7533766 & 0.6615745 & 0.7656498 \\
max & 0.8534013 & 0.9166667 & 0.935051 & 0.9604701 \\
\end{tabular}
\caption{Точність кластерування при $m=3$}
\end{table}

Точність кластерування за допомогою методу нечітких середніх за таких умов ($m=3$, $\beta=2$) рідко перевищує $83\%$ (таблиця 5.4). (Оскільки для обраного датасету існують мітки з вірною класифікацією, ефективність кластеризації вимірювалася у відсотках точності щодо еталонного розбиття після дефаззіфікації.) Проте, якщо не обмежувати пропоновану систему у кількості кластерів (система ініціалізується інтервалом допустимих значень $m$ (кількість кластерів) та параметру фаззифікації $\beta$), вельми цікавими є результати кластерування нейронів кожного з каскадів.  

У таблицяі 5.5 наведена точність розбиття даних, коли $m \gg 3$ кластерів відповідно. Варто зазначити, що нейрони у пулі кожного каскаду реалізують метод нечітких середніх зі змінним значення фазифікатору, а отже є чутливими до довільно ініціалізовних цетрах кластерів, тому у таблицях наведені середня, мінімальна та максимальна точності кластерування (після дефаззіфікації).

\begin{table}[H]
\centering \small\begin{tabular}{rcccc}
%\hline 
%$m=6$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
%avg & 0.8832960 & 0.9186286 & 0.9044973 & 0.9163232 \\
%min & 0.8209877 & 0.8521505 & 0.8490079 & 0.8458041 \\
%max & 0.9489689 & 0.9679570 & 0.9605802 & 0.9790494 \\
\hline 
$m=7$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.8972948 & 0.9150268 & 0.9242503 & 0.9178207 \\
min & 0.8536056 & 0.8461905 & 0.8723182 & 0.8600289 \\
max & 0.9621849 & 0.9736172 & 0.9810146 & 0.9663462 \\
\hline 
$m=8$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9065560 & 0.9296311 & 0.9243606 & 0.9248976 \\
min & 0.8217056 & 0.8562179 & 0.8577202 & 0.8590278 \\
max & 0.9474588 & 0.9789402 & 0.9848214 & 0.9747899 \\
\hline 
$m=9$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9258154 & 0.9282887 & 0.9308971 & 0.9229753 \\
min & 0.8689921 & 0.8270525 & 0.8684641 & 0.8556390 \\
max & 0.9849170 & 0.9806397 & 0.9664112 & 0.9748284 \\
\hline 
$m=10$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9213191 & 0.9285106 & 0.9332528 & 0.9282907 \\
min & 0.8663370 & 0.8722271 & 0.8652272 & 0.8766667 \\
max & 0.9663420 & 0.9838095 & 0.9723656 & 0.9756335 \\
\hline 
$m=11$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9295315 & 0.9408977 & 0.9317242 & 0.9295800 \\
min & 0.8520268 & 0.8964924 & 0.8890781 & 0.8788656 \\
max & 0.9716166 & 0.9848485 & 0.9704892 & 0.9798627 \\
\hline 
$m=12$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9349407 & 0.9433244 & 0.9337934 & 0.9306632 \\
min & 0.8815133 & 0.8949802 & 0.8798160 & 0.8486111 \\
max & 0.9795274 & 0.9783497 & 0.9630952 & 0.9772727 \\
\hline 
$m=13$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9420998 & 0.9398127 & 0.9375204 & 0.9357708 \\ 
min & 0.8823175 & 0.8614025 & 0.8882479 & 0.8828348 \\
max & 0.9807518 & 0.9788034 & 0.9753452 & 0.9748873 \\
\end{tabular}
\caption{Точність кластерування для $m \in [7,13]$, $\beta \in [2,5]$}
\end{table}

На рис~\ref{fig:IrisClusteringEfficiencyFromNumberOfClustersAndFuzzyfier} зображено залежність точності кластерування від кількості кластерів. Цікаво, що при, здавалося б, очевидному рішенні обрати кількість кластерів рівною трьом, отримуємо чиненайгіршу точнічть кластерування (при $\beta = 2$) після дефаззіфікації щодо еталонного розбиття (Для порівняння на рис.~\ref{fig:ClusteredIrisDatasetM7Beta5} та рис.~\ref{fig:ClusteredIrisDatasetM14Beta4} наведені розбиття, що їх запропонували нейрони-переможці деяких каскадів, де $m \gg 3$).

\begin{figure}
\begin{center}
\includegraphics{IrisClusteringEfficiencyFromNumberOfClustersAndFuzzyfier.pdf}
\caption{Точність кластерування від кількості кластерів та параметру фаззіфіказії}
\label{fig:IrisClusteringEfficiencyFromNumberOfClustersAndFuzzyfier}
\end{center}
\end{figure}

Цьому легко знайти пояснення, адже метод нечітких $k$-середніх (а саме цей метод у цьому експерименті реалізовують вузли пулів кожного каскаду) добре розпізнає кластери лише гіперсферичної форми. Проте кластер довільної (негіперсферичної) форми, можна розбити на декілька гіперсферичних підкластерів, що й відбувається у каскадах, де $m > 3$, що пропонують розбиття на дрібні кластери. На рисунках \ref{fig:3DClusteredIrisDatasetM7Beta5} та \ref{fig:3DClusteredIrisDatasetM12Beta4} наведені розбиття деяких каскадів, де кількість кластерів більша від кількості класів еталонної вибірки; тут можна побачити, що декілька кластерів, що після дефазифікації будуть віднесені до одного класу, наприклад, Iris Virginica розташовані поруч один з одним, тобто є складовими більшого кластеру негіперсферичної форми.


\begin{figure}
\begin{center}
\includegraphics{ClusteredIrisDatasetM7Beta5.pdf}
\caption{Розкластерований датасет <<Іриси Фішера>> при $m=7$, $\beta=5$ (Точність кластерування $\approx 93\%$)}
\label{fig:ClusteredIrisDatasetM7Beta5}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics{ClusteredIrisDatasetM12Beta4.pdf}
\caption{Розкластерований датасет <<Іриси Фішера>> при $m=12$, $\beta=4$ (Точність кластерування $\approx 96\%$)}
\label{fig:ClusteredIrisDatasetM14Beta4}
\end{center}
\end{figure}

\begin{table}
\centering \small\begin{tabular}{rcccc}
& $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9369168 & 0.9448829 & 0.9383179 & 0.9403416 \\
min & 0.8847819 & 0.8953380 & 0.8787879 & 0.9069805 \\
max & 0.9731262 & 0.9754579 & 0.9918301 & 0.9762515 \\
\end{tabular}
\caption{Точність кластерування при $m=14$}
\end{table}

Таким чином, видається доречним, навіть у випадку, коли відоме еталонне розбиття датасету, дозволити системі обрати кінцеву кількість кластерів самостійно, особливо у випадку, коли вузли системи реалізують однаковий метод кластерування. 

\begin{figure}[H]
\begin{center}
\includegraphics{3DClusteredIrisDatasetM7Beta5.pdf}
\caption{Розкластерований датасет <<Іриси Фішера>> при $m=7$, $\beta=5$ (Точність кластерування $\approx 93\%$)}
\label{fig:3DClusteredIrisDatasetM7Beta5}
\end{center}
\end{figure}

Варто зауважити, що у цьому випадку для визначення локально оптимального розбиття доцільно використовувати модифіковані індекси валідності, чи такі, що не залжать від відстані цетрів кластері, наприклад ті, що ґрунтуються на щільності (density-based).

\begin{figure}[H]
\begin{center}
\includegraphics{3DClusteredIrisDatasetM12Beta4.pdf}
\caption{Розкластерований датасет <<Іриси Фішера>> при $m=12$, $\beta=4$ (Точність кластерування $\approx 96\%$)}
\label{fig:3DClusteredIrisDatasetM12Beta4}
\end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Придумати назву 3}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Наступну серію експериментів було проведено на датасеті <<Знання студентів про електричні машини постійного струму>>.

\begin{figure}[H]
\begin{center}
\includegraphics{StudentKnowledgeDataSet.pdf}
\caption{Датасет <<Знання студентів про електричні машини постійного струму>>}
\label{fig:StudentKnowledgeDataSet}
\end{center}
\end{figure}

Цей датасет було додано до UCI репозиторію у 2013 році, він містить 403 паттерни, кожен з п'ятьма атрибутами:

\begin{enumerate}
\item STG: кількість часу, що його витратив(витратила) студент(ка) на навчання цільового матеріалу,
\item SCG: Кількість повторюваннь навчання цільового матеріалу студентом(студенткою),
\item STR: Кількість часу, що його використав(використала) студент(ка) на навчання матеріалу, пов'язаного з цільовим матеріалом,
\item LPR: Оцінка, що її отримав(отримала) студент(ка) на іспиті з предмету, пов'язаного з цільовим предметом,
\item PEG: Оцінка, що її отримав(отримала) студент(ка) на іспиті з цільового предмету,
\end{enumerate}

\begin{figure}[H]
\begin{center}
\includegraphics{3DStudentKnowledgeDataSet.pdf}
\caption{Датасет <<Знання студентів про електричні машини постійного струму>>}
\label{fig:3DStudentKnowledgeDataSet}
\end{center}
\end{figure}

Попарні графіки атрибутів наведено на рис.~\ref{fig:StudentKnowledgeDataSet} та у тримірному просторі на рис.~\ref{fig:3DStudentKnowledgeDataSet}.

\begin{figure}
\begin{center}
\includegraphics{3DClusterizedStudentKnowledgeDataSetM4Beta2.pdf}
\caption{Датасет <<Знання студентів про електричні машини постійного струму>>}
\label{fig:3DClusterizedStudentKnowledgeDataSetM4Beta2}
\end{center}
\end{figure}

Для цього експерименту нейрони-узагальнювачі керувалися рекурентним Ксі-Бені Індексом при визначанні локально-оптимального нейрона (з найлішпшим параметром фаззіфікації) та каскаду (з оптимальною кількістю кластерів).

\begin{figure}[H]
\begin{center}
\includegraphics{ClusterizedStudentKnowledgeDataSetM4Beta2.pdf}
\caption{Датасет <<Знання студентів про електричні машини постійного струму>>}
\label{fig:ClusterizedStudentKnowledgeDataSetM4Beta2}
\end{center}
\end{figure}

Оптимальне розбиття, що його наведено на рис.\ref{fig:3DClusterizedStudentKnowledgeDataSetM4Beta2} та на рис.\ref{fig:ClusterizedStudentKnowledgeDataSetM4Beta2}, надав другий нейрон третього каскаду з $m=4$, $\beta=2$ з коефіцієнтом Ксі-Бені $0.38155$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Розв’язування практичних задач за допомогою розробленої самонавчанної гібридної каскадної системи, що еволюціонує}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Проблема здорового харчування - одна з найактуальніших у наші дні. Повноцінне харчування передбачає споживання достатньої кількості білків, жирів, вуглеводів, вітамінів, макро- і мікроелементів для нормального функціонування організму в цілому. Багато хвороб шлунково-кишкового тракту «молодіють» - це гастрити, виразкова хвороба шлунка і різні порушення обміну речовин. Фізичне здоров'я, стан імунітету, довголіття, психічна гармонія - все це безпосередньо пов'язано з проблемою здорового харчування людини. Для студентів проблема харчування стоїть особливо гостро. У зв'язку з браком часу у студентів немає можливості дотримуватися правильного режиму прийомів їжі в кількості 3-4 разів. Також характерний в основному сидячий спосіб життя - гіподинамія. У поєднанні з незбалансованим раціоном харчування це згубно впливає на організм і його стан. 
Звісно, вирішення проблеми здорового харчування потребує комплексного підходу, проте інфомованість - невід'ємна складова правильного підбору раціону здорового харчування. Насьогодні нескладно знайти інформацію щодо рекомендованої денної кількості калорій, білків, жирів та вуглеводів, проте важко дати оцінку конкретому прийому їжі, наприклад, придбаному у їдальні, де немає етикеток з такою інформацією. Мобільний додаток <<Spoon app>> може стати у нагоді, коли користувач прагне бути проінформованим щодо поживності конкретної трапези, роблячи аналіз світлини тарілки з їжею. Вхідними даними мобільного додатку є світлина, що її користувач має зробити таким чином, аби тарілка знаходилася у центрі, а також тип тарілки (звичайна, глибока, дуже глибока) аби на виході додаток мав обґрунтовану кількість калорій та поживність порції, що була зображена на світлині. 

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{SpoonAppClassification.png}}
\caption{Другий етап аналізу світлини з прийомом їжі мобільним додатком Spoon App (навчаняя з  іпленням)}
\label{fig:SpoonAppClassification}
\end{center}
\end{figure}

Аналіз світлини можна розбити на декілька етапів: 
\begin{enumerate}
\item кластерування даних зображених на світлині (відбувається на стороні клієнту)
\item ідентифікація окремих складових прийому їжі: класифікація кожного зображення, після розбиття світлини на кластери на першому етапі (відбувається на серверній стороні), визначення типу продукту за допомогою бази даних, що знаходиться на сервері, та подальше визначанні кількості калорій, співвідношення білкі, жирів та вуглеводів.
\end{enumerate}

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{SpoonAppClustering.png}}
\caption{Перший етап аналізу світлини з прийомом їжі мобільним додатком Spoon App (кластерування за умови невизначенності щодо кількості кластерів)}
\label{fig:SpoonAppClustering}
\end{center}
\end{figure}

Другий етап у певному сенсі є навчанням з підкріпленням, адже користувачеві пропонується підтвердити чи скорегувати кінцевий результат класифікації, як зображено на рис.~\ref{fig:SpoonAppClassification}. Проте більш цікавим нам видається саме перший етап аналізу світлини, і пропонована гібридна самонавчана система використовується саме на цьому етапі, адже вона краще від інших існуючих систем задовольняє умовам, що їх було висунуто на етапі формування технічних вимог до програмного забеспечення (Software Requirements Specifications):

\begin{itemize}
\item кластерування має проходити за умови невизначенності щодо кількості кластерів,
\item оскільки кластерування відбувається на стороні кліенту, важливо мінімізувати обслювальну складність алгоритму, а отже перевага надається методам послідовного кластерування.
\end{itemize}

По завершенні аналізу на першому етапі мобільний додаток попронує розбиття світлини на $m$ кластерів, як показано на рис.~\ref{fig:SpoonAppClustering}. Варто зауважити, що межі кластерів, що їх визначила самонавчана система, дещо відрізняються від тих, що зображені на рис.~\ref{fig:SpoonAppClustering}, для того, щоб користувачеві було зручніше візуально сприймати розбиття світлини на кластери, пункирні лініїї, що зображуть межі кластерів, на декілька міліметрів віддалені від меж дійсних кластерів, проте на сервер для подальшої класифікації відправляється світлина з розбиттям, що запропонувала система. На цьому етапі користувач може скорегувати розбиттся, перетягнувши пунктирну лінію меж кластерів, чи зовсім видалити пропонований кластер. Хоча навчання з підкріпленням у прямому сенсі, відбувається лише на етапі классифікації (база даних на сервері оновлюються, коли користувач корегує результат класифікації), а на цьому етапі маємо саме навчання без учителя, це все ж таки дає змогу у певному сенсі дати оцінку кластеруванню системи: вважаємо кластерування успішним, якщо користувач не робив жодних змін до пропонованого розбиття, та неуспішним, коли розбиття було скореговане.
Після бета тестування мобільного додатку маємо наступні результати:

\begin{table}[t]
\centering  \begin{tabular}{r | c}
кількість та межі кластеров залишилися незмінними & 608 \\ \hline
межі кластеров не було дещо змінено користувачем, \\проте кількість кластерів залишилась незмінною & 61 \\ \hline
користувач змінив кількість кластерів \\ та межі пропонованих кластерів & 52 \\ \hline 
користувач видалив кластер(и), межі пропонованих \\ кластерів, що лишилися, незмінні & 29
\end{tabular}
\caption{Результати бета тестування першого етапу (кластерування за умови невизначенності щодо кількості кластерів) аналізу світлини мобільним додатком <<Spoon App>>}
\end{table}

Цікавим видається перебіг подій, коли користувач видалив один кластер, проте залишим межі інших кластерів незмінними, у такому випадку, як зазначалося вище, вважається що кластерування не було успішним. Проте подальший аналіз показав, що у 90\% таких випадків користувач залишив на тарілці неїстівний предмет (виделку, ложку тощо), і хоча система вірно відвела йому окремий кластер, не має сенсу класифікувати його та визначати калорійність цього предмету, тому логічно, що користувач видалив його на цьому етапі. У 10\% випадків, як показав аналіз світлин з початковим кластеруванням, що запропонувала самонавчана система, та світлин після корегування користувачем, користувач свідомо видаляв складову прийому їжі, зазвичай найменш корисну (тітечко, шоколад тощо). Тому видається доцільним ці 4\% світлин також віднести до таких, що були вірно розкластеровані системою (яка, проте, не бере до уваги людський фактор).



