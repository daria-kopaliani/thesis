<<echo=FALSE, cache=FALSE>>=
set_parent('thesis.Rnw')
@
\chapter{Огляд стану проблеми та постановка задачі дослідження}
\label{ch:ProbleAnalysis} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Навчання та самонавчання штучних нейронних мереж}

Здатність навчатися – основна властивість біологічного мозку, а оскільки штучна нейронна мережа в деякому сенсі моделює мозок, поняття «навчання» посідає щонайперше місце в теорії штучних нейронних мереж. Математичні проблеми, що пов’язані з навчанням, вивчають у напрямі загальної теорії штучних нейронних мереж, який дістав назву «нейроматематика» \hl{[51, 52]}. Із точки зору нейроматематики, навчання тлумачать як завдання адаптувати параметри, а можливо, й архітектуру мережі, щоби, оптимізуючи прийнятий критерій якості, розв’язати поставлену задачу. Таке визначення є узвичаєним та неявно припускає, що нейроматематика ґрунтується на методах оптимізації та ідентифікації.

Звичайно припускають, що навчання має перманентний характер та з часом мережа покращує свої характеристики, постійно «наближаючись» до оптимального розв’язку поставленої задачі.

Тип та характер навчання обумовлені, насамперед, обсягом попередньої та поточної інформації про довкілля, в яке «занурили» мережу, а також критерієм якості (цільовою функцією), що характеризує рівень відповідності нейронної мережі до розв’язуваної нею задачі. Інформацію про довкілля здебільшого задають у вигляді навчальної вибірки образів або зразків, що їх оброблюючи мережа дістає відомості, необхідні для отримання шуканого розв’язку. Саме характер та обсяг цієї інформації визначають як тип навчання, так і конкретний метод.

З погляду математики, навчання нейронних мереж – це багатопараметрична задача нелінійного оптимування. Більшість методів навчання можна розділити на два класи: навчання з учителем (із заохоченням) та навчання без учителя (без заохочення, або самонавчання). Методи навчання з учителем застосовують у випадках, коли відома бажана реакція системи в кожну мить часу, себто відомий навчальний сигнал, який упливає на налаштування параметрів системи, що навчається. Рівень «навченості» системи формально визначають за значенням цільової функції, тобто за тим станом, якого має в результаті набути коректно навчена система.

Парадигму навчання «з учителем» схематично представлено на рис.~\ref{fig:SupervisedLearning}

\begin{figure}[h]
\begin{center}
\includegraphics{SupervisedLearning.png}
\caption{Схема навчання з учителем}
\label{fig:SupervisedLearning}
\end{center}
\end{figure}

У цій схемі «учителю» відома інформація про довкілля, представлена послідовністю або пакетом ухідних векторів $x$, а також «правильна реакція» на ці сигнали, представлена навчальним сигналом $\hat{y}$. В процесі навчання реакція нейронної мережі y розбігається з «правильною» реакцією вчителя, через що постає похибка $e = \hat{y} = y$. Мета навчання – так налаштувати параметри штучної нейронної мережі, щоби деяка скалярна функція похибки $E\left(e\right)$ (критерій якості) досягла свого найменшого значення. Навченою вважають мережу, яка в деякому, переважно статистичному сенсі повторює реакцію вчителя. Позаяк інформація про довкілля здебільшого має нестаціонарний характер, навчання відбувається безперервно, для чого використовують ті чи інші рекурентні процедури.

Альтернатива навчанню «з учителем» – навчання за умов, коли правильна реакція на сигнали довкілля невідома. Цю парадигму називають навчанням «без учителя», або самонавчанням. Штучні нейронні мережі, що самонавчаються, здебільшого, призначені аналізувати внутрішню латентну структуру вхідної інформації та розв’язують задачі автоматичного класифікування, кластерування, факторного аналізу, компресування даних тощо. У теорії ШНМ самонавчання звичайно розглядають як конкурування або самоорганізування нейронів мережі, що топологічно взаємозв’язані між собою.

Самонавчання схематично представлено на рис. \ref{fig:UnsupervisedLearning}.

\begin{figure}[h]
\begin{center}
\includegraphics{UnsupervisedLearning.png}
\caption{Схема самонавчання}
\label{fig:UnsupervisedLearning}
\end{center}
\end{figure}


Щоби покращити якість навчання та прискорити збіжність, ітераційне навчання можна повторювати циклічно на так званому «вікні» – наборі послідовних значень навчального сигналу (у дискретному випадку) або проміжкові часу (у неперервному випадку). Граничним варіантом процедур з повторюванням є пакетне та послідовне (у реальному часі) модифікування. Під пакетним режимом розуміють випадок, коли всю вибірку даних задано попередньо, а навчання відбувається «епохами». У режимі реального часу повторювання відсутнє, хоча якщо зорганізувати навчання в «прискореному» машинному часі, навчання може повторюватися. 

\section{Гібридні системи обчислювального інтелекту}

В обчислювальному інтелекті вельми поширеним є підхід створювати систем обробляння даних на основі кількох наукових напрямів. Як засвідчують теоретичні та практичні результати, таким гібридним системам властивий синергетичний ефект, тобто вони виявляють такі властивості, яких не мають системи, що їх утворюють [85].
Одним з яскравих прикладів гібридних систем обчислювального інтелекту є нейро-фазі системи, які поєднують у собі нейронні мережі другого покоління та нечіткі системи [9, 85]. Нейронна мережа може навчатися на вхідних та вихідних даних для визначення поведінки системи, але отримані знання будуть сховані в її синапсових вагах і їх не можна буде витлумачити. Однак, якщо виразити ваги нейронної мережі за допомогою нечітких правил, з’являється можливість подолати неінтерпретовність результатів роботи нейронної мережі. У такий спосіб нейрофаззі системи дають змогу створювати системи обробляння інформації та отримують більш шіроке застосування.
Розвиваючи гібридний підхід, запропоновано й просунутіші поєднання наукових напрямків, наприклад, теорії штучних нейронних мереж та індуктивного моделювання даних [86]. Ефективність кластерування даних залежить у великій мірі від якості обраної математичної моделі розв’язуваної або досліджуваної задачі. Як уже мовилося вище, однією з проблем кластерування даних є змінна кількість кластерів оброблюваних даних. Відповідно постає складна задача обрати належну математичну модель. Індуктивне моделювання має тут ефективний розв’язок: налаштовувати не лише параметри системи обробляння даних, але також і її структуру. Проекція такого підходу на штучні нейронні мережі веде до ідеї змінювати кількість нейронів в шарах мережі, що обробляє дані. Дієвість роботи побудованих за цим принципом систем [87, 88] засвідчує плідність гібридного підходу.

%%\section {Гибридные нейро-фаззи системы и вопрос структурной адаптации}
\section {Гібрідні нейро-фаззі системи та питанная структурної адаптації}

В наш час для розв'язання задач, які пов'язані з інтелекутальною обробкою даних в умовах апріорної та поточної невизначеності, дослідники часто не обмежуються використанням якогось одного підходу (нейронні мережі, нечітка логіка, генетичні алгоритми тощо), а задля синергії зв'язують групу методів в одну гібридну систему~\cite{ref86, ref87} . Такі системи відповідають усім вимогам до інтелектуальних систем і отримали назву гібридні системи обчислювального інтелекту. Нейро-фаззі системи та м'які обчислювання є напрямами дисципліни обчислювального інтелекту, які й займаються проблемами таких систем. 
Серед основних характеристикамі систем, шо розробляються в рамках нейро-фаззі систем та м'яких обчислень, можна виділити наступні \hl{[27]}:
\begin{itemize}
\item обислювальні моделі, що засновані на біологічних прототипах,
\item паралельна обробка данних у послідовному режимі,
\item в основі системи лежать експертні знання,
\item стійкість системи до зашумління,
\item стійкість системи до виходу із строю підсистем.
\end{itemize}

%%Основными характеристиками систем, разрабатываемых в рамках направления нейро-фаззи системы и мягкие вычисления, являются :
%%– экспертные знания, заложенные в систему;
%%– вычислительные модели, основанные на биологических прототипах;
%%– «интенсивные вычисления» – параллельная обработка в последовательном
%%режиме;
%%– устойчивость к зашумлённым данным;
%%– устойчивость к выходу из строя элементов подсистем;

Варто зазначити, що одною із головних умов до такого типу систем, є їх орієнтованість на розв'язання практичних завдань, що означає здатність оброблювати великі масиви даних великої розмірності, які можуть мати пропущені та зашумленні значення. Однак навчання таких систем зводиться до налаштування сінаптичних коефіцієнтів та/або адаптації бази нечітких правил. Тобто архітектура такої системи не зазнає жодних змін, що може в деяких випадках призвести до погіршення точності результатів. В зв'язку з цим видаюється очевидно корисним зсінтезувати таку гібридну нейро-фаззі архітектуру та такі алгоритми її навчання, що здатні змінувати не тільки параметри систему, а й її архітектуру.

%%Кроме того, необходимо отметить, что одним из требований, выдвигаемых к таким системам, является их ориентирование на практические задачи, т.е. способность обрабатывать значительные объёмы данных большой размерности, содержащие пропущенные и зашумлённые значения. Однако обучение в таких системах сводится к настройке синаптических коэффициентов и/или адаптации базы нечётких правил, не затрагивая саму архитектуру, что может приводить к снижению точности получаемого решения. В связи с этим представляется весьма полезным синтезировать такую гибридную нейро-фаззи архитектуру и алгоритмы её обучения, которые могли бы настаивать не только параметры нейро-фаззи системы, но и саму её архитектуру в целом.

Як вже зазначалось, основною метою навчання є отримання нейронної мережі, яка здатна у найкращий спосіб відтворювати попередньо невідоме відображення $R^{n} \rightarrow R^{m}$. В якості такого відображення може виступати залежність вихідних параметрів процесу від вхідних, прогнозування від передісторії, класа об'єкту від набору його властивостей, управляючої дії від поточного стану об'єкта тощо. Коректне налаштування не тільки синаптичних коефіцієнтів, а й архітектури нейронної мережі, зокрема налаштування кількості шарів та кількості нейронів у кожному шарі, дозволяю суттєво покращити показники такої мережі. Серед підходів до налаштування архітектури нейронної мережі виділяють:
\begin{itemize}
\item деструктивний підхід: за основу береться заздалегіть надлишково складна модель, до неї застосовуються різні процедури, що видаляють із вихідної мережі елементи, які оказують негативниий або незначний позитивний ефект на кінцевий результат,
\item конструктивний підхід: за основу береться максимально проста модель (складається із одного або декількох нейронів), до неї застосовуються процедури, що додають вихідній мережі нові елементи до певного моменту, в залежності від використовуємого алгоритму. Як варіант, конструктивний алгоритм може стартувати з цілком нульової архітектури та самостійно генерувати шари мережі в процесі своєї роботи.
\end{itemize}

\subsection {Деструктивнийй підхід до налаштування архітектури нейронної мережі}

%%\subsection {Деструктивный подход к настройке архитектуры нейронной сети}
 
Основна ідея деструктивних алгоритмів полягає у видаленні параметрів, що мабть найменьший вплив на вихідний сигнал мережі. У ряді публікацій \hl{30-32} був сформульований та підтвержене припущення, що використання деструктивних алгоритмів нерідко призводить до покращення узагальнюючих властивостей мережі, допомагає нейтралізувати появу так званого ефекту перенавчання, а, крім того, після закінчення роботи такого алгоритму, архітектура мережі набуває меншого так більш простого вигляду, що очевидно позитвино відбивається на її обчислювальній складності.

У процесі функціонування деструктивних алгоритмів із мережі могут бути цілком видалені як деякі вхіжні параметри або вузли у прихованих шарах, так і лише деякі синаптичні зв'язки між нейронами, що мають лише один параметр hl{дефіс} ваговий коефіцієнт. Вочевидь, в основі деструктивного піходу до структурної адаптаціі нейрнонної мережі повина бути закладено обчислення деякої \hl{міри значущості}, яка буде характеризувати ступінь впливу кожного конкретного параметру на вихідний сигнал.

В одній із перших публікацій, що розглядає цю проблему, \hl{30} автори запропонували деструктивний алгоритм структурної оптимізації під назвою Optimal Brain Damage(OBD), яких складається із наступних етапів:
\begin{enumerate}
\item вибрати архітектури нейронної мережі,
\item використовуючи один із методів мінімізації цільової функції якості, провести навчання мережі,
\item для кожного елементу мережі обчислити міру значущості:

\begin{equation}\label{eq:obd}
s_{q}=h_{q}u_{q}^2/2,
\end{equation}
\medskip

де $u_{q}$ -- вихідний сигнал $q$-го елементу мережі,\\
$h_q$ розраховується по формулі:

\begin{equation}\label{eq:h_q}
h_q=\sum\limits_{\left(i,j\right)\in V_q}\frac{\partial^{2}{E}}{\partial{w_{ij}^{2}}},
\end{equation}
\medskip

де $V_q$ -- множина пар коефіцієнтів $i$ та $j$ для $q$-го елемента мережі, 

E -- помилка на виході нейронної мережі, 

$w_{ij}$ -- синаптичний ваговий коефіцієнт в $j$-м шарі,

\item видалити із мережи деяку кількість елемантів, для яких міра значущості $s_q$ найменьша. В цьому контексті під видаленням елементу мається на увазі зміна вихідного значення елемента на 0 та замороження його в такому стані,
\item повернутися на крок 2 та повторити процедуру.
\end{enumerate}

Вочевидь, при використанні такого методу значно збільшується обчислювальна складність алгоритма навчання, і через необхідність розрахувати міру значущості для кожного нейрону, і через додаткові ітерації перенавчання, які необхідно виконати після видалення кожного нейрона із мережі. Також суттєвим недоліком є те, що разом з видаленням нейрона ми видаляємо одразу декілька синаптичних зв'язків, хоча деякі із них можуть бути корисними. В \hl{31} було запропаново спосіб обійти цей недолік, а разом із тим збільшити швидкість процесу навчання. Цей подхід отримав назву Optimal Brain Surgeon(OBS) і складається він із наступних етапів:

\begin{enumerate}
\item вибрати мережу із достатньо надлишковою архітектурою та провести її навчання,
\item обчислити $H^{-1}$ - матрицю зворотню до гессіану: $H = \frac{\partial^{2}{E}}{\partial{w^{2}}}$,
\item обчислити міру значущості для кожного елемента:

\begin{equation}\label{eq:L_q}
L_q=w_{q}^{2}/\left(2[H^{-1}]_{qq}\right),
\end{equation}
\medskip

де $w_q$ -- $q$-ий ваговий коефіцієнт в мережі,


\item якщо мінімальний $S_q$ значно меньший за поточну помилку, то $w_q$ має бути видалений, після чого перейти до кроку номер 5, в іншому разі перейти до кроку номер 6,
\item оновити вектор вагових коефіцієнтів мережі, використовуючи наступний вираз:

\begin{equation}\label{eq:delta_w}
\delta{W}=-\frac{w_q}{[H^{-1}]_{qq}}H^{-1}\zeta_{q},
\end{equation}
\medskip


де $\zeta_{q}$ -- орт-вектор в площині вагових коефіцієнтів мережі, який відповідає $q$-й синаптичнй вазі,

\item усі незначущі вагові коефіцієнти видаляються, після чого бажано перенавчити мережу.
\end{enumerate}

Слід зауважити, що перший же крок цього методу, а саме вибір критерію надмірності архітектури, може викликати багато запитань. В цілому ж, запропонований в \hl{[31]} метод також характеризується значною обчислювальною складністю, хоч і меншою, в порівнянні з OBD, оскільки він не потребує перенавчання всю мережу після видалення кожного вагового коефіцієнта. Але описаним вище методам притаманний ще один істотний недолік - необхідність мати вже навчену нейронну мережу до початку роботи алгоритму. Це обмеження вдалося обійті завдяки методу, що був запропонований у \hl{[36]}. Міра значущості для вагових коефіцієнтів визначається через тестову статистику T, опираючись на умову того, що вага обнуляється у процесі навчання мережі:

\begin{equation}\label{eq:T_w}
T_{(w_q)}=\log\left(\frac{\left|\sum\limits_{k=1}^{N}\left(w_q-\eta{\left(\frac{\partial{E}}{\partial{w_q}}\right)}_k\right)\right|}{\eta\sqrt{\sum\limits_{k=1}^{N}{\left({\left(\frac{\partial{E}}{\partial{w_q}}\right)}_k-\left(\frac{\partial{E}}{\partial{w_q}}\right)\right)}^2}}\right),
\end{equation}
\medskip

де $N$ -- кількість прикладів у виборці.

У рамках деструктивного підходу відомі також і багато інших методів оптимізації архітектури нейронної мережі \hl{[33-35,37-39]}, проте в силу специфіки цього підходу всім їм в тій чи іншій мірі властива додаткова обчислювальна складність, а крім того орієнтація на нейронні мережі з архітектурою типу багатошарового персептрона. Використання таких алгоритмів для налаштування інших архітектур, зокрема для нейро-фаззі систем, неможливо, а розробка видається недоцільною, оскільки в будь-якому разі використання деструктивного алгоритму буде мати прямий негативнй вплив як на час навчання, так і на час функціонування мережі. Для систем, що розробляються в рамках напряму нейро-фаззі і м'яких обчисленнь, час є досить критичним параметром, оскільки ці системи орієнтуються на рішення практичних задач. У зв'язку з цим вельми привабливо виглядає використання конструктивного підходу для синтезу архітектури нейронної мережі, про що йдеться далі.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Конструктивний підхід до налаштування архітектури нейронної мережі}

Суть конструктивного підходу полягає в нарощуванні архітектури нейронної мережі і налаштування її вагових коефіцієнтів паралельно доки не будуть задоволені вимоги критерію зупинки. Тобто цец підхід дозволяє не тільки уникнути перенавчання нейронної мережі, а й оптимізацує структуру й архітектури мережі на етапі навчання.

%Суть конструктивного подхода заключается в наращивании архитектуры нейронной сети и настройке её весовых коэффициентов параллельно до того момента, пока не будут удовлетворены требования критерия останова. Этот подход (как и деструктивный) помогает избежать переобучения нейронной сети, а также производит структурную оптимизацию её архитектуры на этапе обучения.

Таким чином, використовуючи конструктивний підхід, можна повністю вирішити питання про вибір початкової архітектури мережі: в загальному випадку вона повинна бути максимально простою, складаєтися з одного або декількох нейронів (залежить від методу). Слід зауважити, що, як правило, в результаті роботи конкретного конструктивного алгоритму на виході виходить нейронна мережа з нетрадиційною архітектурою.

%Используя конструктивный подход, удаётся полностью решить вопрос о выборе начальной архитектуры сети: в общем случае она должна быть максимально простой, состоящей из одного или нескольких нейронов (зависит от метода). Следует заметить, что, как правило, в результате работы конкретного конструктивного алгоритма на выходе получается нейронная сеть с нетрадиционной архитектурой.

В \hl{[40]} Джон Платт описує нейронну мережу (RAN - Resource-Allocating Network), яка в процесі навчання додає в свою архітектуру нові обчислювальний елементи (штучні нейрони) кожен раз, коли на вхід мережі подається новий навчальний приклад, який в RAN має двошарову архітектуру. Перший шар складається з нейронів, які відповідають за локальну область з простору вхідних сигналів. У разі якщо вхідний сигнал віддаляється від області конкретного нейрона, то значення сигналу на його виході буде зменшуватися відповідно до співвідношення:

\begin{equation}\label{eq:x_j}
x_j=
\begin{cases}
{\left(1-\left(z_j/\chi{w^{2}_j}\right)\right)}^2,\text{ ЯКЩО } z_j<\chi{w_j}^2\\
0,\text{      ІНАШКЕ}
\end{cases}
\end{equation}
\medskip
\begin{equation}\label{eq:z_j}
z_j=\sum\limits_{j}{\left(c_{ij}-X_i\right)}^2,
\end{equation}
\medskip

де $X_i$ -- $i$-ий вхід нейронної мережі,

$c_{ij}$ -- $i$-ий ваговий коефіцієнт $j$-го нейрона першого шару,

$\chi$ -- параметр налаштування, який підбирається емпіричним шляхом.

%Одной из первых работ в этом направлении, получившей бурное развитие, является работа Джона Платта. В [40] он описывает нейронную сеть (RAN – Resource-Allocating Network), которая в процессе обучения добавляет в свою архитектуру новые вычислительный элементы (искусственные нейроны) каждый раз, когда на вход сети подаётся новый обучающий пример, который в RAN имеет двухслойную архитектуру. Первый слой состоит из нейронов, которые отвечают за локальную область из пространства входных сигналов. В случае если входной сигнал удаляется от области конкретного нейрона, то значение сигнала на его выходе будет уменьшаться согласно соотношению:

Виходи першого шару $x_j$ подаються на другий шар, який агрегує ці значення і генерує вихідний сигнал. Метою кожного синапсу другого шару є визначити, який вплив надає кожен нейрон першого шару на формування конкретного цільового вектора $\vec{y}$. Вихідним сигналом мережі $\vec{y}$ є зважена сума виходів першого шару плюс незалежний вектор $\vec{\gamma}$, що містить постійні елементи:

\begin{equation}\label{eq:vec_y}
\vec{y}=\sum\limits_{j}\vec{w}^{[\circ]}x_j+\vec{\gamma},
\end{equation}
\medskip

де $\vec{w}^{[\circ]}$---вектор синаптичних ваг вихідного шару, або в скалярній формі:

\begin{equation}
y_i=\sum\limits_{j}w^{[\circ]}_{ji}x_j+{\gamma}.
\end{equation}
\medskip

Також $\vec{\gamma}$ є виходом нейронної мережі у випадку, якщо не активувався жоден з нейронів першого шару. У певному сенсі вираз ${\vec{w_j}^{[\circ]}x_j}$ може розглядатися як певний адитивний елемент, який може бути використаний для того, щоб отримати бажаний вихідний сигнал. 
Навчання RAN починається з нульового стану, тобто в першому шарі не міститься жодного нейрона, а в другому кількість нейронів дорівнєю розмірності задачі, проте, на цьому етапі всі вони, за вийнятком зсуву ${\gamma}$, не мають вхідних параметрів. Після подачі на вхід першого навчального прикладу у вхідній шар додається перший нейрон, центр функції активації \eqref{eq:x_j} якого встановлено наступним чином:

\begin{equation}\label{eq:vec_c_i}
\vec{c_i}=\vec{X},
\end{equation}
\medskip

де $k$ -- номер прикладу у виборці.

Вихідний сигнал першого шару автоматично передається на всі нейрони другого шару, а його лінійні синапси налаштовуються таким чином, щоб різниця між виходом мережі і навчальним сигналом була мінімальна:
\begin{equation}
\vec{w_j}^{[\circ]}=\vec{Y}-\vec{y},
\end{equation}
\medskip
де $\vec{Y}$ -- бажаний вихід мережі.
%Также выход первого слоя автоматически распространяется на все нейроны второго слоя, а его линейные синапсы настраиваются таким образом, чтобы разница между выходом сети и обучающим сигналом была минимальна:

Доданий нейрон буде реагувати на нові вхідні сигнали, якщо вони будуть перебувати в певному інтервалі, який визначаються відстанню між найближчим вектором і новим вхідним образом
\begin{equation}
w_i=\omega\left\|\vec{X}-\vec{c}_{nearest}\right\|,
\end{equation}
\medskip
де $\omega$ -- параметр покриття, підібраний емпіричним шляхом. Чим більше значення цього параметра, тим на більшу кількість вхідних сигналів будуть реагувати вже існуючі нейрони першого шару.

%где   – параметр покрытия, выбираемый эмпирически. Чем больше значение этого параметра, тем на большее количество входных сигналов будут реагировать уже существующие нейроны первого слоя.

%В RAN используются два условия для добавления нового нейрона в первый слой сети. Во-первых, это происходит в случае, если входной сигнал находится далеко от уже существующих центров функций активации нейронов первого слоя:

У RAN використовуються дві умови для додавання нового нейрона в перший шар мережі. По-перше, це відбувається в тому випадку, якщо вхідний сигнал знаходиться далеко від вже існуючих центрів функцій активації нейронів першого шару:
\begin{equation}\label{eq:introActivFuncFirstLayer}
\left\|\vec{X}-\vec{c}_{nearest}\right\|>\delta{(t)},
\end{equation}
\medskip
а також у випадку, коли із допомогою поточного набору елементів не вдається забезпечити необхідну точніть вихідного сигналу:
\begin{equation}\label{eq:introActivFuncOutputSignal}
\left\|\vec{Y}-\vec{y}{(\vec{X})}\right\|>\varepsilon,
\end{equation}
\medskip
де $\varepsilon$ -- необхідна точність вихідного сигналу.
Якщо при подачі на вхід нового вектора на виході мережі ми отримуємо помилку більшу, ніж $\varepsilon$, то у вхідний шар мережі додається новий нейрон з центрами активаційних функцій, налаштованими на поточний вхідний образ.
Відстань $\delta{(k)}$ -- динамічний параметр, який змінює своє значення протягом процесу навчання. Для його калькуляції використовується наступний вираз:
\begin{equation}
\delta(k)=max\left({\delta_{max}e^{-i/{\tau}},\delta_{min}}\right),
\end{equation}
\medskip
де $\delta_{max}$, $\delta_{min}$, $\tau$ -- параметри, що вибираються емпірічно.

 Якщо згідно з умовами  \eqref{eq:introActivFuncFirstLayer} і \eqref{eq:introActivFuncOutputSignal} не потребується додавання нового нейрона у вхідний шар, то проводиться підналаштування вагових коефіцієнтів вихідного шару. Для цього можуть використовуватися градієнтні методи мінімізації або ж метод найменших квадратів.
 
%На первых этапах обучения в сеть преимущественно добавляются новые элементы, однако спустя некоторое время этот процесс замедляется и вместо добавления новых нейронов во входной слой производится настройка синаптических весовых коэффициентов выходного слоя. Такой порядок работы конструктивного алгоритма становится возможным, благодаря использованию двух условий добавления нового нейрона (1.13, 1.14), и обеспечивает оптимальную сложность модели нейронной сети наряду с хорошим уровнем обобщающих способностей. В случае использования исключительно (1.13) наиболее вероятно, что мы столкнёмся с эффектом переобучения, а в случае – (1.14) могут быть пропущены некоторые нейроны входного слоя, что повлияет на точность выходного сигнала сети.

На перших етапах навчання в мережу переважно додаються нові елементи, проте через деякий час цей процес сповільнюється і замість додавання нових нейронів у вхідному шар відбувається налаштування синаптичних вагових коефіцієнтів вихідного шару. Такий порядок роботи конструктивного алгоритму стає можливим завдяки використанню двох умов додавання нового нейрона \eqref{eq:introActivFuncFirstLayer} та \eqref{eq:introActivFuncOutputSignal}, забезпечує оптимальну складність моделі нейронної мережі поряд з хорошим рівнем узагальнюючих здібностей. У разі використання виключно \eqref{eq:introActivFuncFirstLayer} найбільш ймовірно, що це призведе до перенавчання, а в разі -- \eqref{eq:introActivFuncOutputSignal} можуть бути пропущені деякі нейрони вхідного шару, що вплине на точність вихідного сигналу мережі.

Серед недоліків запропонованого Джоном Платтом методу має сенс назвати досить велику кількість емпірично підбираємих параметрів, від яких безпосередньо залежить якість роботи RAN.

Надалі підхід до конструктивної організації архітектури нейронної мережі, відомий в англомовній літературі під назвою resource allocation, породив безліч різних модифікацій, які спрямовані на оптимізацію швидкості навчання і точності вихідного сигналу при вирішенні певного кола завдань \hl{[41-44]}.

%В рамках конструктивного подхода можно выделить такое направление, как каскадные нейронные сети [45-50], наиболее характерным и эффективным представителем которых является каскадно-корреляционная архитектура, предложенная С. Фальманом и К. Лебьером в работе [45]. Основная особенность сети этого типа заключается в возможности добавления новых узлов в процессе обучения. На рис. 1.1 приведена схема подобной сети, содержащая три каскада, n входов и один выход.

В рамках конструктивного підходу можна виділити такий напрямок, як каскадні нейронні мережі \hl{[45-50]}, найбільш характерним і ефективним представником яких є каскадно-кореляційний архітектура, запропонована Фальманом і Лєб'єром в роботі \hl{[45]}. Основна особливість мережі цього типу полягає в можливості додавання нових вузлів в процесі навчання. На \hl{рис. 1.1} приведена схема подібної мережі, яка містить три каскаду, n входів і один вихід.

%В начале процесса обучения формируется стандартная однослойная структура с n входами и 1 единственным выходом (рис. 1.1), которая обучается при помощи того или иного нелинейного метода обучения.

На початку процесу навчання формується стандартна одношарова структура з $n$ входами і єдиним виходом \hl{(рис. 1.1)}, яка навчається за допомогою того чи іншого нелінійного методу навчання.

%После предъявления всей обучающей выборки x(1),x(2), ,x(N) оценивается точность аппроксимации и в случае, если ошибка слишком велика, формируется каскад из n2 нейронов-кандидатов, параллельно подключенных к входам сети 1, x1 , x2 , , xn и выходу первого каскада o[1] . Нейроны-кандидаты, как правило, отличаются друг от друга начальными значениями синаптических весов W[2](0), видом функций активации и алгоритмами обучения. Далее производится обучение нейронов второго каскада при «замороженных» синаптических весах W[1] (N) первого. На рис. 1.1 «замороженные» веса показаны в виде заштрихованных сумматоров. Среди n2 нейронов-кандидатов выбирается один нейрон-победитель, у которого параметр корреляции [46]

Після прохождення всієї навчальної вибірки $x(1),x(2),\dots,x(N)$ оцінюється точність апроксимації і в тому випадку, якщо помилка занадто велика, формується каскад з $n_2$ нейронів-кандидатів, паралельно підключених до входів мережі $1,x_1,x_2,\dots,x_n$ і виходу першого каскаду $o^{[1]}$. Нейрони-кандидати, як правило, відрізняються один від одного початковими значеннями синаптичних вагових коефіцієнтів $W^{[2]}(0)$, видом функцій активації та алгоритмами навчання. На наступному етапі проводиться навчання нейронів другого каскаду при «заморожених» синаптичних коефіцієнтах $W^{[1]}(N)$ першого каскаду. На \hl{рис. 1.1} «заморожені» коефіцієнти показані у вигляді заштрихованих суматорів. Серед $n_2$ нейронів-кандидатів вибирається один нейрон-переможець, параметр кореляції якого \hl{[46]}
\begin{equation}
R^2_{[q]}=\left|\sum^{N}\limits_{k=1}\left(o^{[2]}_q\left(k\right)-\bar{o}_q^{[2]}\right)\left(e^{[2]}_q\left(k\right)-\bar{e}_q^{[2]}\right)\right|,\text{ } q=1,2,\dots,n_2
\end{equation}
\medskip
%(здесь oq[2] и eq[2] – средние значения выходного сигнала и ошибки) является максимальным. Именно этот нейрон с «замороженными» весами W [ 2 ] ( N ) образует второй каскад, в то время как «проигравшие» нейроны изымаются из сети.

(тут $\bar{o}_q^{[2]}$ і $\bar{e}_q^{[2]}$ - середні значення вихідного сигналу і помилки) є максимальним. Саме цей нейрон з «замороженими» вагами $W^{[2]}(N)$ утворює другий каскад, в той час як нейрони, які програли, видаляються з мережі.

%Далее оценивается точность аппроксимации, обеспечиваемая вторым каскадом, и в случае необходимости формируется набор из n3 кандидатов третьего каскада, среди которых выбирается победитель с максимальным значением

Далі оцінюється точність апроксимації, що забезпечується другим каскадом, і в разі потреби формується набір з $n_3$ кандидатів третього каскаду, серед яких вибирається переможець з максимальним значенням
\begin{equation}
R^3_{[q]}=\left|\sum^{N}\limits_{k=1}\left(o^{[3]}_q\left(k\right)-\bar{o}_q^{[3]}\right)\left(e^{[3]}_q\left(k\right)-\bar{e}_q^{[3]}\right)\right|,\text{ } q=1,2,\dots,n_3
\end{equation}
\medskip
У разі досягнення необхідної точності процес нарощування каскадів завершується і вихідний сигнал останнього каскаду (на рис. 1.1 -- $\bar{o}_q^{[3]}$) приймається в якості вихідного сигналу мережі в цілому.
В якості основних відмінних рис каскадно-кореляційних мереж слід зазначити наступні:
\begin{itemize}
\item ці мережі не вимагають попереднього завдання ні архітектури, ні кількості нейронів в каскадах,
\item нейрони в мережу додаються в міру необхідності, утворюючи не приховати шари, а каскади, кожен з яких в якості вхідних сигналів використовує входи мережі і виходи попереднього каскаду,
\item навчання не пов'язане з концепцією зворотнього поширення помилок, що дозволяє істотно скоротити час налаштування
\item за рахунок «заморожування» синаптичних ваг сформованих раніше каскадів скорочуються обчислювальні витрати на навчання.
\end{itemize}

Головним недоліком даних мереж прийнято вважати неможливість їх навчання в режимі послідовної обробки інформації \hl{[51]}. Далі буде показано, як можна подолати це обмеження, синтезувавши на основі мережі, запропонованої в \hl{[45]}, архітектуру, яка буде відповідати критеріям, які висуваються до нейро-фаззі систем.

\subsection{Задача нечіткого кластерування даних}

Кластеризація є основним інструментом для аналізу даних. Він знаходить широке застосування в багатьох інженерних і наукових областях, включаючи розпізнавання образів, виділення ознак, вектор квантування, сегментації зображень, біоінформатики і інтелектуального аналізу даних. Кластеризація є класичним методом вибору прототипу ядра на основі нейронних мереж, таких як RBF мережі, і є найбільш корисним для нейро нечітких систем.
Кластеризація є неконтрольований метод класифікації, яка ідентифікує деяку неіз структуру, присутню в ЛОР безлічі об'єктів на основі показника подібності. Методи кластеризації можуть бути отримані зі статистичних моделей або конкурентного навчання, і, відповідно, вони можуть бути класифіковані в генеративної (або на основі моделі) і дискриміна- TIVE (або подібності на основі) підходів. Проблема кластеризация також може бути змодельована як КС. Кластеризація нейронні мережі є статистичні моделі, де функція щільності ймовірності (PDF) для даних оцінюється шляхом вивчення його параметрів.

Нечітке кластерування даних – один з напрямів кластерного аналізу, що використовує для обробляння даних деякі принципи та елементи нечіткої логіки [77, 78]. Концептуальний взаємозв’язок між кластерним аналізом та нечіткою логікою ґрунтується на тій обставині, що розв’язання задач структурування складних систем формує здебільшого кластери об’єктів, що є розмиті, нечіткі за своєю природою. Така нечіткість може полягати в тому, що перехід од належності до неналежності образів щодо певних кластерів радше поступовий, аніж стрибкуватий. Тому адекватнішою в таких випадках є не однозначна належність до певного кластеру, а низка рівнів належності до кількох кластерів. Вимога однозначно розкластерувати елементи досліджуваної проблемної області є вельми грубою та жорсткою, особливо у випадках, коли треба розв’язати погано або слабко структуровані задачі інтелектуального аналізу даних. Засоби нечіткого кластерування послаблюють цю вимогу введенням до розгляду нечітких кластерів та їхніх функцій належності, які приймають значення на інтервалі [0, 1].

З-поміж цілої низки методів та підходів нечіткого кластерування особливе місце займають методи, що ґрунтуються на цільових функціях [79]. Такі методи розв’язують задачу обробляння даних, оптимуючи деякий заздалегідь заданий критерій якості. Найвідомішим представником цього класу методів є метод нечітких с-середніх [77], що його широко застосовують у задачах різноманітної складності, коли навчальний сигнал невідомий. Але хоча стандартний метод нечітких с-середніх є значно просунутіший супроти методів чіткого кластерування, але все ж таки й він має вади. Справа в тому, що однією
з умов використовування цього методу є вимога, щоби сума рівнів належності будь-якого образа за всіма кластерами дорівнювала одиниці. Ця штучна вимога
у випадках рівновіддаленості деякого образа від усіх кластерів спричиняє те, що такий образ отримує рівень належності до кожного з кластерів, який не залежить од відстані між образом та центром відповідного кластеру. Іще однією вадою методу, яка випливає з попередньої, є припущення, що під час обробляння даних образи, що належать новим кластерам, з’явитися не можуть. Вочевидь, в реалістичних задачах це не завжди так. До того ж образи, що надходять на вхід методу, можуть бути звичайним шумом, завадами. Стандартний метод нечітких с-середніх не впорається з такою ситуацією, що відповідно позначиться на ефективності кінцевого нечіткого розбиття даних.

Зазначену ваду долає метод можливісного нечіткого кластерування даних – метод можливісних с-середніх [77, 80, 81]. Він не має вимог щодо значення суми рівнів належності образів за всіма кластерами, що, відповідно, покращує його дієвість за умов наявності шуму в ухідному сигналі. Проте цей метод має певні труднощі з його початковим ініціалізуванням.

Беручи до уваги те, що обидва розглянуті методи належать до методів нечіткого кластерування даних, тут годилося б зазначити, що коли виникає потреба виокремити стандартний метод нечітких с-середніх, його називають імовірнісним.

Два розглянуті методи нечіткого кластерування даних – імовірнісний та можливісний – є базові методи, які утворюють цілі сім’ї похідних від них і пристосованих до певних специфічних задач обробляння даних [77]. Зазвичай, кожен зі згаданих методів обробляє дані в пакетному режимі. У ситуаціях, коли дані надходять окремо й їх потрібно обробляти он-лайн, для методів нечіткого кластерування запропоновано послідовні модифікації [77, 82].

Іще одна вада стандартних методів нечіткого кластерування даних – нездатність виявляти кластери складної, несферичної форми. Цю ваду, опріч уже згаданого ієрархічного кластерування, долає метод нечіткого кластерування Ґустафсона-Кесселя, який, замість Евклідової, використовує Магаланобісову метрику [83].

Як видно, існує розмаїття методів нечіткого кластерування, зоснованих на оптимуванні цільової функції. Але усі вони ґрунтуються на припущенні, що дослідникові попередньо відома кількість кластерів, що їх треба виявити. Допевне, таке припущення слушне не для всіх задач, адже інколи кількість кластерів у вхідних даних може бути невідомою або вона може змінюватися в часі. Цей випадок є предметом розгляду систем обробляння даних із мінливою (еволюційною) архітектурою [84], які здебільшого є гібридними системами з поліпшеними архітектурами, що поєднують декілька напрямків обчислювального інтелекту.

\section{Постановка завдання дослідження}

Оскільки сучасні обчислювальні технології дозволяють накопичувати і обробляти досить великі масиви інформації, то на перший план виходить швидкість обробляння даних, а також можливість роботи з ними в послідовному режимі. Крім того варто зазначити, що інформація, яка обробляється, може характеризуватися нелінійним і нестаціонарним характером даних. У таких випадках доцільно використання штучних нейронних мереж, які володіють універсальними апроксимуючими властивостями. Застосування апарату нечіткої логіки дозволяє розширити функціональні можливості штучних нейронних мереж і коло вирішуваних завдань. Завдання дослідження полягає в розробці архітектур нейро-фаззі мереж і методів їх навчання, що володіють високою гнучкістю налаштування параметрів для інтелектуального аналізу даних в умовах невизначеності. Для досягнення поставленої мети необхідно розглянути наступні питання:

\begin{enumerate}
\item Аналіз існуючих нейро-фаззі архітектур і методів їх навчання.
\item Розробка спеціалізованих штучних нейронів, які мають підвищену (порівняно з традиційними нейронами) швидкістю навчання, а також здатних ефективно вирішувати завдання прогнозування, ідентифікації і класетрування в умовах апріорної і поточної невизначеності.
\item Розробка на основі цих нейронів штучних нейронних мереж зі зростаючою архітектурою та методів їх навчання.
\item Дослідження методів і способів, що дозволяють виконати гібридизацію (перехід від нейро до нейро-нечіткої системи).
\item Розробка методів навчання, що дозволяють гібридній нейро-нечіткій зростаючій архітектурі функціонувати в режимі послідовного обробляння інформації.
\item Проведення імітаційного моделювання розроблених методів і архітектур та розв'язання з їх допомогою практичних завдань.
\end{enumerate}

\section*{Висновки до розділу~\ref{ch:ProbleAnalysis}}

\begin{enumerate}
\item Розглянуто гібридні нейро-фаззі системи для вирішення завдань обробляння інформації за умови апріорної і поточної невизначеності. У якості головного недоліку таких систем виділено відсутність ефективних способів настройки архітектури з можливістю функціонувати в режимі реального часу.
\item Проаналізовано стан проблеми кластерування даних і розглянуті існуючі підходи до її вирішення. Розглянуто основні принципи нечіткої логіки та систем нечіткого висновування. Проаналізовані існуючі архітектури штучних нейронних мереж і методи їх самонавчання, що використовуються для вирішення завдань кластерування даних.
\item Проведено аналіз існуючих конструктивних і деструктивних методів структурної адаптації нейронних мереж. Виділено їх сильні і слабкі сторони. Обґрунтовано доцільність використання конструктивних алгоритмів для синтезу систем, що мають функціонувати в режимі послідовного обробляння даних.
\item Сформульовано завдання дослідження.
\end{enumerate}