\chapter{Каскадна нейронна мережа, що еволюціонує, для послідовного нечіткого кластерування потоків даних}
\label{ch:evolvingClusteringSystem}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
У цьоому розділі описані архітектура та методи навчання пропонованої каскадної нейро-мережі для нечіткого кластерування, зокрема потоків даних; проведено аналіз існуючих систем, що еволюціонують, для кластерування даних, зокрема нечіткого, і розглянуті особливості та труднощі послідовного кластерування, та описні два підходи, переваги яких поєднує у собі пропонована система: нечітке та ієрархічне кластерування.  

\section{Труднощі та особливості відомих методів кластерування даних}

Завдання кластерування (класифікації без вчителя) досить часто зустрічається в багатьох додатках, пов'язаних з видобутком знань, де у режимі самонавчання необхідно розбити деякий вхідний нерозмічений масив даних на однорідні в прийнятому сенсі групи. Розглянемо деякі iєрархічні та розподільні методи кластерування, адже, як буде показано далі, пропована у цьому розділі самонавчанна система поєднує у собі переваги обох підходів.

Розподільні методи кластерування (чи то жорсткі, чи нечіткі) можна назвати динамічними у тому сенсі, що належність певного образу до певного кластеру (кластерів для нечіткої модифікації) не є постійною. Нездатність методів розподільного кластерування самостійно визначити кількість кластерів у певному сенсі компенсується тим, що знання форми чи розміру кластерів може стати у нагоді на етапі вибору відповідних прототипів та насамперед типу відстані (міри схожості) і суттєво поліпшити кінцеве розбиття вибірки. Але, варто зазначити чутливість таких методів до початкової ініціалізації, шуму і викидів, їх сприйнятливість до локальних мінімумів, адже вони ґрунтуються на оптимізації певної цільової функції. Типові методи розподільного кластерування мають обчислювальну складність $\mathcal{O}\left(N\right)$ для тренувальною вибірки розміру $N$ \cite{ref43}.

Серед методів ієрархічного кластерування виділяють два основних типи: висхідні та спадні методи. Спадні методи працюють за принципом «зверху-вниз»: на початку припускається, що всі образи належать до одного кластеру, який потім розбивається на все більш дрібні кластери. Більш поширеними є висхідні алгоритми, які на початку роботи поміщають кожен об'єкт до окремого кластеру, а потім об'єднують кластери у все більш крупні, доки усі образи не матимуть свій власний кластер. Таким чином будується система вкладених розбиттів. Результати таких алгоритмів зазвичай представляють у вигляді дерева - дендрограми (тут можна провести аналогію між висхідними та спадними методами ієрархічного кластерування та конструктивними і деструктивними системами, що еволюціонують. У цій роботі здебільшого розглядається конструктивний підхід, тому пропонована самонавчанна система є у певному сенсі альтернативою системам висхідного ієрархічного кластерування, що здатна працювати у режимі реального часу).

Для обчислення відстаней між кластерами використовуються такі відстані:
\begin{itemize}
\item одинарний зв'язок (відстань найближчого сусіда): відстань між двома кластерами визначається відстанню між двома найбільш близькими об'єктами (найближчими сусідами) у різних кластерах. Результуючі кластери мають тенденцію об'єднуватися в ланцюжки.
\item  повний зв'язок (відстань найбільш віддалених сусідів): відстані між кластерами визначаються найбільшою відстанню між будь-якими двома об'єктами різних кластерів (тобто найбільш віддаленими сусідами). Цей метод зазвичай працює дуже добре, коли об'єкти походять з окремих груп. Якщо ж кластери мають видовжену форму або їх природний тип є «ланцюжковим» цей метод непридатний.
\item  незважене попарне середнє: відстань між двома різними кластерами обчислюється як середня відстань між усіма парами об'єктів у них. Метод ефективний, коли об'єкти формують різні групи, проте він працює однаково добре і у випадках протяжних («ланцюжкового» типу) кластерів.
\item  зважене попарне середнє: метод ідентичний методу незваженого попарного середнього, за винятком того, що при обчисленнях розмір відповідних кластерів (тобто число об'єктів, що містяться в них) використовується у якості вагового коефіцієнту. Тому доцільно використовувати даний метод у випадку нерівних за розміром кластерів.
\item  незважений центроїдний метод: у цьому методі відстань між двома кластерами визначається як відстань між їх центрами тяжкості.
\item  зважений центроїдний метод (медіана): цей метод ідентичний попередньому, за винятком того, що при обчисленнях використовуються ваги для обліку різниці між розмірами кластерів. Тому, якщо є або підозрюються значні відмінності в розмірах кластерів, цей метод має перевагу над попереднім
\end{itemize}

Порівняно з розподільним кластеруванням, методи ієрархічного кластерування легко ідентифікують викиди, не потребують визначеної кількості кластерів та
нечутливі до початкової ініціалізаціі чи локальних мінімумів. До недоліків варто віднести нездатність методів визначати кластери, що перекривають один інший. Крім того, ієрархічне кластерування є статичним, тобто образи віднесені до певного кластеру на ранніх стадіях не можуть бути пізніше належними іншому, що унеможливлює створення модифікацій методів для послідовного кластерування, на відміну від розподільного кластерування. Методи ієрархічного кластерування здебільшого мають обчислювальну складність принаймні $\mathcal{O}\left(N^2\right)$, що робить їх використання недоцільним для велких наборів даних.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Нечітке послідовне кластерування}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Традиційний підхід до завдання кластерування припускає, що кожне спостереження належить лише одному кластерові, в той час як більш природною видається ситуація, коли кожен вектор-спостереження оброблюваної вибірки можна віднести відразу декільком класам з різними рівнями належності. Така ситуація є предметом розгляду нечіткого кластерного аналізу~\cite{ref21, ref22, ref23, ref24, ref19, ref25}, а для його вирішення широко використовується апарат обчислювального інтелекту [9-12] і, насамперед, нейро-фаззі підхід [13]. При цьому більшість алгоритмів нечіткої кластеризації призначені для роботи в пакетному режимі, коли усі дані, що підлягають обробці, задані апріорно. Вихідною інформацією для такої задачі є вибірка спостережень, сформована з \mbox{$m$-вимірних} векторів ознак \mbox{$x\left(1\right), x\left(2\right),\dots,x\left(1\right),\dot,x\left(N\right)$}, при цьому для зручності чисельної реалізації вихідні дані попередньо деяким чином перетворюються, наприклад, так, щоб всі спостереження належали до гіперкубу~$[-1,1]^n$ або одиничній гіперсфері~$\left\|x\left(k\right)\right\|^2$.

Результатом такого кластерування є розбиття масиву вихідних даних на $M$~кластерів з певним рівнем належності $u_J\left(k\right)$ \mbox{$k$-ого} вхідного образу $x\left(k\right)$ до \mbox{$J$-ого} кластеру \mbox{($J = 1,2,\dots,M$)}. Передбачається, що $N$ та $M$, а також параметри кластерування (в першу чергу, фаззіфікатор) задані апріорі і не змінюються під час обробки даних. Варто зауважити, що існує широкий клас задач динамічного інтелектуального аналізу даних і потоків даних (Dynamik Data Mining, Data Stream Mining) \cite{ref5, ref6, ref27, ref28, ref29, ref30, ref31} у випадку, коли дані надходять у вигляді послідовного потоку в онлайн режимі. Отже, кількість вхідних образів $N$ у цьому випадку не обмежується, а $k$ набуває значення поточного дискретного часу.

Самоорганізовні мапи Кохонена \cite{ref16} добре пристосовані для вирішення завдання кластерування в онлайн режимі. Ці нейронні мережі мають один шар латеральних з'єднань та навчаються за принципами «переможець отримує все» або «переможець отримує більше».
Самоорганізовні мапи також відомі своєю ефективністю вирішення задачі кластерування класів, що перетинаються. Тому, у зв'язку з дедалі більшою кількістю завдань кластерування потоків даних, з'явилися самонавчанні нейро-фазі гібридні системи, що у деякому сенсі поєднують у собі самоорганізовні мапи Кохонена (SOM) та метод нечітких \mbox{$c$-середніх} Бездека \cite{ref15,ref32,ref33,ref34,ref35,ref36,ref37,ref38,ref39,ref40,ref41,ref42}. Такі гібридні системи володіють обширною функціональністю завдяки використанню спеціальних алгоритмів налаштування, що ґрунтуються на процедурах оптимізації прийнятої цільової функції, але потребують попередньо заданої кількості класетрів та фіксованого значення фаззіфікатору. 

\section{Критерії дійсності нечіткого кластерування}

Оскільки коефіціент розбиття залежить лише від значень функції належності, йому властиві деякі недоліки. Коли фаззіфікатор наближається до $1$, індекс дійсності буде однаковим для усіх $c$, коли фаззіфікатор наближається до $\infty$.

Індекс розбиття ентропії PE (Partition Entorpy Index) - ще один критерій дійсності нечіткого кластерування, запропонований \hl{(Bezdek, 1974a, 1981)}, що залежить лише від значень фунції належності

\begin{equation}
\label{eq:validityIndexPE}
PE = -\frac{1}{N}\sum^{M}_{l=1}\sum^N_{i=1}{u_{li}\log_a\left(u_{li}\right)}.
\end{equation}
\medskip

Індекс ентропії розбиття набуває значень у інтервалі $\left[0,\log_aM\right]$. Що ближче значення $PE$ до $0$, то жорсткіше розбиття вхідних даних. Значення $PE$ близькі до верхньої межі вказують на відсутність будь-якої структури, притаманної набору вхідних даних, або на нездатність методу її виявити. Індекс ентропії розбиття має ті самі недоліки, що і коефіцієнт розбиття. Оптимальній кількості кластерів $M^{*}$ відповідає мінімальне значення~\eqref{eq:validityIndexPE}.

Фукуяма та Суґено запропонували індекс дійсності нечіткого кластерування, залежний як від рівнів належності так і від самих вхідних даних:

\begin{equation}\label{eq:validityIndexFS}
FS =\sum^{N}_{i=1}\sum^M_{l=1}{u_{li}^\beta\left(\left\|x_i-z_l\right\|^2-\left\|z_l-z\right\|^2\right)},
\end{equation}
\medskip

де $z$ та $z_l$~--~ середнє арифметичне усієї виборки та образів віднесених до кластеру $M_l$ відповідо. З визначення \eqref{eq:validityIndexFS} видно, що малі значення FS говорять про компактні добре визначені кластери.

Нечітка множина $i$-ого образу визначається як

\begin{equation}
\tilde{A_l}=\sum^N_{i=1}\frac{u_{li}}{x_i},l=1,2,\dots,M.
\end{equation}
Ступінь, в якій $A_l$ є підмножиною $A_p$ визначається наступним чином
\begin{equation}\label{eq:validityIndexFSim1}
\begin{cases}
S\left(\tilde{A_l},\tilde{A_p}\right)=\frac{U\left(\tilde{A_l}\cap\tilde{A_p}\right)}{U\left(\tilde{A_l}\right)},\\
U\left(\tilde{A_j}\right)=\sum^N_{i=1}u_{ji}.
\end{cases}
\end{equation}
\medskip

Зважаючи на \eqref{eq:validityIndexFSim1}, можна запропонувати такі варіанти обчислення міри подібності:

\begin{subequations}\label{eq:validityIndexFSim2}
\begin{align}
&N_1\left(\tilde{A_l},\tilde{A_p}\right)=\frac{S\left(\tilde{A_l},\tilde{A_p}\right) + S\left(\tilde{A_p},\tilde{A_l}\right)}{2},\\
&N_2\left(\tilde{A_l},\tilde{A_p}\right)=\min\left(S\left(\tilde{A_l},\tilde{A_p}\right),S\left(\tilde{A_p},\tilde{A_l}\right)\right),\\
&N_3\left(\tilde{A_l},\tilde{A_p}\right)=S\left(\tilde{A_l}\cup\tilde{A_p},\tilde{A_p}\cap\tilde{A_l}\right).
\end{align}
\end{subequations}
\medskip

Тоді індекс дійсності кластерування, що грунтується на нечіткій подібності, можна визначити як

\begin{equation}
FSim=\max\limits_{1\leq{l}\leq{M}}\max\limits_{1\leq{p}\leq{M},p\neq{l}}N\left(\tilde{A_l},\tilde{A_p}\right),
\end{equation}
\medskip

де міру нечіткої подібності $N\left(\tilde{A_l},\tilde{A_p}\right)$ можна знайти за будь-яким виразом~\eqref{eq:validityIndexFSim2}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Архітектура каскадної мережі, що еволюціонує, для нечіткого кластерування}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Архітекуру каскадної мережі, що еволюціонує, для нечіткого кластерування наведено на 
  
До нульовго шару системи послідовно передаються дані у формі векторного сигналу $x(k)=(x_{1}(k),x_{2}(k),\dots, x_{1}(k))^{T}$, де $k=1,2,\dots,N,N+1,\dots$~---~індекс поточного дискретного часу. Вхідні сигнали надходять до всіх вузлів системи $N_{j}^{[m]}$, де $j=1,2,\dots,q$ - кількість вузлів у пулі-ансамблі, $m=1,2,\dots$~---~номер каскаду. Вузол кожного каскаду призначений для онлайн кластерування потоку данних і відрізняється від вузлів-сусідів використаним алгоритмом навчання або, у випадку спільного методу кластерування, параметрами алгоритму. Кількість кластерів для кожного каскаду є відомою і дорівнює $m+1$. Елемент~$PC_{j}^{[m]}$ дає оцінку якості кластерування кожного вузла у пулі, а елемент $PC^{*[m]}$ визначає найкращий елемент у пулі кожного каскаду. Елемент системи~$XB^{[m]}$ оцінює загальную якість кластеризації пула, враховуючи прийняту кількість кластерів $m+1$. Таким чином, система розв'язує задачу кластерування нестаціонарного потоку даних в умовах невизначенності щодо кількості кластерів, а також їх вигляду і рівню взаємного перекриття. І, нарешті, вихідний вузол системи~$XB^{*}$, порівнюючи якість кластеризації кожного з каскадів, виділяє найкращий результат~---~кількість кластерів, їх центроїди-прототипи та рівні належності кожного спостереження до кожного з сформованих центроїдів. Незважаючи на удавану громіздкість чисельна реалізація запропонованої архітектури не викликає принципових труднощів завдяки тому, що потік даних, що надоходить до системи, може оброблятися у паралельному режимі вузлами системи $N_{j}^{[m]}$\cite{ref5,ref6}.
	
\section{Адаптивне навчання вузлів каскадної нейро-фаззі системи, що еволюціонує}
  
В основі алгоритмів навчання вузлів системи лежать алгоритми нечіткого кластерування, засновані на цільових функціях, такі, що вирішують задачу їх оптимізації при деяких апріорних припущеннях. Найбільш поширеним є ймовірнісний підхід, заснований на мінімізації цільової функції
  
\begin{equation}\label{eq:goal_function}
E\left(u_{jl}^{[m]}\left(k\right),\: c_{jl}^{[m]}\right)=\sum\limits_{k=1}^{N}\sum\limits_{l=1}^{m+1}\left(u_{jl}^{[m]}\left(k\right)\right)^{\beta }\left \| x\left(k\right)-c_{jl}^{[m]} \right \|^2
\end{equation}
\medskip

при обмеженнях

\begin{equation}\label{eq:goal function constraints}
\sum\limits_{l=1}^{m+1}\left(k\right)=1,\quad0\leq \sum\limits_{k=1}^{N}u_{jl}^{[m]}\left(k\right)\leq N
\end{equation}
\medskip

де $u_{ij}^{[m]}\left(k\right)\in [0,1]$~---~pівень належності спостереження $x\left(k\right)$ до $l$-ого кластеру у $j$-ому вузлі каскаду $m$,

$c_{jl}^{[m]}$~---~$(n\times1)$~-~вимірній вектор-центроїд $l$-ого кластеру у $j$-ому вузлі каскаду $m$,

$\beta>1$~---~параметр фаззіфікації (фаззіфікатор), що визначає розмитість границь між кластерами,

$k=\overline{1,N}$~---~номер образу ($N$~---~кількість образів у вхідній виборці, що, у рамках класичного подходу Бездека, вважається незмінною та такою, що задана апріорі).

\begin{samepage}
Вводячи функцію Лагранжа

\begin{equation}
\begin{aligned}
L\left(u_{jl}^{[m]}\left(k\right),\, c_{jl}^{[m]},\,\lambda _j^{[m]}\left(k\right)\right)= \sum\limits_{k=1}^{N}\sum\limits_{l=1}^{m+1}\left(u_{jl}^{[m]}\left(k\right)\right)^\beta\left \| x\left(k\right)-c_{jl}^{[m]} \right \|^2+\\
+\sum\limits_{k=1}^{N}\lambda _j^{[m]}\left(k\right)\left ( \sum\limits_{l=1}^{m+1}u_{jl}^{[m]}\left(k\right)-1 \right)
\end{aligned}
\end{equation}
\end{samepage}
\medskip

(тут $\lambda_j^{[m]}\left(k\right)$~---~невизначений множник Лагранжа) та вирішивши систему рівнянь Каруша-Куна-Таккера, нескладно отримати шукане рішення у вигляді

\begin{equation}\label{eq:generalizedFCM}
\begin{cases}
u_{jl}^{[m]}\left(k\right)=\frac{\displaystyle\left(\left \|x\left(k\right)-c_{jl}^{[m]}\right \|^2\right)^{\frac{\scriptstyle1}{\scriptstyle1-\beta}}}{\displaystyle\sum\limits_{l=1}^{m+1}\left(\left\|x\left(k\right)-c_{jl}^{[m]}\right\|^2\right)^{\frac{\scriptstyle1}{\scriptstyle1-\beta}}},\\
c_{jl}^{[m]}=\frac{\displaystyle\sum\limits_{k=1}^{N}\left(u_{jl}^{[m]}\left(k\right)\right)^{\beta}x\left(k\right)}{\displaystyle\sum\limits_{k=1}^{N}\left(u_{jl}^{[m]}\left(k\right)\right)^{\beta}},\\
\lambda_{j}^{[m]}\left(k\right)=-\left(\left(\displaystyle\sum_{l=1}^{m+1}\beta\left\|x\left(k\right)-c_{jl}^{[m]}\right\|^2\right)^{\frac{\scriptstyle1}{\scriptstyle1-\beta}}\right)^\frac{\scriptstyle1}{\scriptstyle1-\beta},
\end{cases}
\end{equation}
\medskip

що при $\beta = 2$ збігається з алгоритмом нечітких с-середніх Бездека~(FCM)~\cite{ref13} i приймає форму

\begin{equation}\label{eq:classicFCM}
\begin{cases}
u_{jl}^{[m]}\left(k\right)=\frac{\displaystyle\left\|x\left(k\right)-c_{jl}^{[m]}\right\|^{-2}}{\displaystyle\sum_{l=1}^{m+1}\left\|x\left(k\right)-c_{jl}^{[m]}\right\|^{-2}},\\
c_{jl}^{[m]}=\frac{\displaystyle\sum_{k=1}^{N}\left(u_{jl}^{[m]}\left(k\right)^{2}x\left(k\right)\right)}{\displaystyle\sum\limits_{k=1}^{N}\left(u_{jl}^{[m]}\left(k\right)\right)^{2}}.
\end{cases}
\end{equation}
\medskip

Тут варто відзначити, що вибір фаззіфікатора~$\beta = 2$ в \eqref{eq:classicFCM} не дає жодних переваг порівняно з довільним значенням~$\beta$ у \eqref{eq:generalizedFCM}, у зв'язку з чим пропонується використовувати різні значення параметра фаззіфікації для кожного вузла пулу каскаду, після чого вибирати найкращий результат залежно від прийнятого критерію якості нечіткого кластерування \cite{ref7,ref8,ref9}.

Для послідовної обробки потоку даних, що надходять в online режимі, y~\cite{ref10,ref11} були запропоновані рекурентні алгоритми, в основі яких лежить процедура нелінійного програмування Ерроу-Гурвіца-Удзави~\cite{ref12}. Так, пакетному алгоритмові \eqref{eq:generalizedFCM} відповідає вираз

\begin{equation}\label{eq:recurrentFCM}
\begin{cases}
u_{jl}^{[m]}\left(k+1\right)=\frac{\displaystyle\left\|x\left(k+1\right) - c_{jl}^{[m]}\left(k\right)\right\|^{\frac{\scriptstyle1}{\scriptstyle1-\beta}}}{\displaystyle\sum\limits_{l=1}^{m+1}\left\|x\left(k+1\right) - c_{jl}^{[m]}\left(k\right)\right\|^{\frac{\scriptstyle1}{\scriptstyle1-\beta}}},\\
c_{jl}^{[m]}\left(k+1\right)=c_{jl}^{[m]}\left(k\right)+\eta\left(k+1\right)\left(u_{jl}^{[m]}\left(k+1\right)\right)^{\beta_{\scriptstyle{j}}}\left(x\left(k+1\right)-c_{jl}^{[m]}\left(k\right)\right),
\end{cases}
\end{equation}
\medskip

(тут $\eta\left(k+1\right)$~---~параметр кроку навчання), що є узагальненням алгоритму навчання Чанга-Лі~\cite{ref14} і при $\beta=2$ близьке до ґрадієнтної процедури Парка-Деера~\cite{ref15}.

\begin{equation}
\begin{cases}
u_{jl}^{[m]}\left(k+1\right)=\frac{\displaystyle\left\|x\left(k+1\right) - c_{jl}^{[m]}\left(k\right)\right\|^{-2}}{\displaystyle\sum\limits_{l=1}^{m+1}\left\|x\left(k+1\right) - c_{jl}^{[m]}\left(k\right)\right\|^{-2}},\\
c_{jl}^{[m]}\left(k+1\right)=c_{jl}^{[m]}\left(k\right)+\eta\left(k+1\right)\left(u_{jl}^{[m]}\left(k+1\right)\right)^{2}\left(x\left(k+1\right)-c_{jl}^{[m]}\left(k\right)\right).
\end{cases}
\end{equation}
\medskip

Варто зауважити, що, розглянувши співвідношення~\eqref{eq:recurrentFCM} з позицій навчання Кохоненової самоорганізованої мапи~(SOM)~\cite{ref16}, можна помітити, що множник~$\left(u_{jl}^{[m]}\right)^{\beta_{\scriptstyle{j}}}$ відповідає функції сусідства в правилі навчання на основі принципу «переможецю дістається більше», маючи при цьому дзвонуватий вигляд.

Вочевидь, у випадку, коли $\beta_{j}=1$ та $u_{jl}^{[m]}\left(k\right)\in\left[0,1\right]$, процедура~\eqref{eq:recurrentFCM} збігається з чітким алгоритмом $c$-середніх (HCM), коли ж $\beta_{j}=0$, маємо стандартне правило навчання Кохонена «переможцю дістається все»~\cite{ref16}

\begin{equation}\label{eq:winnerTakesAllKohonenLearningRule}
c_{jl}^{[m]}\left(k+1\right)=c_{jl}^{[m]}\left(k\right)+\eta\left(k+1\right)\left(x\left(k+1\right)-c_{jl}^{[m]}\left(k\right)\right),
\end{equation}
\medskip

запропоноване Каш'япом та Блейдоном~\cite{ref17} у шістдесятих роках минулого століття. Легко побачити, що процедура~\eqref{eq:winnerTakesAllKohonenLearningRule} оптимізує цільову функцію

\begin{equation}
E\left(c_{jl}^{[m]}\right)=\sum\limits_{k=1}^{N}\left\|x\left(k\right)-c_{jl}^{[m]}\right\|^{2},\quad\sum\limits_{l=1}^{m+1}N_{l}=N,
\end{equation}
\medskip

мінімум якої збігається із середнім арифметичним

\begin{equation}\label{eq:arithmeticMean}
c_{jl}^{[m]}=\frac{1}{N}\sum\limits_{k=1}^{{N}_{\scriptstyle{l}}}x\left(k\right),
\end{equation}
\medskip

де $N_{l}$~---~кількість векторів, віднесених до $l$-го кластеру у процесі конкуренції.

Якщо записати~\eqref{eq:arithmeticMean} у рекурентній формі, отримаємо оптимальний алгоритм самонавчання Ципкіна~\cite{ref18}

\begin{equation}
c_{jl}^{[m]}(k+1) = c_{jl}^{[m]}(k)+\frac{1}{N_{l}(k+1)}\left(x\left(k+1\right)-c_{jl}^{[m]}(k)\right),
\end{equation}
\medskip

де $N_{l}\left(k+1\right)$~---~число векторів, віднесених до $l$-го кластеру в $k+1$-й момент реального часу, що є стандартною процедурою стохастичної апроксимації.

У загальному випадку алгоритм навчання~\eqref{eq:recurrentFCM} вузла можна розглядати як правило самонавчання нечіткої модифікації самоорганізовної мапи Кохонена, архітектура якої наведена на рис

TODO:рис

Тут $N_{jl}^{[m]K}$~---~стандартні нейрони Кохонена, пов'язані між собою латеральними зв'язками, що налаштовуються згідно "переможцю дістається більше" правилу навчання на основі другого співвідношення~\eqref{eq:recurrentFCM}. Вузли $N_{jl}^{[m]u}$ обчислюють рівні належності згідно першому співвідношенню~\eqref{eq:recurrentFCM}. Вузли $N_{j}^{[m]}$ кожного з каскадів відрізняються тільки фаззіфікатором алгоритму самонавчання, а вузол кожного наступного каскаду містить додатково один нейрон Кохонена і один елемент для розрахунку рівнів належності.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Керування каскадами самонавчанної нейро-фаззі системи, що еволюціонує}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Якість кластерування кожого вузла системи може бути оцінена за допомогою будь-якого з індексів, що використовуються у задачах нечіткого кластерування \cite{ref19}. Одим за найпростіших та разом з тим найефективніших індексів є так званий «коефіцієнт розбиття», який, власне, є середнім квадратів рівнів належності всіх спостережень до кожного кластеру і має вигляд

\begin{equation}
\V{PC}_j^{[m]}=\frac{1}{N}\sum\limits_{k=1}^{N}\sum\limits_{l=1}^{m+1}\left(u_{jl}^{[m]}\left(k\right)\right)^{2}.
\end{equation}
\medskip

Цей коефіцієнт має ясний фізичний зміст: щокраще виражені кластери, то більше значення $\V{PC}_{j}^{[m]}$ (верхня межа~---~$\V{PC}_{j}^{[m]}=1$), а його мінімум $\V{PC}_{j}^{[m]}=\left(m+1\right)^{-1}$ досягається, якщо дані належать усім кластерам рівномірно, що, вочевидь, є тривіальним рішенням. Для розглянутої нами системи цей коефіцієнт зручний тим, що його легко розрахувати в online режимі 

\begin{equation}\label{eq:reccurentPartitioningCoefficient}
\V{PC}_{j}^{[m]}\left(k+1\right)=\V{PC}_j^{[m]}(k)+\frac{\displaystyle1}{k+1}\left(\sum\limits_{l=1}^{m+1}\left(u_{jl}^{[m]}\left(k+1\right)\right)^{2}-\V{PC}_j^{[m]}\left(k\right)\right).
\end{equation}
\medskip

Розрахунок коефіцієнту розбиття проводиться для кожного вузла системи разом з налаштуванням їх параметрів, тобто співвідношення~\eqref{eq:recurrentFCM} та~\eqref{eq:reccurentPartitioningCoefficient} реалізуються одночасно. На кожному такті навчання вузол~$PC^{*[m]}$ визначає найкращий елемент каскаду, що забезпечує максимальне значення коефіцієнта розбиття у кожний поточний момент $k$, при цьому не виключається ситуація, коли в різні моменти обробки інформації "переможцями" виявляться різні вузли.

Кожен з каскадів розглянутої системи відрізняється від інших числом кластерів, на які розбивається оброблюваний потік даних. Тому якщо вузли $PC_{j}^{[m]}$ і $PC^{*[m]}$ оцінюють якість кластеризації без урахування кількості сформованих класів, то вузли системи, позначені $XB^{[m]}$ та $XB^{*}$, оцінюють результати з урахуванням числа кластерів у кожному каскаді. Одним з таких показників є індекс Ксі-Бені~\cite{ref20}, який для фіксованої вибірки з $N$~спостережень може бути записаний у вигляді

\begin{equation}\label{eq:XieBeniIndex}
\V{XB}_{j}^{[m]}=\frac{\displaystyle\left(\sum\limits_{k=1}^{N}\sum\limits_{l=1}^{m+1}\left(u_{jl}^{[m]}\left(k\right)\right)^{2}\left\|x\left(k\right)-c_{jl}^{[m]}\right\|^{2}\right)\Big/{N}}{\min\limits_{l\neq{q}}\left\|c_{jl}^{[m]}-c_{jq}^{[m]}\right\|^{2}}=\frac{\displaystyle\V{NXB}_{j}^{[m]}}{\V{DXB}_{j}^{[m]}}
\end{equation}
\medskip

Вираз \eqref{eq:XieBeniIndex} також можна записати у рекурентній формі

\begin{samepage}
\begin{equation}\label{eq:recurrentXieBeniIndex}
\begin{aligned}
&\V{XB}_{j}^{[m]}\left(k+1\right)=\frac{\V{NXB}_{j}^{[m]}\left(k+1\right)}{\V{DXB}_{j}^{[m]}\left(k+1\right)}=\\
&\frac{\displaystyle\V{NXB}_{j}^{[m]}\left(k\right){+}\frac{1}{k{+}1}\left({\sum\limits_{l=1}^{m+1}\left(u_{jl}^{[m]}\left(k{+}1\right)\right)^{2}\left\|x\left({k+}1\right){-}c_{jl}^{[m]}\left(k{+}1\right)\right\|^{2}}{-}\V{NXB}_{j}^{[m]}\left(k\right)\right)}{\displaystyle\min\limits_{l\neq{q}}\left\|c_{jl}^{[m]}\left(k+1\right)-c_{jq}^{[m]}\left(k+1\right)\right\|^{2}},\end{aligned}
\end{equation}
\end{samepage}
\medskip

при цьому рекурентні вирази \eqref{eq:reccurentPartitioningCoefficient} та \eqref{eq:recurrentXieBeniIndex} реалізуються одночасно.

Індекс Ксі-Бені є по суті співвідношенням відхилення всередині кластерів~$\V{NXB}_j^{[m]}$ до величини поділу кластерів~$\V{DXB}_j^{[m]}$. Оптимальному числу кластерів у каскаді відповідає мінімальне значення \eqref{eq:XieBeniIndex} та \eqref{eq:recurrentXieBeniIndex}. Тому процес нарощування каскадів у системі продовжується доки значення індексу не почне збільшуватися. Цей процес контролює вузол архітектури~$\V{XB}^*$.

Варто заувважити, що оскільки вузли кожного каскаду відрізняються тільки значенням фаззифікатору, ефективність роботи кожного каскаду доцільно оцінювати за допомогою розширеного індексу Ксі-Бені~$\V{EXB}$~\cite{ref19}.

\begin{equation}
\V{EXB}_j^{[m]}=\frac{\displaystyle\left(\sum\limits_{k=1}^{N}\sum\limits_{l=1}^{m+1}\left(u_{jl}^{[m]}\left(k\right)\right)^{\scriptstyle\beta_{\scriptstyle[m]}}\left\|x\left(k\right)-c_{jl}^{[m]}\right\|^{2}\right)\Big/{N}}{\displaystyle\min\limits_{l\neq{q}}\left\|c_{jl}^{[m]}-c_{jq}^{[m]}\right\|^2}=\frac{\V{NEXB_j^{[m]}}}{\V{DEXB_j^{[m]}}}
\end{equation}
\medskip

або його рекурентої форми

\begin{samepage}
\begin{equation}
\begin{aligned}
&\V{XB}_{j}^{[m]}\left(k+1\right)=\frac{\V{NXB}_{j}^{[m]}\left(k+1\right)}{\V{DXB}_{j}^{[m]}\left(k+1\right)}=\\
&\frac{\displaystyle\V{NXB}_{j}^{[m]}\left(k\right){+}\frac{1}{k{+}1}\left({\sum\limits_{l=1}^{m+1}\left(u_{jl}^{[m]}\left(k{+}1\right)\right)^{\beta_{\scriptstyle{[m]}}}\left\|x\left({k+}1\right){-}c_{jl}^{[m]}\left(k{+}1\right)\right\|^{2}{-}\V{NXB}_{j}^{[m]}\left(k\right)}\right)}{\displaystyle\min\limits_{l\neq{q}}\left\|c_{jl}^{[m]}\left(k+1\right)-c_{jq}^{[m]}\left(k+1\right)\right\|^{2}},
\end{aligned}
\end{equation}
\end{samepage}
\medskip

де $\beta^{[m]}$~---~фаззіфікатор найкращого з вузлів $m$-го каскаду.

Таким чином, процес еволюції пропонованої системи зумовлений максимізуванням поточного значення показника якості кластерування потоку даних, що надходять на обробку в онлайн режимі.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Висновки до розділу~\ref{ch:evolvingClusteringSystem}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
\item Розглянуто завдання нечіткого кластерування у режимі послідовного надходження даних до системи.
\item Запропоновано метод визначення локально оптимальної кількості кластерів і значення параметру фаззіфікації для послідовного кластерування потоків даних.
\item Запропоновано архітектуру і метод самонавчання каскадної нейро-фаззі системи, що еволюціонує, для послідовного кластерування потоків даних з автоматичним визначенням оптимальної кількості кластерів. Кожен вузол кожного каскаду системи вирішує завдання кластерування незалежно від інших, що дозволяє організувати паралельну обробку інформації в каскадах, тобто підвищити швидкодію цього процесу. Система не містить жодних порогових параметрів, що задаються суб'єктивно, а процес оцінювання якості її функціонування визначається шляхом відшукання оптимального значення певного індексу дійсності розбиття даних на кластери (їх поточна оцінка також проводиться в режимі реального часу). Відмінною особливістю пропонованої системи є те, що вона самостійно визначає і поточне значення фаззіфікатору, і оптимальну кількість кластерів на кожному етапі обробляння даних.
\end{enumerate}