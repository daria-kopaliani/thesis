\documentclass{vakthesis}
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R


\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian,ukrainian]{babel}
\usepackage{geometry}
\usepackage{mathtools}% http://ctan.org/pkg/mathtools
\usepackage{amsmath}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{color,soul}
\usepackage{graphicx}
\usepackage{MnSymbol}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage[labelsep=endash]{caption}
\usepackage[shortcuts]{extdash}

\graphicspath{{images/}}


%\geometry{hmargin={30mm,15mm},lines=29,vcentering}
\everymath=\expandafter{\the\everymath\displaystyle}

\geometry{a4paper, total={170mm,257mm}, left=25mm, top=20mm}

%This is to make proper table captions, you most definately do *not* want to edit this
\makeatletter
\let\ORG@makecaption\@makecaption
\let\ORGlongtable\longtable
\let\ORGLT@makecaption\LT@makecaption
\AtBeginDocument{%
  \let\@maketablecaption\ORG@makecaption
  \let\longtable\ORGlongtable
  \let\LT@makecaption\ORGLT@makecaption
}
\makeatother


%\DeclareMathSizes{10}{10}{10}{10}


\begin{document}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in grepl(db, input): input string 43 is invalid in this locale}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in grepl(db, input): input string 44 is invalid in this locale}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in grepl(db, input): input string 45 is invalid in this locale}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in grepl(db, input): input string 48 is invalid in this locale}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in grepl(db, input): input string 53 is invalid in this locale}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in grep("{}\textasciicircum{}\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}bibliography.+"{}, input, value = TRUE): input string 43 is invalid in this locale}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in grep("{}\textasciicircum{}\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}bibliography.+"{}, input, value = TRUE): input string 44 is invalid in this locale}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in grep("{}\textasciicircum{}\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}bibliography.+"{}, input, value = TRUE): input string 45 is invalid in this locale}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in grep("{}\textasciicircum{}\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}bibliography.+"{}, input, value = TRUE): input string 48 is invalid in this locale}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in grep("{}\textasciicircum{}\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}bibliography.+"{}, input, value = TRUE): input string 53 is invalid in this locale}}\end{kframe}
\end{knitrout}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Моделювання та практичне застосування розроблених методів та архітектур}%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{ch:Experiments}

Для проведення чисельних експериментів, що наведені у підрозділах~\ref{sec:ENFNExperiments}, \ref{sec:SISOCascadedSystemOnENFNExperiments} та \ref{sec:MIMOCascadedSystemExperiments} було обрано такі критерії оцінки:

\begin{itemize}
\item RMSE (Root Mean Squared Error, середньоквадратична похибка),
\item SMAPE (Symmetric Mean Absolute Percentage Error, симетрична абсолютна процентна похибка),
\end{itemize}

що обчислюються за формулами 

\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{N}\sum\limits_{k=1}^{N}\left(y\left(k\right)-\hat{y}\left(k\right)\right)},
\end{equation}

та 

\begin{equation}
\text{SMAPE} = \frac{1}{N}\sum\limits_{k=1}^{N}\frac{\left|y\left(k\right)-\hat{y}\left(k\right)\right|}{y\left(k\right)+\hat{y}\left(k\right)}
\end{equation}
\medskip

відповдіно, де $y$~-- шуканий сигнал, $\hat{y}$~-- вихідний сигнал системи.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Моделювання розширенного нейро-фаззі нейрона}
\label{sec:ENFNExperiments}

Датасет для першого експерименту (фазовий портрет наведено на рис.~\ref{fig:ENFNDataSetPhasePortrait}) було сгенеровано за формулою

\begin{equation}
\sin{\bigl(k + \sin\left(2k\right)\bigr)} \textnormal{ для } k \in \left[1, 600\right].
\end{equation}
\medskip

Результати експерименту наведено у таблиці нижче, а також проілюстровано залежність точності прогнозу від порядку висновування (рис.~\ref{fig:ENFNErrorFromInferenceOrder}) та кількості фунцій належності (рис.~\ref{fig:ENFNErrorFromNumberOfMembershipFunctions}).

\begin{figure}[H]
\begin{center}
\includegraphics{ENFNDataSetPhasePortrait.png}
\caption{Фазовий портрет штучно сгенерованого датасету}
\label{fig:ENFNDataSetPhasePortrait}
\end{center}
\end{figure}

Отже, як видно з таблиці~\ref{tab:ENFNPredictionAccuracyGeneratedDataSet} та рис.~\ref{fig:ENFNPrediction3IO3MF}, можна зробити висновок, що точність прогнозу розширеного нео-фаззі нейрону вища від точності звичайного нео-фаззі нейрону (ENFN з нульовим порядком висновування).

\begin{figure}[H]
\begin{center}
\includegraphics{ENFNErrorFromInferenceOrder.png}
\caption{Похибка прогнозу розширенного нео-фаззі нейрону від порядку висновування (для трьох та п'яти функцій належності)}\label{fig:ENFNErrorFromInferenceOrder}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics{ENFNErrorFromNumberOfMembershipFunctions.png}
\caption{Похибка прогнозу розширенного нео-фаззі нейрону від кількості фунцій належності (порядок висновування - 2)}
\label{fig:ENFNErrorFromNumberOfMembershipFunctions}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics{ENFNPrediction3IO3MF.png}
\caption{Прогноз розширенного нео-фаззі нейрону з 3 трикутними функціями належності, що реалізує нечітке висновування 3-ого порядку (зелена лінія -- шуканий сигнал, синя пунктирна лінія -- прогноз розширенного нео-фаззі нейрону, червона лінія -- похибка; жовтобагряна вертикальна лінія позначає закінчення тренувальної частини датасету)}
\label{fig:ENFNPrediction3IO3MF}
\end{center}
\end{figure}

\begin{table}[H]
\caption{Точність прогнозу розширенного нео-фаззі нейрону на штучно сгенерованому датасеті} \label{tab:ENFNPredictionAccuracyGeneratedDataSet}
\centering \small \begin{tabular}{rcc} \hline
Нечітке висновування 0 порядку \\ \hline
Функції належності & RMSE & SMAPE \\ \hline
%2 & 0.0743020867216913 & 3.39992930699002 \\
3 & 0.07593154138757 & 7.2130842903092 \\ 
4 & 0.0816015155371995 & 3.74330006151476 \\
5 & 0.0899940897089563 & 7.28423693259539 \\
6 & 0.098548606433121 & 4.97414996889677 \\ \hline
Нечітке висновування I порядку \\ \hline
Функції належності & RMSE & SMAPE \\ \hline
%2 & 0.088863162670838 & 2.86925982823229 \\
3 & 0.107342623622113 & 2.61031587883906 \\ 
4 & 0.114302161682684 & 2.43845462677015 \\
5 & 0.121556611017793 & 2.21187746150696 \\
6 & 0.129922583600673 & 2.3504927587044 \\ \hline
Нечітке висновування II порядку \\ \hline
Функції належності & RMSE & SMAPE \\ \hline
%2 & 0.0949715863284214 & 3.10116527241627 \\
3 & 0.127570672613453 & 3.21603084937958 \\ 
4 & 0.121809301731276 & 2.49191939755954 \\
5 & 0.134120568445479 & 2.74555892364861 \\
6 & 0.146724724859333 & 2.39716069975922 \\ \hline
Нечітке висновування III порядку \\ \hline
Функції належності & RMSE & SMAPE \\ \hline
%2 & 0.100666657170784 & 3.26456771933306 \\
3 & 0.130173960343431 & 2.12509843555904 \\ 
4 & 0.154580667425542 & 2.17237774290269 \\
5 & 0.139334351692536 & 3.04236918521622 \\
6 & 0.150497843534837 & 2.43818195954604 \\ \hline
\end{tabular}
\end{table}

Для подальшої апробації розширеного нео-фаззі нейрону розглянемо задачу прогнозування хаотичного ряду, що описується диференціальним рівнянням Мекі-Гласса \hl{[147 vik]}:

\begin{equation}
\label{eq:M}
y'\left(t\right)=\frac{0.2\left(t-\tau\right)}{1+y^{10}\left(t-\tau\right)}-0.1y\left(t\right),
\end{equation}
\medskip

при цьому значення часового ряду в кожній точці обчислене за допомогою методу Рунге-Кутта четвертого порядку. Часовий крок прийнятий рівним $0.1$, початкові умови: $x\left(0\right)=1.2$.

Традиційно завдання прогнозування полягає у визначенні $x\left(t+6\right)$x часового ряду (5.2) з параметром затримки 17 по
значенням x (t 18), x (t 12), x (t 6) і x (t).  Перед початком обробки отриманий часовий ряд нормувався таким чином, щоб його значення лежали в інтервалі $\left[0, 1\right]$ (область визначення трикутних функцій належності та кубічних сплайнів, що використовуються у синапсах розширеного нео-фаззі нейрону. 

\medskip
\begin{figure}[H]
\begin{center}
\includegraphics{ENFNMackeyGlassPredictionsIO3M3.png}
\caption{Прогнозування хаотичного часового ряду Мекі-Гласса розширенним нео-фаззі нейроном з 3 трикутними функціями належності, що реалізує нечітке висновування 3-ого порядку (зелена лінія -- шуканий сигнал, синя пунктирна лінія -- сигнал на виході нейрону, червона лінія -- похибка; жовтобагряна вертикальна лінія позначає закінчення тренувальної частини датасету)}
\label{fig:ENFNMackeyGlassPredictionsIO3M3}
\end{center}
\end{figure}

На рис.\ref{fig:ENFNMackeyGlassPredictionsIO3M3} зображений результат прогнозування розширеного нео-фаззі нейрона з трьома функціями належності, що реалізує нечітке висновування 3-ого порядку. Для порівняння на рис.~\ref{fig:ENFNMackeyGlassPredictionsIO0M3} наведено прогнозування традиційного нео-фаззі нейрону (що реалізує нечітке висновування нульового порядку) з аналогічними функціями належності (3 трикутні функції належності).

\begin{table}[H]
\caption{Точність прогнозу ряду Мекі-Глассу розширеним нео-фаззі нейроном від порядку висновування та кількості фунцій належності}
\label{tab:ENFNPredictionAccuracyFromInferenceOrderAndMembershipFunctionsOnMackeyGlassTimeSeries}
\centering \small \begin{tabular}{rcc} \hline
Нечітке висновування 0 порядку \\ \hline
Функції належності & RMSE & SMAPE \\ \hline
%2 & 0.156093623279716 & 0.302950765207994 \\
3 & 0.143482904587951 & 0.325353000527264 \\
4 & 0.106294989490131 & 0.264501060116694 \\
5 & 0.094578207548207 & 0.259630642224594 \\
6 & 0.094578207548207 & 0.259630642224594 \\ \hline
Нечітке висновування I порядку \\ \hline
Функції належності & RMSE & SMAPE \\ \hline
%2 & 0.0337893129825338 & 0.108432487493878 \\
3 & 0.0491962584140483 & 0.136034984261077 \\
4 & 0.0300830061654526 & 0.0978878141219238 \\
5 & 0.0312245619190396 & 0.10776716206662 \\
6 & 0.0277981639612314 & 0.0989376310206592 \\ \hline
Нечітке висновування II порядку \\ \hline
Функції належності & RMSE & SMAPE \\ \hline
%2 & 0.0318577433605903 & 0.0933793821810412 \\
3 & 0.0227629037042365 & 0.0658002874902031 \\
4 & 0.0295377560571133 & 0.108260856276106 \\
5 & 0.0283735498990618 & 0.0946515030316458 \\
6 & 0.0251818186025806 & 0.0847398989365615 \\ \hline
Нечітке висновування III порядку \\ \hline
Функції належності & RMSE & SMAPE \\ \hline
%2 & 0.0435853909382971 & 0.102398663434075 \\
3 & 0.0221516072948077 & 0.0622040928066835 \\
4 & 0.0261638552968437 & 0.0827913036508308 \\
5 & 0.0274818416982828 & 0.114797690969503 \\
6 & 0.0237394982902305 & 0.0846415828789446 \\ \hline
\end{tabular}
\end{table}

На рис.~\ref{fig:ENFNMackeyGlassIO3M3ErrorFromInferenceOrder} показана залежність похибки від порядку висновування розширенних нео-фаззі нейронів з трьома та п'ятьома дзвонуватими функціями належності.

\begin{figure}[H]
\begin{center}
\includegraphics{ENFNMackeyGlassPredictionsIO0M3.png}
\caption{Прогнозування хаотичного часового ряду Мекі-Гласса традиційним нео-фаззі нейроном з 3 трикутними функціями належності (зелена лінія -- шуканий сигнал, синя пунктирна лінія -- сигнал на виході нейрону, червона лінія -- похибка; жовтобагряна вертикальна лінія позначає закінчення тренувальної частини датасету)}
\label{fig:ENFNMackeyGlassPredictionsIO0M3}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics{ENFNMackeyGlassIO3M3ErrorFromInferenceOrder.png}
\caption{Похибка прогнозування хаотичного часового ряду Мекі-Гласса розширенними нео-фаззі нейронами з 3 та 5 дзвонуватими функціями належності від порядку нечіткого висновування}
\label{fig:ENFNMackeyGlassIO3M3ErrorFromInferenceOrder}
\end{center}
\end{figure}

Як видно з таблиці~\ref{tab:ENFNPredictionAccuracyFromInferenceOrderAndMembershipFunctionsOnMackeyGlassTimeSeries} та рис.~\ref{fig:ENFNMackeyGlassIO3M3ErrorFromInferenceOrder}, розширенний нео-фаззі нейрон, що реалізує нечітке висновування вищого від 0 порядку, прогнозує хаотичний часовий ряд за рівнянням Мекі-Гласса з суттєво вищою точністю ніж традиційний нео-фаззі нейрон.

Також пропонований розширений нео-фаззі нейрон було апробовано на реальному часовому ряді <<Споживання електоренергії у м. Сімферополь за 2007 рік>> (фазовий портрет наведено на рис.~\ref{fig:UkrEnergoPhasePortrait}).

\begin{figure}[H]
\begin{center}
\includegraphics{UkrEnergoPhasePortrait.png}
\caption{Фазовий портрет часового ряду <<Споживання електоренергії у м. Сімферополь за 2007 рік>>}
\label{fig:UkrEnergoPhasePortrait}
\end{center}
\end{figure}

Найліпший прогноз (RMSE $\approx 0.14$, SMAPE $\approx 0.21$) надав нейрон, що реалізує нечітке висновування 1-ого порядку з 6-ма функціями належності (рис.~\ref{fig:ENFNUkrEnergoPredictionIO1M6})

Експеременти, що описані у цьому підрозділі, підтвержують, що розширені нео-фаззі нейрони, які реалізують нечітке висновування довільного порядку, мають підвищену точність прогнозування хаотичних рядів (як штучно сгенерованих, так і реальних) порівняно з традиційними нео-фаззі нейронами (які реалізують нечітке висновування нульового порядку).

\begin{figure}[H]
\begin{center}
\includegraphics{ENFNUkrEnergoPredictionIO1M6.png}
\caption{Прогнозування часового ряду <<Споживання електоренергії у м. Сімферополь за 2007 рік>> розширеним нео-фаззі нейроном з 6-ма дзвонувати функціями належності, що реалізує нечітке висновування 1-ого порядку (зелена лінія -- шуканий сигнал, синя пунктирна лінія -- сигнал на виході нейрону, червона лінія -- похибка; жовтобагряна вертикальна лінія позначає закінчення тренувальної частини датасету)}
\label{fig:ENFNUkrEnergoPredictionIO1M6}
\end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Моделювання гiбридної каскадної нейро-фаззi мережі з розширенними нео-фаззі нейронами та оптимiзацiєю пулу нейронiв}
\label{sec:SISOCascadedSystemOnENFNExperiments}

Низку експериментів для апробації гiбридної каскадної нейро-фаззi мережі на розширенних нео-фаззі нейронах з оптимiзацiєю пулу нейронiв було проведено на датасетах, що їх надала дослідницька группа <<The Applications of Machine Learning (AML)>> з Університету Aалто, що у Фінлядії (Aalto University School of Science, Espoo, Finland).
Одним з таких датасетів є часовий ряд <<Споживання електроенергії у Польщі за період з 1990-х років>> (фазовий портрет наведено на рис~\ref{fig:ElectricityDemandPhasePortrait}).
 
\begin{figure}
\begin{center}
\includegraphics[width=4in]{ElectricityDemandPhasePortrait.pdf}
\caption{Фазовий портрет часового ряду Прогнозування часового ряду <<Споживання електроенергії у Польщі за період з 1990-х років>>}
\label{fig:ElectricityDemandPhasePortrait}
\end{center}
\end{figure}

Вихідний сигнал гібридної каскадної нейро-мережі наведено на рис.~\ref{fig:ENFNNet+FuzzyGeneralizerElectricityDemand}, а похибки розширенних нео-фаззі нейронів та нейронів-узагальнювачів для кожного каскаду системи наведно у таблиці~\ref{tab:SimpheropolEnergyConsumptionPredictionAccuracy}.

\begin{figure}
\begin{center}
\includegraphics{ENFNNet+FuzzyGeneralizerElectricityDemand.png}
\caption{Прогнозування часового ряду <<Споживання електоренергії у м. Сімферополь за 2007 рік>> гiбридної каскадної нейро-фаззi мережі з оптимiзацiєю пулу нейронiв (жовтобагряна вертикальна лінія позначає закінчення тренувальної частини датасету)}
\label{fig:ENFNNet+FuzzyGeneralizerElectricityDemand}
\end{center}
\end{figure}

\begin{table}[H]
\caption{Результати прогнозування часового ряду <<Споживання електоренергії у м. Сімферополь за 2007 рік>> нейронів (зокрема нейронів-узагальнювачів) каскадної нейро-фаззі системи}
\label{tab:SimpheropolEnergyConsumptionPredictionAccuracy}
\centering \small \begin{tabular}{lcc}
Каскад I & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.080830 & 0.39288 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.075014 & 0.036680 \\ 
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.074955 & 0.038170 \\
Нейрон-узагальнювач I каскаду & 0.059835 & 0.030302 \\ \hline
Каскад II & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.080835 & 0.039290 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.059837 & 0.036683 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.074966 & 0.038195 \\
Нейрон-узагальнювач II каскаду & 0.059821 & 0.036683 \\ \hline
Каскад III & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.080934 & 0.039333 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.059869 & 0.036711 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.75009 & 0.038213 \\
Нейрон-узагальнювач III каскаду & 0.059869 & 0.030320 \\ \hline
Каскад IV & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.080892 & 0.039316 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.059869 & 0.030303 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.075034 & 0.038213 \\
Нейрон-узагальнювач IV каскаду & 0.059849 & 0.030303 \\ \hline
Нейрон-узагальнювач системи & 0.059821 & 0.030302 \\ \hline
\end{tabular}
\end{table}

Фазовий портрет другого датасету <<Коливання рівню  приловоотливної зони>> (Subtidal coastal level of fluctuations) наведено на рис.~\ref{fig:ENFNNet+FuzzyGeneralizerElectricityDemand}, похибки вузлів системи -- у таблиці~\ref{tab:SubtidalCoastalLevelOfFluctuationsPredictionAccuracy}.

\begin{figure}
\begin{center}
\includegraphics[width=4in]{SubtidalCoastalLevelPhasePortrait.pdf}
\caption{Фазовий портрет часового ряду <<Коливання рівню  приловоотливної зони>>}
\label{fig:SubtidalCoastalLevelPhasePortrait}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics{ENFNNet+FuzzyGeneralizerSubtidalCoastalLevelFlunctuations.pdf}
{Прогнозування часового ряду <<Коливання рівню  приловоотливної зони>> гiбридної каскадної нейро-фаззi мережі з оптимiзацiєю пулу нейронiв (жовтобагряна вертикальна лінія позначає закінчення тренувальної частини датасету)}
\label{fig:ENFNNet+FuzzyGeneralizerSubtidalCoastalLevelFlunctuations}
\end{center}
\end{figure}

\begin{table}[H]
\caption{Результати прогнозування часового ряду <<Коливання рівню приловоотливної зони>> нейронів (зокрема нейронів-узагальнювачів) каскадної нейро-фаззі системи}
\label{tab:SubtidalCoastalLevelOfFluctuationsPredictionAccuracy}
\centering \small \begin{tabular}{lcc}
Каскад I & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.110067 & 0.036612 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.105192 & 0.035474 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.103129 & 0.034814 \\
Нейрон-узагальнювач I каскаду & 0.105598 & 0.035301 \\ \hline
Каскад II & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.110023 & 0.036593 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.105118 & 0.035457 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.103148 & 0.035818 \\
Нейрон-узагальнювач II каскаду & 0.102377 & 0.034698 \\ \hline
Каскад III & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.110013 & 0.036591 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.105126 & 0.035458 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.103155 & 0.034820 \\
Нейрон-узагальнювач III каскаду & 0.103155 & 0.034820 \\ \hline
Каскад IV & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.110026 & 0.036594 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.105153 & 0.035464 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.103168 & 0.034822 \\
Нейрон-узагальнювач IV каскаду & 0.102545 & 0.034083  \\ \hline
Нейрон-узагальнювач системи & 0.102377 & 0.030302 \\ \hline
\end{tabular}
\end{table}

Останній датасет для цієї низки експериментів було взято зі змагань у прогнозуванні часових рядів <<European Symposium on Time Series Prediction 2008>>, фазовий портрет наведено на рис.~\ref{fig:ESTPCompetitionTimeSeriesPhasePortrait}, результат роботи пропонованої системи -- на рис.~\ref{fig:ENFNNetFuzzyGeneralizerESTPCompettionTimeSeries}.

\begin{figure}
\begin{center}
\includegraphics[width=4in]{ESTPCompetitionTimeSeriesPhasePortrait.pdf}
\caption{Фазовий портрет часового ряду <<ESTP Competition Time Series>>}
\label{fig:ESTPCompetitionTimeSeriesPhasePortrait}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics{ENFNNetFuzzyGeneralizerESTPCompettionTimeSeries.pdf}
\caption{Прогнозування часового ряду <<ESTP Competition Time Series>> гiбридної каскадної нейро-фаззi мережі з оптимiзацiєю пулу нейронiв (жовтобагряна вертикальна лінія позначає закінчення тренувальної частини датасету)}
\label{fig:ENFNNetFuzzyGeneralizerESTPCompettionTimeSeries}
\end{center}
\end{figure}

Результати прогнозування нейронів окремих каскадів, нейронів-узагальнювачів, а також системи в цілому можно побачити у таблиці~\ref{tab:ESTPCompetitionTimeSeriesPredictionAccuracy}.

\begin{table}[H]
\caption{Результати прогнозування часового ряду <<ESTP Competition Time Series>> окремих нейронів (зокрема нейронів-узагальнювачів) каскадної нейро-фаззі системи}
\label{tab:ESTPCompetitionTimeSeriesPredictionAccuracy}
\centering \small \begin{tabular}{lcc}
Каскад I & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.175623 & 0.062985 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.149139 & 0.056551 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.146186 & 0.054414 \\
Нейрон-узагальнювач I каскаду  & 0.159002 & 0.055274 \\ \hline
Каскад II & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.175602 & 0.062942 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.149092 & 0.056487 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.146278 & 0.054415 \\
Нейрон-узагальнювач II каскаду  & 0.158996 & 0.055264 \\ \hline
Каскад III & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.175592 & 0.062933  \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.149090 & 0.056487 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.146270 & 0.054415 \\
Нейрон-узагальнювач III каскаду  & 0.158986 & 0.055262 \\ \hline
Каскад IV & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.175605 & 0.062947 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.149127 & 0.056487 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.146323 & 0.054419 \\
Нейрон-узагальнювач IV каскаду  & 0.159015 & 0.055268 \\ \hline
Нейрон-узагальнювач системи & 0.102377 & 0.030302 \\ \hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Моделювання багатовимірної гiбридної каскадної нейро-фаззi мережі, що еволюціонує, з оптимiзацiєю пулу нейронiв}
\label{sec:MIMOCascadedSystemExperiments}

В якості тестового датасету для моделювання багатовимірної гiбридної каскадної нейро-фаззi мережі з оптимiзацiєю пулу нейронiв (що ґрунтується на багатовимірних нео-фаззі нейронах, як описано у \hl{linkchapter}) застосовувався багатовимірний ряд, сгенерований за допомогою диференціальних рівнянь моделі Лоренца:

\begin{equation}
\label{eq:Lorenz}
\begin{cases}
\dot{x}=\sigma\left(y-x\right),\\
\dot{y}=-xz+rx-y,\\
\dot{z}=xy-bz.
\end{cases}
\end{equation}
\medskip

У моделі Лоренца присутні три невідомих функції, а також кілька невідомих параметрів \hl{[21]}.

\begin{figure}[H]
\begin{center}
\includegraphics{MIMOLorenz3D.pdf}
\caption{Прогнозування багатовимірного часового ряду гiбридною каскадною нейро-фаззi мережею з оптимiзацiєю пулу нейронiв}
\label{fig:MIMOLorenz3D}
\end{center}
\end{figure}

При плавній зміні параметра динамічна система змінює тип свого атрактора. Рішення системи рівнянь Лоренца \ref{eq:Lorenz} при значенні параметра $r$, що перевищує біфуркаційних, виглядає майже ідентично випадковому процесу. У певному сенсі, атрактор Лоренца є стохастичними автоколиваннями, зо підтримуються у динамічній системі за рахунок зовнішнього джерела. 

\begin{table}[H]
\caption{Результати прогнозування багатовимірного часового ряду нейронами (зокрема нейронами-узагальнювачами) MIMO гiбридної каскадної нейро-фаззi мережі з оптимiзацiєю пулу нейронiв}
\label{tab:LorenzPredictionAccuracy}
\centering \small \begin{tabular}{lcc}
Каскад I & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.132351 & 0.64333 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.106885 & 0.055176 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.118058 & 0.059517 \\
Нейрон-узагальнювач IV каскаду & 0.106823 & 0.055111 \\ \hline
Каскад II & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.132370 & 0.064339 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.106840 & 0.55165 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.118059 & 0.059517 \\
Нейрон-узагальнювач IV каскаду &0.106840 & 0.55165 \\ \hline
Каскад III & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.132325 & 0.064324 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.106858 & 0.055171 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.118059 & 0.059513 \\
Нейрон-узагальнювач IV каскаду & 0.106858 & 0.055171 \\ \hline
Каскад IV & SMAPE & RMSE \\ \hline
Нейрон I  (3 фунції належності, нечітке висновування III порядку) & 0.132258 & 0.0642449 \\
Нейрон II  (5 фунцій належності, нечітке висновування IV порядку) & 0.106805 & 0.055144 \\
Нейрон III (4 фунції належності, нечітке висновування V порядку) & 0.118015 & 0.059424 \\
Нейрон-узагальнювач IV каскаду & 0.106805 & 0.055144  \\ \hline
Нейрон-узагальнювач системи & 0.106789 & 0.55105  \\ \hline
\end{tabular}
\end{table}

У фазовому просторі дивний атрактор має топологію деякого клубка траєкторій, в межах якого можна виділити дві області. У кожен момент часу рішення знаходиться в одній з цих областей, причому зміна станів системи в одну або іншу область є абсолютно непередбачуваною. 

Атрактор Лоренца демонструє ще одну особливість, притаманну дивним атракторам -- чутливість до початкових умов. Атрактори, тобто нерухомі точки і граничні цикли, характеризуються тим, що для різних початкових умов сімейства рішень сходилися до одного асимптотичному рішення, тобто різні категорії вийшли з різних точок, які відповідають різним початковим умовам, сходилися при $t \rightarrow \infty$ в одну точку або близькі криві. Тому поведінку звичайних систем, що мають атрактори поблизу нерухомих точок і граничних циклів, на великих часових інтервалах можно доволі добре передбачити. З дивними атракторами все зовсім інакше. Які б близькі початкові умови не вибиралися, при $t \rightarrow \infty$ рішення будуть розходитися, віддаляючись одне від одного в фазовому просторі.Оскільки в реальних задачах початкові умови відомі з деякою погрішністю, абсолютно неможливо вказати поведінку такого атрактора при досить великому $t$, тому поведінка систем, що описуються дивними атракторами, є абсолютно непередбачуваною.

Для генерування тестового датасету використовувались такі параметри:

\begin{equation*}
\begin{aligned}
r =& 28,\\
dt =& 0.001;\\
\end{aligned}
\end{equation*}
\medskip

По завершенні експерименту маємо систему з чотирьох (рідше -- трьох) каскадів з трьома багатовимірними нейронами MNFN та одним нейроном-узагальнювачем у кожному каскаді. Результати роботи системи зображені на рис.~\ref{fig:MIMOLorenz3D} та більш детально описані у таблиці~\ref{tab:LorenzPredictionAccuracy}.

\begin{figure}[H]
\begin{center}
\includegraphics{MIMOLorenz.pdf}
\caption{Прогнозування багатовимірного часового ряду гiбридною каскадною нейро-фаззi мережею з оптимiзацiєю пулу нейронiв}
\label{fig:MIMOLorenz}
\end{center}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Моделювання самонавчанної нейро-фаззі системи, що еволюціонує}
\label{sec:SelfLearningCascadedNetworkExperiments}

Одна з основних переваг, притаманних пропонованій самонавчанній нейро-фаззі системі, що еволюціонує, полягає в автоматичному визначенні оптимальної кількості кластерів та значення фаззіфікатору на кожному етапі обробляння даних. Першу серію експериментів було проведено на штучно зсинтезованих наборах даних з різним ступенем розмитості та перекриття класів, аби дослідити вплив значення параметру фаззіфікації на якість кластерування в режимі реального часу відповідно до обраного критерію дійсності.

\begin{figure}
\begin{center}
\includegraphics{clustering01.pdf}
\caption{Штучно сгенеровані набори даних}
\label{fig:clustering01}
\end{center}
\end{figure}

Кожен з наборів даних, що їх наведено на рис.~\ref{fig:clustering01}, містить вісімдесят спостережень з двома ознаками (для очності) у кожному спостереженні. Тестові дані були сгенеровані таким чином, аби у першому наборі класи були чітко розподілені (crisp dataset), у другому наборі кластерні границі були дещо розмиті (fuzzy dataset), у третьому випадку класи сильно перетиналися (extra fuzzy dataset). Логічно припустити, що система, яка тестується, обере менше значення параметру фаззіфікації для першого датасету та більше для останнього, де границі класів спостережень є більш розмитими.

Спостереження надходили до нейро-фаззі мережі у послідовному режимі, вагові коефіцієнти нейронів були проініціалізовані, використовуючи пакетну модифікацію обраного алгоритму кластерування на датасеті з довільних двадцятьох спостережень відповідного набору даних (адже система, як і класичний fuzzy c-means, досить чутлива до параметрів ініціалізації. Локально оптимальні кількість кластерів та значення параметру фаззіфікації обумовлювалися максимальним середнім значенням рекурентних коефіцієнту розбиття PC \eqref{eq:reccurentPartitioningCoefficient} та Ксі-Бені індексу \eqref{eq:recurrentXieBeniIndex}: $\max{\frac{PC_j^{[m]} + 1 - XB_j^{[m]}}{2}}$ (у данному випадку використовувалося від'ємне значення Ксі-Бені індексу $1-XB\left(k\right)$, оскільки чим меншим є $XB_j^{[m]}$, то ліпшим є розбиття даних на кластери). 

Для першого набору даних (crisp dataset), як і передбачалося, оптимальним виявися другий каскад ($m=3$) з трьома кластерами і нейроном-переможцем із найменшим значенням параметру фаззіфікацїї $\beta = 2$ (рис.~\ref{fig:clustering02}). Така конфігурація є оптимальною відповідно до обох використовуваних індексів валідності -- найменше значення Ксі-Бені індексу $XB_j^{[m]}$ та найбільший коефіцієнт розбиття $PC_j^{[m]}$: 

\begin{equation*}
\begin{aligned}
PC_1^{[2]}=&0.9009951,\\
XB_1^{[2]}=&0.03349166.
\end{aligned}
\end{equation*}
\medskip

\begin{table}[t]
\caption{Індекси валідності кластерування I датасету}
\label{tab:ClusteringValidityIndiciesDataSetI}
\centering \small \begin{tabular}{rcccc}
\hline {Каскад 1 ($m=2$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.91758 & 0.7446 & 0.64787 & 0.59236 \\
Індекс Ксі-Бені &
0.052129 & 0.061034 & 0.092235 & 0.1294 \\
%——————————————————————————————————————————% 
\hline {Каскад 2 ($m=3$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.92643 & 0.6609 & 0.50214 & 0.43305 \\
Індекс Ксі-Бені &
0.027232 & 0.06872 & 0.17281 & 0.26914 \\
%——————————————————————————————————————————% 
\hline {Каскад 3 ($m=4$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.87218 & 0.5256 & 0.37605 & 0.31993 \\
Індекс Ксі-Бені &
0.15687 & 0.4153 & 0.84699 & 1.1765 \\
%——————————————————————————————————————————% 
\hline {Каскад 4 ($m=5$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.73909 & 0.45445 & 0.32428 & 0.27063 \\
Індекс Ксі-Бені &
0.12985 & 0.30637 & 0.68584 & 1.0551 \\
\hline
\end{tabular}
\end{table}

Лише одне спостереження у цьому датасеті (його позначено багряним квадратом) не належить жодному кластерові зі ступенем більшим від $0.6$. Індекси валідності нейронів системи наведені у таблиці~\ref{tab:ClusteringValidityIndiciesDataSetI}.

Для набору даних з середньою вираженістю класів найліпшим виявився нейрон другого каскаду ($m=3$) і фаззіфікатором $\beta=3$ (таблиця 5.2). 

Як показано на рис.~\ref{fig:clustering03}, декілька спостережень у центрі (позначені багряними квадратами) можна віднести до 2 кластерів з відносно високим ступенем належності, проте більшість спостережнь можна чітко розкластеризувати, що ілюструється високим значенням коефіцієнту розбиття, та дуже низьким Ксі-Бені індексом:

\begin{equation*}
\begin{aligned}
PC_2^{[2]}=&0.9727868,\\
XB_2^{[2]}=&0.087474.
\end{aligned}
\end{equation*}
\medskip

\begin{figure}
\begin{center}
\includegraphics{clustering03.pdf}
\caption{Набір даних з нечіткими межами класів (fuzzy dataset)}
\label{fig:clustering03}
\end{center}
\end{figure}

Для набору з найменш чіткими межами класів (таблиця 5.3), система обрала нейроном-переможцем вузол третього каскаду ($m=4$) з високим параметром фаззіфікації $\beta = 4$:

\begin{equation*}
\begin{aligned}
PC_3^{[3]}=&0.335525,\\
XB_3^{[3]}=&0.2128333.
\end{aligned}
\end{equation*}
\medskip

\begin{figure}[H]
\begin{center}
\includegraphics{clustering002.pdf}
\caption{Набір даних з чітко вираженими класами (Crisp dataset)}
\label{fig:clustering02}
\end{center}
\end{figure}

\begin{table}[H]
\caption{Індекси валідності (датасет 2)}
\label{tab:ClusteringValidityIndiciesDataSetII}
\centering \small \begin{tabular}{rcccc}
\hline {Каскад 1 ($m=2$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.78414 & 0.58928 & 0.53853 & 0.52239 \\
Індекс Ксі-Бені &
0.16668 & 0.30834 & 0.3745 & 0.38723 \\
%——————————————————————————————————————————% 
\hline {Каскад 2 ($m=3$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
50084 & 0.71164 & 0.97275 & 0.4191 \\
Індекс Ксі-Бені &
0.009751 & 0.031235 & 0.087474 & 0.1323 \\
%——————————————————————————————————————————% 
\hline {Каскад 3 ($m=4$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.91888 & 0.47532 & 0.32777 & 0.28912 \\
Індекс Ксі-Бені &
0.052563 & 0.1757 & 0.27516 & 0.33766 \\
%——————————————————————————————————————————% 
\hline {Каскад 4 ($m=5$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.85618 & 0.34327 & 0.24778 & 0.22445 \\
Індекс Ксі-Бені &
0.048316 & 0.19887 & 0.34307 & 0.41228 \\
%——————————————————————————————————————————% 
\hline {Каскад 5 ($m=6$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.81295 & 0.30709 & 0.21636 & 0.19214 \\
Індекс Ксі-Бені &
0.060896 & 0.19702 & 0.31393 & 0.38668 \\
\hline
\end{tabular}
\end{table}

На рис.~\ref{fig:clustering04} спостереження, для яких ступінь належності до будь-якого кластеру не перевищує $0.6$, позначені багряними квадратами. Як і очікувалося,  для цього набору даних кількість таких спостережень значно вища від попередніх датасетів з більш компактними та <<чіткими>> класами.

\begin{table}
\centering \small \begin{tabular}{rcccc}
\hline {Каскад 1 ($m=2$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.85094 & 0.71415 & 0.61734 & 0.57085 \\
Індекс Ксі-Бені &
0.10584 & 0.11462 & 0.13797 & 0.16101 \\
%——————————————————————————————————————————% 
\hline {Каскад 2 ($m=3$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.61668 & 0.42848 & 0.37779 & 0.35884 \\
Індекс Ксі-Бені &
0.1754 & 0.20364 & 0.22364 & 0.23995 \\
%——————————————————————————————————————————% 
\hline {Каскад 3 ($m=4$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.33458 & 0.44082 & 0.79405 & 0.29615 \\
Індекс Ксі-Бені &
0.20989 & 0.129 & 0.051039 & 0.26282 \\
%——————————————————————————————————————————% 
\hline {Каскад 4 ($m=5$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.50244 & 0.33067 & 0.26029 & 0.23318 \\
Індекс Ксі-Бені &
0.37268 & 0.61417 & 0.79695 & 0.93626 \\
%——————————————————————————————————————————% 
\hline {Каскад 5 ($m=6$)} & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
Коефіцієнт розбиття &
0.53279 & 0.29731 & 0.22648 & 0.19858 \\
Індекс Ксі-Бені &
0.27407 & 0.47298 & 0.60569 & 0.70716 \\
\hline
\end{tabular}
\caption{Індекси валідності (датасет 3)}
\end{table}

Для очності у всіх наведених рисунках кольором позначені не тільки розкластеровані спостереження і центри кластерів, а й задній план (фон) малюнків, що дозволяє візуально визначити, до якого кластеру система віднесла б нові спостереження. Не дивно, що, тоді як для перших двох датасетів важко визначити домінуючий колір, оскільки кластери їх спостережень більш менш компактні та явно виражені, для останнього набору даних домінуючий колір -- сірий, сформований кольорами усіх кластерів, що ілюструє великий ступінь перекриття класів і, відповідно, високе значення оптимального параметру фаззіфікації $\beta$, що обрала система. 

\begin{figure}
\begin{center}
\includegraphics{clustering04.pdf}
\caption{Набір даних з класами, що перетинаються (extra fuzzy dataset)}
\label{fig:clustering04}
\end{center}
\end{figure}

Ця низка експериментів проілюструвала як важливо вірно визначати параметр фаззіфікації, оптимальне значення якого у випадку обробляння даних у послідовному режимі з високою вірогідністю змінюється у часі, а саме здатність визначати оптимальне значення цього параметру в онлайн режимі є відмінною особливістю попропонованої самонавчанної нейро-системи.
%\setFloatBlockFor{sssec:SelfLearningNetworkArtificialGeneratedExperiments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Придумати назву2}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\label{sssec:SelfLearningNetworkIris}%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Наступна низка експериментів була проведена на наборі даних <<Іриси Фішера>> (Fisher's Iris data set). 

\begin{figure}[th]
\begin{center}
\includegraphics{HierarchialClusteringOfIrisDataset.pdf}
\caption{Ієрархічне класерування датасету <<Іриси Фішера>>}
\label{fig:HierarchialClusteringOfIrisDataset}
\end{center}
\end{figure}

Це багатовимірний датасет для задачі класифікації, на прикладі якого англійський статистик та біолог Рональд Фішер в 1936 році продемонстрував роботу розробленого ним методу дискримінантного аналізу. Іноді його також називають <<Ірисами Андерсона>> (через те, що дані були зібрані американським ботаніком Едгаром Андерсоном). Цей набір даних став класичним і часто використовується в літературі для ілюстрації роботи різних статистичних алгоритмів.

\begin{figure}
\begin{center}
\includegraphics{ClusteredIrisDatasetM3Beta2.pdf}
\caption{Розкластерований датасет <<Іриси Фішера>> при $m=3$, $\beta = 2$ (Точність кластерування -- 96\%)}
\label{fig:ClusteredIrisDatasetM3Beta2}
\end{center}
\end{figure}

Проте цей датасет рідко використовується у кластерному аналізі, адже межі класів <<Verginica>> та <<Versicolor>> не можна чітко визначити, ґрунтуючись на даних, що їх використовував Фішер (що легко продемонструвати за допомогою ієрархічного кластерування, рис.~\ref{fig:HierarchialClusteringOfIrisDataset}). Саме цим і цікавий для нас цей набір даних: коли класичні методи чіткого кластерного аналізу не справляються з задачею, може стати у нагоді система, що реалізує нечітке кластерування зі змінним параметром фаззіфікації та кількістю кластерів. Для більшості методів кластерного аналізу, зокрема для методу нечітких середніх (fuzzy c-means), необхідно заздалегідь задати кількість кластерів, і очевидним рішенням є прийняти $m=3$, адже маємо три класи: Iris Verginica, Iris Versicolor та Iris Setosa (рис.~\ref{fig:ClusteredIrisDatasetM3Beta2}). 

\begin{table}
\centering \small\begin{tabular}{rcccc}
& $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.8313073 & 0.8741245 & 0.8709475 & 0.8888124 \\
min & 0.7859722 & 0.7533766 & 0.6615745 & 0.7656498 \\
max & 0.8534013 & 0.9166667 & 0.935051 & 0.9604701 \\
\end{tabular}
\caption{Точність кластерування при $m=3$}
\end{table}

Точність кластерування за допомогою методу нечітких середніх за таких умов ($m=3$, $\beta=2$) рідко перевищує $83\%$ (таблиця 5.4). (Оскільки для обраного датасету існують мітки з вірною класифікацією, ефективність кластеризації вимірювалася у відсотках точності щодо еталонного розбиття після дефаззіфікації.) Проте, якщо не обмежувати пропоновану систему у кількості кластерів (система ініціалізується інтервалом допустимих значень $m$ (кількість кластерів) та параметру фаззіфікації $\beta$), вельми цікавими є результати кластерування нейронів кожного з каскадів.  

У таблицяі 5.5 наведена точність розбиття даних, коли $m \gg 3$ кластерів відповідно. Варто зазначити, що нейрони у пулі кожного каскаду реалізують метод нечітких середніх зі змінним значення фаззіфікатору, а отже є чутливими до довільно ініціалізовних центрів кластерів, тому у таблицях наведені середня, мінімальна та максимальна точності кластерування (після дефаззіфікації).

\begin{table}[H]
\caption{Точність кластерування для $m \in [7,13]$, $\beta \in [2,5]$}
\centering \small\begin{tabular}{rcccc}
%\hline 
%$m=6$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
%avg & 0.8832960 & 0.9186286 & 0.9044973 & 0.9163232 \\
%min & 0.8209877 & 0.8521505 & 0.8490079 & 0.8458041 \\
%max & 0.9489689 & 0.9679570 & 0.9605802 & 0.9790494 \\
\hline 
$m=7$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.8972948 & 0.9150268 & 0.9242503 & 0.9178207 \\
min & 0.8536056 & 0.8461905 & 0.8723182 & 0.8600289 \\
max & 0.9621849 & 0.9736172 & 0.9810146 & 0.9663462 \\
\hline 
$m=8$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9065560 & 0.9296311 & 0.9243606 & 0.9248976 \\
min & 0.8217056 & 0.8562179 & 0.8577202 & 0.8590278 \\
max & 0.9474588 & 0.9789402 & 0.9848214 & 0.9747899 \\
\hline 
$m=9$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9258154 & 0.9282887 & 0.9308971 & 0.9229753 \\
min & 0.8689921 & 0.8270525 & 0.8684641 & 0.8556390 \\
max & 0.9849170 & 0.9806397 & 0.9664112 & 0.9748284 \\
\hline 
$m=10$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9213191 & 0.9285106 & 0.9332528 & 0.9282907 \\
min & 0.8663370 & 0.8722271 & 0.8652272 & 0.8766667 \\
max & 0.9663420 & 0.9838095 & 0.9723656 & 0.9756335 \\
\hline 
$m=11$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9295315 & 0.9408977 & 0.9317242 & 0.9295800 \\
min & 0.8520268 & 0.8964924 & 0.8890781 & 0.8788656 \\
max & 0.9716166 & 0.9848485 & 0.9704892 & 0.9798627 \\
\hline 
$m=12$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9349407 & 0.9433244 & 0.9337934 & 0.9306632 \\
min & 0.8815133 & 0.8949802 & 0.8798160 & 0.8486111 \\
max & 0.9795274 & 0.9783497 & 0.9630952 & 0.9772727 \\
\hline 
$m=13$ & $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9420998 & 0.9398127 & 0.9375204 & 0.9357708 \\ 
min & 0.8823175 & 0.8614025 & 0.8882479 & 0.8828348 \\
max & 0.9807518 & 0.9788034 & 0.9753452 & 0.9748873 \\
\end{tabular}
\end{table}

На рис~\ref{fig:IrisClusteringEfficiencyFromNumberOfClustersAndFuzzyfier} зображено залежність точності кластерування від кількості кластерів. Цікаво, що при, здавалося б, очевидному рішенні обрати кількість кластерів рівною трьом, отримуємо чи не найгіршу точність кластерування (при $\beta = 2$) після дефаззіфікації щодо еталонного розбиття (Для порівняння на рис.~\ref{fig:ClusteredIrisDatasetM7Beta5} та рис.~\ref{fig:ClusteredIrisDatasetM14Beta4} наведені розбиття, що їх запропонували нейрони-переможці деяких каскадів, де $m \gg 3$).

\begin{figure}
\begin{center}
\includegraphics[width=5.5in]{IrisClusteringEfficiencyFromNumberOfClustersAndFuzzyfier.pdf}
\caption{Точність кластерування від кількості кластерів та параметру фаззіфіказії}
\label{fig:IrisClusteringEfficiencyFromNumberOfClustersAndFuzzyfier}
\end{center}
\end{figure}

Цьому легко знайти пояснення, адже метод нечітких $k$-середніх (а саме цей метод у цьому експерименті реалізовують вузли пулів кожного каскаду) добре розпізнає кластери лише гіперсферичної форми. Проте кластер довільної (негіперсферичної) форми, можна розбити на декілька гіперсферичних підкластерів, що й відбувається у каскадах, де $m > 3$, що пропонують розбиття на дрібні кластери. На рисунках \ref{fig:3DClusteredIrisDatasetM7Beta5} та \ref{fig:3DClusteredIrisDatasetM12Beta4} наведені розбиття деяких каскадів, де кількість кластерів більша від кількості класів еталонної вибірки; тут можна побачити, що декілька кластерів, що після дефаззіфікації будуть віднесені до одного класу, наприклад, Iris Virginica, розташовані поруч один з одним, тобто є складовими більшого кластеру негіперсферичної форми.

\begin{figure}
\begin{center}
\includegraphics{ClusteredIrisDatasetM7Beta5.pdf}
\caption{Розкластерований датасет <<Іриси Фішера>> при $m=7$, $\beta=5$ (Точність кластерування $\approx 93\%$)}
\label{fig:ClusteredIrisDatasetM7Beta5}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics{ClusteredIrisDatasetM12Beta4.pdf}
\caption{Розкластерований датасет <<Іриси Фішера>> при $m=12$, $\beta=4$ (Точність кластерування $\approx 96\%$)}
\label{fig:ClusteredIrisDatasetM14Beta4}
\end{center}
\end{figure}

\begin{table}
\caption{Точність кластерування при $m=14$} \label{tab:ClusteringAccuracyM=14}
\centering \small\begin{tabular}{rcccc}
& $\beta=2$ & $\beta=3$ & $\beta=4$ & $\beta=5$ \\ \hline
avg & 0.9369168 & 0.9448829 & 0.9383179 & 0.9403416 \\
min & 0.8847819 & 0.8953380 & 0.8787879 & 0.9069805 \\
max & 0.9731262 & 0.9754579 & 0.9918301 & 0.9762515 \\
\end{tabular}

\end{table}

Таким чином, видається доречним навіть у випадку, коли відоме еталонне розбиття датасету, дозволити системі обрати кінцеву кількість кластерів самостійно, а особливо, коли вузли системи реалізують однаковий метод кластерування. 

\begin{figure}[H]
\begin{center}
\includegraphics{3DClusteredIrisDatasetM7Beta5.pdf}
\caption{Розкластерований датасет <<Іриси Фішера>> при $m=7$, $\beta=5$ (Точність кластерування $\approx 93\%$)}
\label{fig:3DClusteredIrisDatasetM7Beta5}
\end{center}
\end{figure}

Варто зауважити, що у цьому випадку для визначення локально оптимального розбиття доцільно використовувати модифіковані індекси валідності, чи такі, що не залежать від відстані цетрів кластерів, наприклад ті, що ґрунтуються на щільності (density-based).

\begin{figure}[H]
\begin{center}
\includegraphics{3DClusteredIrisDatasetM12Beta4.pdf}
\caption{Розкластерований датасет <<Іриси Фішера>> при $m=12$, $\beta=4$ (Точність кластерування $\approx 96\%$)}
\label{fig:3DClusteredIrisDatasetM12Beta4}
\end{center}
\end{figure}

Наступну серію експериментів було проведено на датасеті <<Знання студентів про електричні машини постійного струму>>.

\begin{figure}[H]
\begin{center}
\includegraphics{StudentKnowledgeDataSet.pdf}
\caption{Датасет <<Знання студентів про електричні машини постійного струму>>}
\label{fig:StudentKnowledgeDataSet}
\end{center}
\end{figure}

Цей датасет було додано до UCI репозиторію у 2013 році, він містить 403 патерни, кожен з п'ятьма атрибутами:

\begin{enumerate}
\item STG: кількість часу, що його витратив(витратила) студент(ка) на вивчання цільового матеріалу,
\item SCG: Кількість повторюваннь вивчання цільового матеріалу студентом(студенткою),
\item STR: Кількість часу, що його використав(використала) студент(ка) на вивчання матеріалу, пов'язаного з цільовим матеріалом,
\item LPR: Оцінка, що її отримав(отримала) студент(ка) на іспиті з предмету, пов'язаного з цільовим предметом,
\item PEG: Оцінка, що її отримав(отримала) студент(ка) на іспиті з цільового предмету,
\end{enumerate}

\begin{figure}[H]
\begin{center}
\includegraphics{3DStudentKnowledgeDataSet.pdf}
\caption{Датасет <<Знання студентів про електричні машини постійного струму>>}
\label{fig:3DStudentKnowledgeDataSet}
\end{center}
\end{figure}

Попарні графіки атрибутів наведено на рис.~\ref{fig:StudentKnowledgeDataSet} та у тримірному просторі на рис.~\ref{fig:3DStudentKnowledgeDataSet}.

\begin{figure}
\begin{center}
\includegraphics{3DClusterizedStudentKnowledgeDataSetM4Beta2.pdf}
\caption{Датасет <<Знання студентів про електричні машини постійного струму>>}
\label{fig:3DClusterizedStudentKnowledgeDataSetM4Beta2}
\end{center}
\end{figure}

Для цього експерименту нейрони-узагальнювачі керувалися рекурентним Ксі-Бені Індексом при визначанні локально-оптимального нейрона (з найлішпшим параметром фаззіфікації) та каскаду (з оптимальною кількістю кластерів).

\begin{figure}[H]
\begin{center}
\includegraphics{ClusterizedStudentKnowledgeDataSetM4Beta2.pdf}
\caption{Датасет <<Знання студентів про електричні машини постійного струму>>}
\label{fig:ClusterizedStudentKnowledgeDataSetM4Beta2}
\end{center}
\end{figure}

Оптимальне розбиття, що його наведено на рис.\ref{fig:3DClusterizedStudentKnowledgeDataSetM4Beta2} та на рис.\ref{fig:ClusterizedStudentKnowledgeDataSetM4Beta2}, надав другий нейрон третього каскаду з $m=4$, $\beta=2$ з коефіцієнтом Ксі-Бені $0.38155$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Розв’язування практичних задач за допомогою розробленої самонавчанної гібридної каскадної системи, що еволюціонує}

Проблема здорового харчування - одна з найактуальніших у наші дні. Повноцінне харчування передбачає споживання достатньої кількості білків, жирів, вуглеводів, вітамінів, макро- і мікроелементів для нормального функціонування організму в цілому. Багато хвороб шлунково-кишкового тракту «молодіють» - це гастрити, виразкова хвороба шлунка і різні порушення обміну речовин. Фізичне здоров'я, стан імунітету, довголіття, психічна гармонія - все це безпосередньо пов'язано з проблемою здорового харчування людини. Для студентів проблема харчування стоїть особливо гостро, в зв'язку з браком часу у них немає можливості дотримуватися правильного режиму прийому їжі. Також для студентів характерний в основному сидячий спосіб життя - гіподинамія. У поєднанні із незбалансованим раціоном харчування це згубно впливає на організм і його стан. 
Звісно, вирішення проблеми здорового харчування потребує комплексного підходу, проте інформованість - невід'ємна складова правильного підбору раціону здорового харчування. Сьогодні нескладно знайти інформацію щодо рекомендованої денної кількості калорій, білків, жирів та вуглеводів, проте важко дати оцінку конкретому прийому їжі, наприклад, придбаному у їдальні, де немає етикеток з такою інформацією. Мобільний додаток <<Spoon app>> може стати у нагоді, коли користувач прагне бути проінформованим щодо поживності конкретної страви, користуючись аналізом світлини тарілки з їжею. Вхідними даними мобільного додатку є світлина, що її користувач має зробити таким чином, аби тарілка знаходилася у центрі, а також тип тарілки (звичайна, глибока, дуже глибока) аби на виході додаток мав обґрунтовану кількість калорій та поживність порції, що була зображена на світлині. 

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{SpoonAppClassification.png}}
\caption{Другий етап аналізу світлини зі стравою мобільним додатком Spoon App (навчаняя з підкріпленням)}
\label{fig:SpoonAppClassification}
\end{center}
\end{figure}

Аналіз світлини можна розбити на декілька етапів: 
\begin{enumerate}
\item кластерування даних зображених на світлині (відбувається на стороні клієнту)
\item ідентифікація окремих складових страви: класифікація кожного зображення, після розбиття світлини на кластери на першому етапі (відбувається на серверній стороні), визначення типу продукту за допомогою бази даних, що знаходиться на сервері, та подальше визначання кількості калорій, співвідношення білкі, жирів та вуглеводів.
\end{enumerate}

\begin{figure}[H]
\begin{center}
\fbox{\includegraphics{SpoonAppClustering.png}}
\caption{Перший етап аналізу світлини зі стравою мобільним додатком Spoon App (кластерування за умови невизначенності щодо кількості кластерів)}
\label{fig:SpoonAppClustering}
\end{center}
\end{figure}

Другий етап у певному сенсі є навчанням з підкріпленням, адже користувачеві пропонується підтвердити чи скорегувати кінцевий результат класифікації, як зображено на рис.~\ref{fig:SpoonAppClassification}. Проте більш цікавим нам видається саме перший етап аналізу світлини, і пропонована гібридна самонавчана система використовується саме на цьому етапі, адже вона краще від інших існуючих систем задовольняє умовам, що їх було висунуто на етапі формування технічних вимог до програмного забеспечення (Software Requirements Specifications):

\begin{itemize}
\item кластерування має проходити за умови невизначенності щодо кількості кластерів,
\item оскільки кластерування відбувається на стороні кліенту, важливо мінімізувати обчислювальну складність алгоритму, а отже перевага надається методам послідовного кластерування.
\end{itemize}

По завершенні аналізу на першому етапі мобільний додаток попронує розбиття світлини на $m$ кластерів, як показано на рис.~\ref{fig:SpoonAppClustering}. Варто зауважити, що межі кластерів, що їх визначила самонавчана система, дещо відрізняються від тих, що зображені на рис.~\ref{fig:SpoonAppClustering}, для того, щоб користувачеві було зручніше візуально сприймати розбиття світлини на кластери, пунктирні лініїї, що зображуть межі кластерів, на декілька міліметрів віддалені від меж дійсних кластерів, проте на сервер для подальшої класифікації відправляється світлина з розбиттям, що запропонувала система. На цьому етапі користувач може скорегувати розбиття, перетягнувши пунктирну лінію меж кластерів, чи зовсім видалити пропонований кластер. Хоча навчання з підкріпленням у прямому сенсі, відбувається лише на етапі класифікації (база даних на сервері оновлюється, коли користувач корегує результат класифікації), а на цьому етапі маємо саме навчання без учителя, це все ж таки дає змогу у певному сенсі дати оцінку кластеруванню системи: вважаємо кластерування успішним, якщо користувач не робив жодних змін до пропонованого розбиття, та неуспішним, коли розбиття було скореговане.
Після бета тестування мобільного додатку маємо наступні результати:

\begin{table}[t]
\centering  \begin{tabular}{r | c}
кількість та межі кластерів залишилися незмінними & 608 \\ \hline
межі кластеров було дещо змінено користувачем, \\проте кількість кластерів залишилась незмінною & 61 \\ \hline
користувач змінив кількість кластерів \\ та межі пропонованих кластерів & 52 \\ \hline 
користувач видалив кластер(и), межі інших пропонованих \\ кластерів лишилися незмінні & 29
\end{tabular}
\caption{Результати бета тестування першого етапу (кластерування за умови невизначенності щодо кількості кластерів) аналізу світлини мобільним додатком <<Spoon App>>}
\end{table}

Цікавим видається перебіг подій, коли користувач видалив один кластер, проте залишив межі інших кластерів незмінними, у такому випадку, як зазначалося вище, вважається що кластерування не було успішним. Проте подальший аналіз показав, що у 90\% таких випадків користувач залишив на тарілці неїстівний предмет (виделку, ложку тощо), і хоча система вірно відвела йому окремий кластер, не має сенсу класифікувати його та визначати калорійність цього предмету, тому логічно, що користувач видалив його. У 10\% випадків, як показав аналіз світлин з початковим кластеруванням, що запропонувала самонавчана система, та світлин після корегування користувачем, користувач свідомо видаляв складову страви, зазвичай найменш корисну (тістечко, шоколад тощо). Тому видається доцільним ці 4\% світлин також віднести до таких, що були вірно розкластеровані системою (яка, проте, не бере до уваги людський фактор).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Розв’язування практичних задач за допомогою розробленої гібридної каскадної нейро-фаззі мережі, що еволюціонує, з оптимізацією пулу нейронів}

Пропоновану гібридну каскадну нейро-фаззі мережу, що еволюціонує, було використано для вирішення задачі прогнозування витрат нормогодин на ремонтні роботи візків вагонів типу 61-425, 61-181, 47Д та 47К у ТОВ <<Харківський вагонобудівний завод>>. 
Підприємствам машинобудівного сектору економіки притаманне глибоке використання виробничої програми в якості основного інструменту планування. Коректне формування й оптимізація виробничої програми є досить нелегким процесом, але дуже важливим. Незаплановані втрати фінансових або інших ресурсів можуть не тільки мати серйозний негативний ефект на стан підприємства, а й у сучасних економічних умовах поставити під загрозу саме його існування. Все це робить оптимізацію виробничої програми першочерговим завданням планового відділу підприємства. 
Галузь вагонобудування України в цілому знаходиться не в найкращому стані останні роки. Серед основних негативних факторів, які призвели до цього, слід виділити наступні:
\begin{itemize}
\item структурна та фінансова криза підприємств Укрзалізниці,
\item скорочення попиту на вагони, через свуження або закриття традиційних ринків збуту,
\item непрозорість та неринковість більшості державних тендерів у сфері залізниці приводить до дуже високого порогу вхождення для відносоно малих вагонобудівних підприємств.
\end{itemize}
В таких умовах навіть найбільш оптимальна виробнича програма не гарантує підприємству виживання, особливо це стосується відносно невеликих підприємств. Саме до таких підприємств і відноситься ТОВ <<Харківський вагонобудівний завод>>. Через малий попит на нові вагони завод був вимушений сконцетруватися на ремонті та обслуговуванні візків та колісних пар. Характерними відзнаками таких ремонтних робіт є те, що горизонт їх планування менший, ніж при роботах із вагонами в цілому, та їх нерегулярність. Це й призводить до того, що керівництво підприємства має потребу в інструментарії, який дозволяє оперативно сформувати та внести корективи в поточний виробничий план без масштабних змін у річному плані виробництва. Ці корективи мають бути максимально наближені до реальності. Якщо виділити недостатньо часу на нові поточні задачі, то доведеться вносити зміни в річний план виробництва, що може призвести до зриву виконання масштабних робіт. Якщо ж корективи формуються з використанням максимальних виробничих нормативів, то оперативні задачі затягуються і підприємство може втратити шанс на нові аналогічні замовлення, що в умовах низького попиту на вагоноремонтні послуги може призвести до краху. Саме тому було поставлено задачу прогнозування найбільш реальних термінів виконання оперативних завдань по ремонту візків.

У таблиці~\ref{tab:TrolleyCarRepairs} наведені типи та послідовність ремонтних робіт для візків вагонів типу 61-425, 61-181, 47Д та 47К. Для формування вектору вхідних даних були залучені експерти, що надали оцінку складності тому чи іншому типу ремонтних робіт, якщо це необхідно. Вочевидь, складність та тривалість деякіх робіт не залежить від стану візку на момент початку ремонтного циклу, наприклад, виконання <<Сушки атмосферної відремонтованих деталей і складових одиниць візку 0.162-04.20.00: 000 (1) після грунтування>> або <<Випробування гасителів коливання 45.30.045>> займає приблизно однакову кількість нормогодин, а отже не потребує експертної оцінки (якщо такі типи робіт необхідні, оцінка їх складності завжди дорівнює одиниці). Проте витрата нормогодин на більшість типів ремонтих робіт (наприклад, на <<Ремонт деталей підвісок 30.30.025>> чи <<Ремонт важелiв гальмiвної важiльної передачi>>) суттєво варіюється в залежності від характеру та ступеню отриманих пошкоджень і потребує експертної оцінки (оцінка надається в інтервалі $(0, 1]$, де 1 - найвища складність роботи).

Для вирішення поставленого завдання було обрано саме пропоновану гідбридну нейронну мережу, адже важливим аспектом є можливість системи працювати з датасетом, де кількість патернів нижча від кількості вхідних параметрів. Пропоновану гібридну нейро-фаззі мережу було навчано на тренувальному датасеті з 62-х патернів, кожен з яких містить 69 атрибутів та, на момент впровадження, перевірено на тестовому датасеті, що містив 20 патернів. Система продемонструвала високу точність прогнозу: RMSE = 0.08506766, SMAPE = 0.2545574, абсолютна похибка не перевищує 27 нормогодин (при середній витраті у 1124 нормогодин для повного циклу ремонтих робіт та 722 нормогодин для часткового).

\begin{longtable}{|p{2.2cm}|p{7cm}|p{2.2cm}|p{2.2cm}|}
\caption{Типи ремонтних робіт для візків вагонів 61-425, 61-181, 47Д та 47К}\\\hline
\label{tab:TrolleyCarRepairs}
Код\newlineроботи & Назва роботи & Попередні роботи & Подальші роботи\\\hline
\endfirsthead
\hline
Код\newlineроботи &   Назва роботи &   Попередні роботи &   Подальші роботи	\\\hline
\endhead %
109.01.001 & 	Розбирання візків 0.162-04.20.00~: 000~(1) &  	&  	109.05.046 103.01.002	\\\hline
109.01.002 & 	Розбирання візків 0.114-04.10.00~: 000~(1) & 	&	109.05.047 103.01.002 	\\\hline
109.05.046 &	Попередня дефектація вузлів візку 0.162-04.20.00~: 000~(1) & 	 109.01.001 &  	109.05.047 109.05.049 109.01.004	\\\hline
109.05.047 & 	Попередня дефектація вузлів візку 0.114-04.10.00~: 000~(1) & 	109.05.046 109.01.002 &  	109.05.050 109.01.004	\\\hline
109.05.049 & 	Абразивне очищення деталей і вузлів візку 0.162-04.20.00~: 000~(1) & 	109.05.046 &  	109.01.022 109.01.035 109.01.036 109.01.003 	\\\hline
109.05.050 & 	Абразивне очищення деталей і вузлів візку 0.114-04.10.00~: 000~(1) & 	109.05.047 &  	109.01.022 109.01.035 109.01.036 109.01.003 	\\\hline
109.01.003 & 	Підготовка шпінтонов для ремонту &  	109.05.049 109.05.050 &  	109.05.051 	\\\hline
109.05.051 & 	Pемонт шпінтонов &  	109.01.003 &  	109.01.031 109.01.032	\\\hline
109.01.004 & 	Виготовлення комплекту деталей для ремонту візків &  	109.05.046 109.05.047 109.05.048 &  	109.01.005 109.01.006	\\\hline
109.01.005 & 	Фарбування виготовлених деталей візку 0.162-04.20.00~: 000~(1) &  	109.01.004 &  	109.05.052	\\\hline
109.05.052 & 	Сушка атмосферна виготовлених деталей візку 0.162-04.20.00~: 000~(1) &  	109.01.005 &  	109.01.044 109.01.007 109.01.009 	\\\hline
109.01.006 & 	Фарбування виготовлених деталей візку 0.114-04.10.00~: 000~(1) &  	109.01.004 &  	109.05.053	\\\hline
109.05.053 & 	Сушка атмосферна виготовлених деталей візку 0.114-04.10.00~: 000~(1) &  	109.01.006 &  	109.01.044 109.01.007 109.01.009 109.01.047 	\\\hline
109.01.007 & 	Ремонт рами візку 0.162-04.20.00~: 000~(1) &  	109.05.052 109.05.053 109.05.048 &  	109.01.008	\\\hline
109.01.008 & 	Грунтування рами візку 0.162-04.20.00~: 000~(1) &  	109.01.007 &  	109.05.054	\\\hline
109.05.054 & 	Сушка атмосферна рами візку 0.162-04.20.00~: 000~(1) &  	109.01.008 &  	109.01.029 109.01.010	\\\hline
109.01.009 & 	Ремонт рами візку 0.114-04.10.00~: 000~(1) &  	109.05.052 109.05.053 109.05.048 &  	109.01.010	\\\hline
109.01.010 & 	Грунтування рами візку 0.114-04.10.00~: 000~(1) &  	109.01.009 &  	109.05.055	\\\hline
109.05.055 & 	Сушка атмосферна рами візку 0.114-10.00~: 000~(1) після грунтування &  	109.01.010 &  	109.01.030 109.01.032	\\\hline
109.01.044 & 	Ремонт надресорного брусу 3751-Н~(1) &  	109.05.052 109.05.053 109.05.048 &  	109.01.045	\\\hline
109.01.045 & 	Грунтування надресорного брусу 3751-Н~(1)  &  	109.01.044 &  	109.05.056	\\\hline
109.05.056 & 	Сушка атмосферна надресорного брусу 3751-Н~(1) після грунтування &  	109.01.045 &  	109.01.046	\\\hline
109.01.046 & 	Фарбування надресорного брусу 3751-Н~(1) &  	109.05.056 &  	109.05.057	\\\hline
109.05.057 & 	Фарбування надресорного брусу 3751-Н~(1) &  	109.01.046 &  	109.01.040 109.01.039	\\\hline
109.01.047 & 	Ремонт надресорного брусу 3751-Н~(2) &  	109.05.052 109.05.053 109.05.048 &  	109.01.048	\\\hline
109.01.048 & 	Грунтування надресорного брусу 3751-Н~(2)  &  	109.01.047 &  	109.05.058	\\\hline
109.05.058 & 	Сушка атмосферна надресорного брусу 3751-Н~(2) після грунтування &  	109.01.048 &  	109.01.049	\\\hline
109.01.049 & 	Фарбування надресорного брусу 3751-Н~(2) &  	109.05.058 &  	109.05.059	\\\hline
109.05.059 & 	Сушка атмосферна надресорного брусу 3751-Н~(2) після фарбування &  	109.01.049 &  	109.01.041 109.01.039	\\\hline
109.01.017 & 	Ремонт деталей підвісок 30.30.025 &  	109.05.052 109.05.053 &  	109.01.041 109.01.023 109.01.024  109.01.040	\\\hline
109.01.018 & 	Ремонт тяги повідця &  	109.05.052 109.05.053 &  	109.01.041 109.01.039 109.01.025 	\\\hline
109.01.019 & 	Ремонту запобіжного стрижня центрального підвішування &  	109.05.052 109.05.053 & 	109.01.041 109.01.039 109.01.025 	\\\hline 
109.01.020 & 	Ремонт траверсів &  	109.05.052 109.05.053 & 	109.01.037 109.01.038 109.01.025 109.01.027	\\\hline
109.01.021 & 	Ремонт тяги 13.41.033 &  	109.05.049 109.05.050 &  	109.01.037 109.01.038 109.01.025 109.01.027	\\\hline
109.01.022 & 	Ремонт важелів гальмівної важільної передачі~(візків) &  	109.05.049 109.05.050 &  	109.01.037 109.01.038 109.01.025 109.01.027	\\\hline
109.01.023 & 	Випробування деталей на розтяг 0.162-04.20.00~: 000~(1)  &  	109.01.017 &  	109.01.025 109.01.027	\\\hline
109.01.024 & 	Випробування деталей на розтяг 0.114-04.10.00~: 000~(1) &  	109.01.017 &  	109.01.025 109.01.027	\\\hline
109.01.025 & 	Грунтування відремонтованих деталей і складових одиниць візку 0.162-04.20.00~: 000~(1) &  	109.01.023 109.01.017 109.01.018 109.01.019 109.01.020  &  	109.05.060	\\\hline
109.05.060 & 	Сушка атмосферна відремонтованих деталей і складових одиниць візку 0.162-04.20.00~: 000~(1) після грунтування &  	109.01.025 &  	109.01.026	\\\hline
109.01.026 & 	Фарбування відремонтованих деталей і складових одиниць візку 0.162-04.20.00~: 000~(1) &  	109.05.060 &  	109.05.061	\\\hline
109.05.061 & 	Сушка атмосферна відремонтованих деталей і складових одиниць візку 0.162-04.20.00~: 000~(1) після фарбування  &  	109.01.026 &  	109.01.037	\\\hline
109.01.027 & 	Грунтування відремонтованих деталей і складових одиниць візку 0.114-10.00~: 000~(1) &  	109.01.024  109.01.017 109.01.018 109.01.019 109.01.020 &  	109.05.062	\\\hline
109.05.062 & 	Сушка атмосферна відремонтованих деталей і складових одиниць візку 0.114-04.10.00~: 000~(1) після грунтування  &  	109.01.027 &  	109.05.057	\\\hline
109.01.028 & 	Фарбування відремонтованих деталей і складових одиниць візку 0.114-04.10.00~: 000~(1)  &  	109.05.062 &  	109.05.063	\\\hline
109.05.063 & 	Сушка атмосферна відремонтованих деталей і складових одиниць візку 0.114-04.10.00~: 000~(1) після фарбування  &  	109.01.028 &  	109.01.038	\\\hline
109.01.029 & 	Установка термодатчиків на візок 0.162-04.20.00~: 000~(1) &  	109.05.054 &  	109.01.033 109.01.039 109.01.040	\\\hline
109.01.030 & 	Установка термодатчиків на візок 0.114-04.10.00~: 000~(1)  &  	109.05.055 &  	109.01.041 109.01.034 109.01.039	\\\hline
109.01.031 & 	Установка шпінтонов на рами візків 0.162-04.20.00~: 000~(1)  &  	109.05.051 109.05.054 &  	109.01.033 109.01.037	\\\hline
109.01.032 & 	Установка шпінтонов на рами візків 0.114-10.00~: 000~(1)  &  	109.05.051  109.05.055 &  	109.01.034 109.01.038	\\\hline
109.01.033 & 	Фарбування рами візку 0.162-04.20.00~: 000~(1)  &  	109.01.029 109.01.031 &  	109.05.064	\\\hline
109.05.064 & 	Сушка атмосферна рами візку 0.162-04.20.00~: 000~(1) після фарбування  &  	109.01.033 &  	109.01.037	\\\hline
109.01.034 & 	Фарбування рами візку 0.114-04.10.00~: 000~(1)  &  	109.01.030 109.01.032 &  	109.05.065	\\\hline
109.05.065 & 	Сушка атмосферна рами візку 0.114-04.10.00~: 000~(1) після фарбування &  	109.01.034 &  	109.01.038	\\\hline
109.01.035 & 	Ремонт підвісок башмаків візків  &  	109.05.049 109.05.050 &  	109.01.037 109.01.038	\\\hline
109.01.036 & 	Ремонт пружин візків  &  	109.05.049 109.05.050 &  	109.01.040 109.01.041 109.01.039	\\\hline
109.01.037 & 	Збірка і монтаж гальмової важільної передачі візку 0.162-04.20.00~: 000~(1)  &  	109.01.021 109.01.022 109.01.020 109.01.035 109.05.061 109.05.064 109.01.031 &  	109.01.040 109.01.039	\\\hline
109.01.038 & 	Збірка і монтаж гальмової важільної передачі візку 0.114-04.10.00~: 000~(1)  &  	109.01.021 109.01.022 109.01.020 109.01.035 109.05.063 109.05.065 109.01.032 &  	109.01.041 109.01.039	\\\hline
109.01.039 & 	Випробування гасителів коливання 45.30.045  &  	109.01.036 109.01.037 109.05.057  &  	109.01.040 109.01.041	\\\hline
109.01.040 & 	Збірка візку 0.162-04.20.00~: 000~(1)  &  	109.01.036 109.01.037 109.01.039 109.05.057 109.01.017 109.01.018 &  	109.01.043	\\\hline
109.01.043 & 	Фарбування візку 0.162-04.20.00~: 000~(1) в зборі  &  	109.01.040 &  	109.05.066	\\\hline
109.05.066 & 	Сушка атмосферна візок 0.162-04.20.00~: 000~(1) в зборі  &  	109.01.043 &  		\\\hline
109.01.041 & 	Збірка візку 0.114-04.10.00~: 000~(1)  &  	109.01.036 109.01.038 109.01.039 109.01.017 109.01.018  &  	109.01.042	\\\hline
109.01.042 & 	Фарбування візку 0.114-10.00~: 000~(1) в зборі  &  	109.01.041 &  	109.05.067	\\\hline
109.05.067 & 	Сушка атмосферна візок 0.114-10.00~: 000~(1) в зборі  &  	109.01.042 &  		\\\hline
109.05.048 & 	Дефектація візків  &  	109.05.049 109.05.050 &  	109.01.007 109.01.009 109.01.044 109.01.047 109.01.004	\\\hline
103.01.002 & 	Нове формування пасажирської колісної пари РУ1Ш-950 Б  &  	 109.01.001 109.01.002 &  	103.01.003	\\\hline
103.01.003 & 	Фарбування колісної пари РУ1Ш-950 ТУ 24.05.816-82~(для вагона мод. 47К)  &  	103.01.002 &  	109.01.040 109.01.041	\\\hline

\end{longtable}

\section*{Висновки до розділу~\ref{ch:Experiments}}

\begin{enumerate}
\item Виконано програмну реалізацію запропонованого у підрозділі \ref{sec:ExtendedNeoFuzzyNeuron} розширеного нео-фаззі нейрону, який реалiзує нечiтке висновуння за Такаґi\=/Суґено довiльного порядку. Продемонстровано, що розширені нео-фаззі нейрони мають покращенi апроксимуючi властивостi у поріинянні з традиційними нео-фаззі нейронами. Досліджено залежність точності прогнозування розширеними нео-фаззі нейронами як штучно сгенерованих, так і дійсних хаотичних часових рядів від порядку нечіткого висновування та кількості функцій належності.
\item Проведено імітаційне моделювання запропонованої у розділі \ref{ch:CascadedNeoFuzzySystemWithPoolOptimization} архiтектури (що ґрунтується на розширених нео-фаззі нейронах) та методів навчання гiбридної каскадної нейронної мережi, що еволюцiонує, з оптимiзацiєю пулу нейронiв у кожному каскадi. Показано що пропоновані нейрони-узагальнювачі реалiзують оптимальний за точнiстю прогноз нелiнiйних стохастичних i хаотичних сигналiв у онлайн режимi.
\item Виконано програмну реалізацію запропонованого у підрозділі \ref{sec:MultidimentionalNeoFuzzyNeuron} багатовимірного нео-фаззі нейрона.
\item Змодельовано запропоновану у підрозділі \ref{sec:MIMOEvolvingCascadedSystemBuiltOnMNFNs} MIMO архiтектуру з оптимiзацiєю пулу багатовимiрних нейронiв у кожному каскадi, що реалізує нелінійне відображення $R^n \rightarrow R^g$ у режимі послідовного обробляння даних. Показано, що запропоновані багатовимiрні узагальнюючі елементи в режимi реального часу реалiзують оптимальне об’єднання багатовимiрних вихiдних сигналiв нейронiв пулу каскадів.
\item Проведено імітаційне моделювання запропонованої у розділі \ref{ch:EvolvingClusteringSystem} архiтектури i методу самонавчання каскадної нейро-фаззi системи, що еволюцiонує, для послiдовного кластерування потокiв даних з автоматичним визначенням оптимальної кiлькостi кластерiв.
\item Розв’язано практичну задачу нечіткого кластерування світлин для подальшого їх классифікування за умови невизначенності щодо кількості кластерів та рівня їх розмитості за допомогою самонавчанної гібридної системи, що її було запропоновано у підрозділі \ref{sec:EvolvingSelfLearningSystemArchitecture}.
\item Розв’язано практичну задачу прогнозування витрат нормогодин для ремонтних робіт візків вагонів типу 61-425, 61-181, 47Д та 47К на ТОВ <<Харківський вагонобудівний завод>> за допомогою каскадної гібридної нейро-мережі, що еволюціонує, та модифікованого методу її навчання, які було запропоновано у підрозділі~\ref{sec:OptimizedCascadedNeuralNetworkArchitecture}.
\end{enumerate}
\bibliographystyle{ugost2008ns}
\bibliography{references}	
\end{document}
